{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Fixel Algorithms](https://fixelalgorithms.co/images/CCExt.png)](https://fixelalgorithms.gitlab.io/)\n",
    "\n",
    "# Machine Learning Methods\n",
    "\n",
    "## Exercise 005 - Regression\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 0.1.000 | 17/02/2023 | Royi Avital | First version                                                      |\n",
    "|         |            |             |                                                                    |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/MachineLearningMethods/2023_01/Exercise0005RegressionSolution.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T09:30:06.492269Z",
     "start_time": "2022-02-02T09:30:06.220934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Miscellaneous\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "from platform import python_version\n",
    "import random\n",
    "import urllib.request\n",
    "import re\n",
    "\n",
    "# Typing\n",
    "from typing import Callable, List, Tuple\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# from bokeh.plotting import figure, show\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython\n",
    "from IPython.display import Image, display\n",
    "from ipywidgets import Dropdown, FloatSlider, interact, IntSlider, Layout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# %matplotlib inline\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "# sns.set_theme() #>! Apply SeaBorn theme\n",
    "\n",
    "runInGoogleColab = 'google.colab' in str(get_ipython())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "FIG_SIZE_DEF    = (8, 8)\n",
    "ELM_SIZE_DEF    = 50\n",
    "CLASS_COLOR     = ('b', 'r')\n",
    "EDGE_COLOR      = 'k'\n",
    "MARKER_SIZE_DEF = 10\n",
    "LINE_WIDTH_DEF  = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixel Algorithms Packages\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "In this exercise we'll use the most advanced Gradient Boosted models: `LightGBM` and `XGBoost`.  \n",
    "We'll work on the [Insurance Data Set from OpenML](https://www.openml.org/search?type=data&id=43463).  \n",
    "In the data set we're given data of people and their insurance charges.\n",
    "\n",
    "This exercise introduces:\n",
    "\n",
    " - Some basic EDA / Feature Engineering / Pre Processing.\n",
    " - Working with categorical features.\n",
    " - Building a pipeline based on a Data Frame with processing only sub set of the features.\n",
    " - Utilizing the `LightGBM` and `XGBoost` packages with the `LGBMRegressor` and `XGBRegressor`.\n",
    " - Optimizing a pipeline which one the hyper parameters is the model type.\n",
    "\n",
    "The objective is to predict a person insurance by a regression model.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> In `LightGBM` support for categorical data is fully implemented. In `XGBoost` it is still a work in progress.\n",
    "\n",
    "In this exercise:\n",
    "\n",
    "1. Download the data (Automatically by the code).\n",
    "2. Parse data into a data structure to work with (Automatically by the code).\n",
    "3. Explore the data and the features (EDA).\n",
    "4. Feature Engineering and Pre Process of the data.\n",
    "4. Optimize the _Hyper Parameters_ of the models using the `R2` score.\n",
    "5. Build a _pipeline_ which process only a sub set of the columns.\n",
    "6. Plot the _regression error_ of the best model on the data.\n",
    "\n",
    "The hyper parameters optimization and the features engineering should get you `R2 > 0.8`.  \n",
    "With some effort even `R2 > 0.85` is achievable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "numSamplesTrain = 35_000\n",
    "numSamplesTest  = None\n",
    "\n",
    "# Hyper Parameters of the Model\n",
    "\n",
    "lRegModel       = ['LightGBM', 'XGBoost']\n",
    "#===========================Fill This===========================#\n",
    "# 1. Set the list of learning rate (3 values in range [0.05, 0.5]).\n",
    "# 2. Set the list of maximum number of trees in the model (3 integer values in range [10, 200]).\n",
    "# 3. Set the list of maximum leaf nodes (3 integer values in range [10, 50]).\n",
    "# !! Start with small number of combinations until the code is stable.\n",
    "# !! You may want to optimize the polynomial degree list after the data analysis.\n",
    "lLearnRate      = ???\n",
    "lNumEstimators  = ???\n",
    "lMaxLeafNodes   = ???\n",
    "lPolyDeg        = ???\n",
    "#===============================================================#\n",
    "\n",
    "numFold = 4 #<! Don't change!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary Functions\n",
    "\n",
    "def PlotRegResults( vY, vYPred, hA:plt.Axes = None, figSize: Tuple[int, int] = FIG_SIZE_DEF, lineWidth: int = LINE_WIDTH_DEF, elmSize: int = ELM_SIZE_DEF, classColor: Tuple[str, str] = CLASS_COLOR, axisTitle: str = None ) -> plt.Axes:\n",
    "\n",
    "    if hA is None:\n",
    "        hF, hA = plt.subplots(figsize = figSize)\n",
    "    else:\n",
    "        hF = hA.get_figure()\n",
    "\n",
    "    numSamples = len(vY)\n",
    "    if (numSamples != len(vYPred)):\n",
    "        raise ValueError(f'The inputs `vY` and `vYPred` must have the same number of elements')\n",
    "    \n",
    "    \n",
    "    hA.plot(vY, vY, color = 'r', lw = lineWidth, label = 'Ground Truth')\n",
    "    hA.scatter(vY, vYPred, s = elmSize, color = classColor[0], edgecolor = 'k', label = f'Estimation')\n",
    "    hA.set_xlabel('Label Value')\n",
    "    hA.set_ylabel('Prediction Value')\n",
    "    # hA.axis('equal')\n",
    "    if axisTitle is not None:\n",
    "        hA.set_title(axisTitle)\n",
    "    hA.legend()\n",
    "    \n",
    "    return hA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate / Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading / Generating Data\n",
    "\n",
    "dfData, _ = fetch_openml('Insurance-Premium-Data', version = 1, return_X_y = True, as_frame = True, parser = 'auto')\n",
    "\n",
    "print(f'The data shape: {dfData.shape}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic info on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data frame Head\n",
    "# Look at the structure of the data\n",
    "dfData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Frame Information\n",
    "# Look at the types of each feature \n",
    "dfData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Features Name\n",
    "dfData.columns = ['Age', 'Sex', 'BMI', 'NumberChildren', 'Smoker', 'Region', 'Charges']\n",
    "dfData"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic EDA, Feature Engineering & Pre Processing\n",
    "\n",
    "Work with data starts with looking at the data and the connection between the features.  \n",
    "This is an iterative procedure: EDA -> Feature Engineering -> Model Optimization -> Error Analysis -> EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair Plot\n",
    "# The basic connection between the features\n",
    "sns.pairplot(data = dfData)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Visualization\n",
    "# Basic EDA on the Data: Box Plot for discrete data, Scatter plot for continuos data.\n",
    "\n",
    "numCol = dfData.shape[1]\n",
    "lCols  = dfData.columns\n",
    "numAx  = int(np.ceil(np.sqrt(numCol)))\n",
    "\n",
    "hIsCatLikData = lambda dsX: (pd.api.types.is_categorical_dtype(dsX) or pd.api.types.is_bool_dtype(dsX) or pd.api.types.is_object_dtype(dsX) or pd.api.types.is_integer_dtype(dsX))\n",
    "\n",
    "hF, hAs = plt.subplots(nrows = numAx, ncols = numAx, figsize = (20, 12))\n",
    "hAs = hAs.flat\n",
    "\n",
    "for ii in range(numCol):\n",
    "    colName = dfData.columns[ii]\n",
    "    if hIsCatLikData(dfData[colName]):\n",
    "        sns.boxplot(data = dfData, x = colName, y = 'Charges', ax = hAs[ii])\n",
    "    else:\n",
    "        sns.scatterplot(data = dfData, x = colName, y = 'Charges', ax = hAs[ii])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> What do you think about the Dynamic Range of the `Age` feature?\n",
    "* <font color='brown'>(**#**)</font> If the data set is large enough, one might consider specialized models. For instance, specialization for the `Sex` and the `Smoker` features.  \n",
    "  Though, in their basic levels, _decision trees_ with proper support for categorical features, can do exactly that inherently. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important thing is to look at the distribution of the objective values.  \n",
    "Some models make assumptions on the values.  \n",
    "Sometimes we even process the target value, for instance applying some transform to make it more Gaussian like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hF, hA = plt.subplots(figsize = (10, 7))\n",
    "\n",
    "sns.kdeplot(data = dfData, x = 'Charges', fill = True, clip = (0, np.inf), ax = hA)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> In the sense of \"Classification\", is the data balanced?\n",
    "* <font color='red'>(**?**)</font> How should we handle the test / train split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of the `Age` Feature\n",
    "\n",
    "hF, hA = plt.subplots(figsize = (10, 7))\n",
    "\n",
    "sns.histplot(data = dfData, x = 'Age', discrete = True, ax = hA)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter of the Age Feature\n",
    "\n",
    "hF, hA = plt.subplots(figsize = (10, 7))\n",
    "\n",
    "sns.scatterplot(data = dfData, x = 'Age', y = 'Charges', ax = hA)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> What kind of relationship do you see between the `Age` and the `Charges`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouped Scatter Plot with Linear Model\n",
    "# It is sometimes good to see the behavior within a slice of features.\n",
    "# In this case we want to see the relation between BMI to charges split to different sex.\n",
    "\n",
    "sns.lmplot(data = dfData, x = 'BMI',  y = 'Charges', hue = 'Smoker', col = 'Sex', palette = 'magma')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouped Scatter Plot with Linear Model\n",
    "# It is sometimes good to see the behavior within a slice of features.\n",
    "# In this case we want to see the relation between BMI to charges split to different regions.\n",
    "sns.lmplot(data = dfData, x = 'BMI',  y = 'Charges', hue = 'Smoker', col = 'Region', palette = 'magma')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Do all regions behave the same? Is there an outlier?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "In this section we'll do a simple feature engineering / pre processing:\n",
    "\n",
    "1. Create 2 Binary features based on the `Region` feature:\n",
    "  - `RegionNorth` if there is `north` in `Region`.\n",
    "  - `RegionWest` if there is `west` in `Region`.\n",
    "2. Create a new feature based on the BMI according to [American Cancer Society - Normal Weight Ranges: Body Mass Index (BMI)](https://www.cancer.org/healthy/cancer-causes/diet-physical-activity/body-weight-and-cancer-risk/adult-bmi.html).\n",
    "3. Create a new feature based on the age range.\n",
    "\n",
    "You may try more features, for instance:\n",
    " \n",
    " - High Risk: If `Obese`, `Smoker` and `Elder` (Or other combination which makes sense).\n",
    " - Family Size: `Small`, `Large` (Analyze the histogram to determine).\n",
    " - Family Role: `Father` / `Mother` / `None`.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> Feature engineering is the the magic in the process. We don't want too much features, but we want good ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre Process Data\n",
    "# Region\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Create 2 new features: `RegionNorth` and `RegionWest`:\n",
    "#  - `RegionNorth` = 1 if `north` in `Region` else 0.\n",
    "#  - `RegionWest` = 1 if `west` in `Region` else 0.\n",
    "# 2. Remove the `Region` feature.\n",
    "????\n",
    "#===============================================================#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre Process Data\n",
    "# BMI\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Create a function which gets a float number `inBMI` and returns a string.\n",
    "# 2. The string should match: 'Under Weight', 'Normal Weight', 'Over Weight' and 'Obese'.\n",
    "# 3. The criteria should match https://www.cancer.org/cancer/cancer-causes/diet-physical-activity/body-weight-and-cancer-risk/adult-bmi.html.\n",
    "def BmiCategory( inBmi: float ) -> str:\n",
    "    ?????\n",
    "\n",
    "#===============================================================#\n",
    "\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Create a new feature `BMI Category` by mapping `BMI` using `BmiCategory()`.\n",
    "dfData['BMI Category'] = ???\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre Process Data\n",
    "# Age\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Create a function which gets an integer number `inAge` and returns a string.\n",
    "# 2. The string should match: Young Adult', 'Senior Adult', 'Elder'.\n",
    "# 3. Set the limits according to data or any other sensible choice.\n",
    "def AgeCategory( inAge: int ) -> str:\n",
    "    ????\n",
    "\n",
    "#===============================================================#\n",
    "\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Create a new feature `Age Category` by mapping `Age` using `AgeCategory()`.\n",
    "dfData['Age Category'] = ???\n",
    "#===============================================================#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> In practice, after some feature engineering, we need to redo the EDA part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the Data\n",
    "dfData"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre Processing\n",
    "\n",
    "We'll apply simple transforms on the data:\n",
    "\n",
    "1. Map the `Sex`, `Smoker`, `Age Category` and `BMI Category` into numerical values.\n",
    "2. Set the categorical features data types into `categorical`.\n",
    "3. Split the data into `dfX` and `dsY`.\n",
    "\n",
    "\n",
    "* <font color='brown'>(**#**)</font> In practice, part of the pre processing is rejecting outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Categorical Data into Numeric Value\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Map the values into numerical values.\n",
    "dfData['Sex'] = ???\n",
    "dfData['Smoker'] = ???\n",
    "dfData['Age Category'] = ???\n",
    "dfData['BMI Category'] = ???\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe the Data\n",
    "dfData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe the Data Types\n",
    "dfData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the Categorical / Numerical Columns\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Create a list of the columns which are categorical features.\n",
    "# 2. Create a list of the numerical features.\n",
    "lCatData = ???\n",
    "lNumData = ???\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data into Features and Labels\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Set the data of `dfX`.\n",
    "# 2. Set the labels of `dsY`.\n",
    "dfX = ???\n",
    "dsY = ???\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Categorical Features\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Convert the columns in `lCatData` to categorical data.\n",
    "# !! You may use `astype('category')` or `pd.Categorical()`.\n",
    "?????\n",
    "#===============================================================#\n",
    "\n",
    "# Observe the Data\n",
    "dfX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe the Data Types\n",
    "# Make sure all columns are set correctly.\n",
    "dfX.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize Regressors\n",
    "\n",
    "In this section we'll train an Ensemble of Trees which are optimized by Gradient Boosting.  \n",
    "One of the hyper parameters to optimize is the implementation: `LGBMRegressor` or `XGBRegressor`.\n",
    "\n",
    "This models has a lot of hyper parameters yet we'll focus on:\n",
    "\n",
    " - Implementation: `LGBMRegressor` or `XGBRegressor`.\n",
    " - Number of Leaves Nodes (`num_leaves` / `max_leaves`) - Sets the maximum number of leaves in each tree.\n",
    " - Learning Rate (`learning_rate`) - The learning rate of the ensemble (The significance of each model compared to those before it).\n",
    " - Number of Trees (`n_estimators`) - The number of iterations of the algorithm. In each iteration a single tree is added.\n",
    "\n",
    "The score will be the `R2` score.  \n",
    "We'll use `KFold` for cross validation and `cross_val_predict()` to build the predicted values.\n",
    "\n",
    "The actual model is a pipeline of `PolynomialFeatures` and the model.  \n",
    "Yet, we want to use `PolynomialFeatures` only on subset of the features (The non categorical).\n",
    "\n",
    "* <font color='brown'>(**#**)</font> In some cases people can use the categorical features in the polynomial transform. Yet in order to learn the ability to process a sub set, we'll focus on the numerical ones.\n",
    "\n",
    "In order to process a subset of the features we'll use [`ColumnTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html).  \n",
    "\n",
    "The process we'll do is as following:\n",
    "\n",
    "1. Build a data frame which has a row per a combination of the hyper parameters.\n",
    "2. Iterate on all rows of the data frame, for each combination build the pipeline and predict the labels with `cross_val_predict()`.\n",
    "3. Calculate the score and update the row with it.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> Make sure to read about the `remainder` parameter. As it will allow us pipelining the data properly for our use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Data Frame\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Calculate the number of combinations.\n",
    "# 2. Create a nested loop to create the combinations between the parameters (Use `itertools.product()`).\n",
    "# 3. Store the combinations as the columns of a data frame.\n",
    "numComb = len(???) * len(???) * len(???) * len(???) * len(???)\n",
    "dData   = {'Model': [], 'Learn Rate': [], 'Number of Estimators': [], 'Max Leaf Nodes': [], 'Poly Deg': [], 'R2': [0.0] * numComb}\n",
    "\n",
    "for (regModel, learnRate, numEst, maxLeafNode, polyDeg) in itertools.product(???):\n",
    "    dData['Model'].append(???)\n",
    "    dData['Learn Rate'].append(???)\n",
    "    dData['Number of Estimators'].append(???)\n",
    "    dData['Max Leaf Nodes'].append(???)\n",
    "    dData['Poly Deg'].append(???)\n",
    "#===============================================================#\n",
    "\n",
    "dfModelScore = pd.DataFrame(data = dData)\n",
    "dfModelScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the Model\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Iterate over each row of the data frame `dfModelScore`. Each row defines the hyper parameters.\n",
    "# 2. Construct the model.\n",
    "# 3. Train it on the Train Data Set.\n",
    "# 4. Calculate the score.\n",
    "# 5. Store the score into the data frame column.\n",
    "\n",
    "\n",
    "for ii in range(numComb):\n",
    "    modelName       = ???\n",
    "    learningRate    = ???\n",
    "    numEst          = ???\n",
    "    maxLeafNodes    = ???\n",
    "    polyDeg         = ???\n",
    "\n",
    "    print(f'Processing model {ii + 1:03d} out of {numComb}')\n",
    "    print(f'Model Parameters: {modelName=}, {learningRate=}, {numEst=}, {maxLeafNodes=}, {polyDeg=}') #<! Python trick for F strings\n",
    "\n",
    "    #!! Set the parameters of the column transformer. Set `remainder` properly to have all data moving forward.\n",
    "    oColTrns = ColumnTransformer([('PolyFeatures', PolynomialFeatures(degree = ???), lNumData)], remainder = ???)\n",
    "    if modelName == 'LightGBM':\n",
    "        oModelReg = ???(n_estimators = ???, learning_rate = ???, num_leaves= ???)\n",
    "    elif modelName == 'XGBoost':\n",
    "        oModelReg = ???(n_estimators = ???, learning_rate = ???, max_leaves = ???)\n",
    "    else:\n",
    "        raise ValueError(f'The {modelName=} is not supported.')\n",
    "    \n",
    "    # Building the pipeline\n",
    "    oPipeReg = Pipeline([('PolyFeat', ???), ('Regressor', ???)])\n",
    "    \n",
    "    # Prediction by Cross Validation\n",
    "    vYPred = cross_val_predict(???, dfX, dsY, cv = KFold(n_splits = ???, shuffle = True))\n",
    "\n",
    "    # Score based on the prediction\n",
    "    scoreR2 = r2_score(???, ???)\n",
    "    dfModelScore.loc[ii, 'R2'] = ???\n",
    "    print(f'Finished processing model {ii + 1:03d} with `R2 = {???}.')\n",
    "#===============================================================#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> Efficiency wise, it would be better to calculate the features once per `polyDeg`.\n",
    "* <font color='red'>(**?**)</font> Why don't we use a stratified K-Fold split in the case above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Sorted Results (Descending)\n",
    "# Pandas allows sorting data by any column using the `sort_values()` method\n",
    "# The `head()` allows us to see only the the first values\n",
    "dfModelScore.sort_values(by = ['R2'], ascending = False).head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> With good optimization the `LightGBM` models should be high ranked. In the data above their built in support for categorical data can assist squeezing more.\n",
    "* <font color='brown'>(**#**)</font> The reason it is easy for the `LightGBM` model to optimize on categorical data is related to the way they work (Analyzing the histograms of the data)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Model\n",
    "\n",
    "In this section we'll extract the best model an retrain it on the whole data (`dfXNum`).  \n",
    "We need to export the model which has the best Test values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Optimal Hyper Parameters\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Extract the index of row which maximizes the score.\n",
    "# 2. Use the index of the row to extract the hyper parameters which were optimized.\n",
    "\n",
    "#! You may find the `idxmax()` method of a Pandas data frame useful.\n",
    "idxArgMax = ???\n",
    "#===============================================================#\n",
    "\n",
    "modelName       = dfModelScore.loc[idxArgMax, 'Model']\n",
    "learningRate    = dfModelScore.loc[idxArgMax, 'Learn Rate']\n",
    "numEst          = dfModelScore.loc[idxArgMax, 'Number of Estimators']\n",
    "maxLeafNodes    = dfModelScore.loc[idxArgMax, 'Max Leaf Nodes']\n",
    "polyDeg         = dfModelScore.loc[idxArgMax, 'Poly Deg']\n",
    "\n",
    "print(f'The optimal hyper parameters are: {modelName=}, {learningRate=}, {numEst=}, {maxLeafNodes=}, {polyDeg=}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the Optimal Model & Train on the Whole Data\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Construct the model with the optimal hyper parameters.\n",
    "# 2. Fit the model on the whole data set.\n",
    "oColTrns = ???\n",
    "if modelName == 'LightGBM':\n",
    "    oModelReg = ???\n",
    "elif modelName == 'XGBoost':\n",
    "    oModelReg = ???\n",
    "else:\n",
    "    raise ValueError(f'The model name: {modelName} is not supported.')\n",
    "\n",
    "oPipeReg = ???\n",
    "\n",
    "oPipeReg = ???\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Score (R2)\n",
    "\n",
    "print(f'The model score (R2) is: {oPipeReg.score(dfX, dsY):0.2f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Regression Error\n",
    "\n",
    "hF, hA = plt.subplots(figsize = (10, 10))\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "hA = PlotRegResults(???, hA = hA)\n",
    "#===============================================================#\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='green'>(**@**)</font> Try to get more features and improve results.  \n",
    "  Pay attention to the samples which have large error.\n",
    "* <font color='green'>(**@**)</font> Try building a multiple models in a single model.  \n",
    "  For instance, a model for smokers and non smokers.\n",
    "* <font color='green'>(**@**)</font> Analyze the feature importance. Create features which are important. Remove those which are not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
