{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Fixel Algorithms](https://fixelalgorithms.co/images/CCExt.png)](https://fixelalgorithms.gitlab.io/)\n",
    "\n",
    "# Machine Learning Methods\n",
    "\n",
    "## Exercise 008 - Anomaly Detection\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 0.1.000 | 05/03/2023 | Royi Avital | First version                                                      |\n",
    "|         |            |             |                                                                    |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/MachineLearningMethods/2023_01/Exercise0008AnomalyDetection.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T09:30:06.492269Z",
     "start_time": "2022-02-02T09:30:06.220934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "# Miscellaneous\n",
    "import itertools\n",
    "import os\n",
    "from platform import python_version\n",
    "import random\n",
    "import re\n",
    "\n",
    "# Typing\n",
    "from typing import Callable, List, Tuple\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython\n",
    "from IPython.display import Image, display\n",
    "from ipywidgets import Dropdown, FloatSlider, interact, IntSlider, Layout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# %matplotlib inline\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "# sns.set_theme() #>! Apply SeaBorn theme\n",
    "\n",
    "runInGoogleColab = 'google.colab' in str(get_ipython())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "FIG_SIZE_DEF    = (8, 8)\n",
    "ELM_SIZE_DEF    = 50\n",
    "CLASS_COLOR     = ('b', 'r')\n",
    "EDGE_COLOR      = 'k'\n",
    "MARKER_SIZE_DEF = 10\n",
    "LINE_WIDTH_DEF  = 2\n",
    "\n",
    "DATA_FILE_URL = r'https://raw.githubusercontent.com/nsethi31/Kaggle-Data-Credit-Card-Fraud-Detection/master/creditcard.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixel Algorithms Packages\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "In this exercise we'll use _Dimensionality Reduction_ and _UnSupervised Anomaly Detection_ for _Supervised Anomaly Detection_ based on a classifier:\n",
    "\n",
    " - The _dimensionality reduction_ method used is based on feature selection and not feature mixing.\n",
    " - The _unsupervised anomaly detection_ is based on _Isolation Forest_ or _Local Outlier Factor_.\n",
    "\n",
    "The idea is as following:\n",
    "\n",
    " - Use _dimensionality reduction_ to reduce the number of features.\n",
    " - Use the the score of the anomaly detector as a feature.\n",
    "\n",
    "In this exercise:\n",
    "\n",
    "1. We'll process real world credit card fraud data.\n",
    "2. We'll use a classifier to identify the fraud transactions.\n",
    "3. We'll use SciKit Learn's `RFE` for feature selection.\n",
    "4. We'll build an unsupervised anomaly detection model to use its decision function score as a feature.\n",
    "5. We'll optimize some of the hyper parameters using a labels vector predicted by cross validation.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> Some stages requires the full implementation of the stage (Not only a single line completion).\n",
    "* <font color='brown'>(**#**)</font> The idea of this exercise is to show that there are clever way to generate features.  \n",
    "Yet it still might be that the process suggested here won't improve results (Such in a case the base model used all information the anomaly detector used).  \n",
    "Still, the idea of using models (Usually unsupervised) to generate features is a skill to master.\n",
    "\n",
    "**Objective**: Get `F1` of above 80% for the predict classification using cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# Data\n",
    "numSamples = 6_000\n",
    "lClass      = ['Legit', 'Fraud']\n",
    "\n",
    "# Cross Validation\n",
    "numFolds    = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary Functions\n",
    "\n",
    "def PlotFeatureHistogram(dfData: pd.DataFrame, featColName: str, classColName: str, hA: plt.Axes = None, figSize: Tuple[int, int] = FIG_SIZE_DEF) -> plt.Axes:\n",
    "\n",
    "    if hA is None:\n",
    "        hF, hA = plt.subplots(figsize = figSize)\n",
    "    \n",
    "    sns.histplot(data = dfData, x = featColName, hue = classColName, stat = 'density', common_norm = False, multiple = 'dodge', ax = hA)\n",
    "    sns.kdeplot(data = dfData, x = featColName, hue = classColName, common_norm = False, ax = hA)\n",
    "\n",
    "    return hA\n",
    "\n",
    "def PlotLabelsHistogram(vY: np.ndarray, hA = None, lClass = None, xLabelRot: int = None) -> plt.Axes:\n",
    "\n",
    "    if hA is None:\n",
    "        hF, hA = plt.subplots(figsize = (8, 6))\n",
    "    \n",
    "    vLabels, vCounts = np.unique(vY, return_counts = True)\n",
    "\n",
    "    hA.bar(vLabels, vCounts, width = 0.9, align = 'center')\n",
    "    hA.set_title('Histogram of Classes / Labels')\n",
    "    hA.set_xlabel('Class')\n",
    "    hA.set_ylabel('Number of Samples')\n",
    "    hA.set_xticks(vLabels)\n",
    "    if lClass is not None:\n",
    "        hA.set_xticklabels(lClass)\n",
    "    \n",
    "    if xLabelRot is not None:\n",
    "        for xLabel in hA.get_xticklabels():\n",
    "            xLabel.set_rotation(xLabelRot)\n",
    "\n",
    "    return hA\n",
    "\n",
    "def CrossValPredWeighted( modelEst, mX: np.ndarray, vY: np.ndarray, vW: np.ndarray = None, numFolds: int = 5, stratifyMode: bool = True, seedNum: int = None ) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    modelEst - A model with `fit()` and `predict()` methods.\n",
    "    mX - A NumPy array of the data.\n",
    "    vY - A NumPy array of the labels.\n",
    "    vW - A NumPy array of the per sample weight.\n",
    "    numFolds - An integer of the number of folds.\n",
    "    stratifyMode - A boolean, if `True` use stratified split, if False use regular random split.\n",
    "    seedNum - An integer to set the seed of the splitters.\n",
    "    \"\"\"\n",
    "\n",
    "    numSamples  = mX.shape[0]\n",
    "    vYPred      = np.zeros_like(vY)\n",
    "\n",
    "    #===========================Fill This===========================#\n",
    "    # 1. Construct the K-Fold split object using `StratifiedKFold` or `KFold` according to `stratifyMode`.\n",
    "    # 2. Iterate over the splits, per split, fit the model and predict the labels on the rest of the data.\n",
    "    # !! Set `shuffle = True` for the splitters.\n",
    "    ????\n",
    "    \n",
    "    #==============================================================#\n",
    "    \n",
    "    return vYPred\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate / Load Data\n",
    "\n",
    "In this notebook we'll use the [`creditcard`](https://www.openml.org/search?type=data&id=1597) data set.\n",
    "\n",
    "The datasets contains transactions made by credit cards in September 2013 by european cardholders.  \n",
    "This dataset present transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions.  \n",
    "The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
    "\n",
    "It contains only numerical input variables which are the result of a **PCA transformation** in order to preserve confidentiality.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The features: `V1`, `V2`, ..., `V28` the PCA transformed data.\n",
    "* <font color='brown'>(**#**)</font> The `Class` column is the labeling where `Class = 1` means a fraud transaction.\n",
    "\n",
    "\n",
    "The **tasks** in this section:\n",
    "\n",
    "1. Normalize the data such each feature has zero mean and unit variance.\n",
    "2. Create a sample weight vector `vW`. The `ii` -th element of the vector is the training weight of the sample.   \n",
    "   The weight should be according to the `balanced` policy: ${w}_{i} = \\frac{ \\sum_{ k = 1 }^{ N } \\mathbb{I} \\left( {y}_{k} \\neq {y}_{i} \\right ) }{N}$.  \n",
    "   You may use SciKit Learn's [`compute_sample_weight()`](https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_sample_weight.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading / Generating Data\n",
    "\n",
    "dfData = pd.read_csv(DATA_FILE_URL)\n",
    "\n",
    "\n",
    "print(f'The features data shape: {dfData.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Data\n",
    "\n",
    "dfData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of Labels\n",
    "\n",
    "hA = PlotLabelsHistogram(dfData['Class'], lClass = lClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Data\n",
    "\n",
    "featNameDropdown = Dropdown(options = dfData.columns, value = dfData.columns[0], description = 'Feature Name')\n",
    "\n",
    "hPlotFeatHist = lambda featColName: PlotFeatureHistogram(dfData, featColName, 'Class')\n",
    "interact(hPlotFeatHist, featColName = featNameDropdown)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Look at different features above. Which features are good? Why?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "\n",
    "mX = dfData.drop(columns = ['Time', 'Class']).to_numpy()\n",
    "vY = dfData['Class'].to_numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> If the time had actual hour or something to understand time of day, we could have used it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sub Sample Data\n",
    "# Data is large, hence we'll keep a sub sample of it to make things run fast.\n",
    "\n",
    "# Identify Anomalies\n",
    "vAnomalyIdx = np.flatnonzero(vY == 1)\n",
    "numAnomalies = len(vAnomalyIdx)\n",
    "\n",
    "# Sub Sample Indices\n",
    "vIdx = np.random.choice(np.flatnonzero(vY != 1), numSamples - numAnomalies)\n",
    "vIdx = np.concatenate((vIdx, vAnomalyIdx), axis = 0)\n",
    "\n",
    "mX = mX[vIdx]\n",
    "vY = vY[vIdx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize Data\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Normalize data to have zero mean and unit standard deviation per feature.\n",
    "?????\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samples Weights\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Create a vector `vW` of length `numSamples`.\n",
    "# 2. Set `vW[ii]` to have a weight which balances the classes.\n",
    "# !! You may use SciKit Learn `compute_sample_weight()`.\n",
    "\n",
    "?????\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of Labels\n",
    "\n",
    "hA = PlotLabelsHistogram(vY, lClass = lClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The features data shape: {mX.shape}')\n",
    "print(f'The labels data shape: {vY.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 001\n",
    "\n",
    "In this stage we'll build a base classifier and reduce the number of features by feature selection.  \n",
    "We'll also implement the equivalent of `cross_val_predict()` which supports a weighted samples.\n",
    "\n",
    "The **tasks** in this section:\n",
    "\n",
    "1. Implement the function `CrossValPredWeighted()` in `Auxiliary Functions` section above.\n",
    "2. Set a base model (You may choose any method of a supervised classifier which has the attribute `coef_` or `feature_importances_`).\n",
    "3. Apply a feature selection using SciKit Learn's [`RFE`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html) (Recursive Feature Elimination).\n",
    "4. Optimize, using the cross validation loop of `CrossValPredWeighted()`, the number of feature in `RFE` and at least one hyper parameter of the classifier model of your choice.\n",
    "5. Generate `mXX` which a sub set of the selected features by the optimization of `RFE`. You may use the `support_` attribute of the `RFE` object.\n",
    "\n",
    "**Objective**: Get `F1` of above 65% for the predict classification using cross validation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> What's the maximum number of features to set in `RFE`? What's the minimum?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?????"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 002\n",
    "\n",
    "In this section we'll generate a feature for the classifier using an _unsupervised anomaly detector_.  \n",
    "To generate the feature we'll the models: `IsolationForest` and `LocalOutlierFactor`.  \n",
    "The feature will be generated based on the model's decision function.\n",
    "\n",
    "The **tasks** in this section:\n",
    "\n",
    "1. Set the models and the hyper parameters of the models to optimize.\n",
    "2. Optimize, using the cross validation loop of `CrossValPredWeighted()`, the hyper parameters of the models:\n",
    "  - Set the Anomaly Detection model per hyper parameter combination.\n",
    "  - Fit it to data (`mXX`).\n",
    "  - Generate the score per sample using `decision_function()`.\n",
    "  - Concatenate the score to the features (`mXX`).\n",
    "  - Optimize the classifier on the enriched features using `CrossValPredWeighted()`.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> You must use `novelty = True` in the `LocalOutlierFactor` model in order to have the `decision_function()` method available.\n",
    "\n",
    "**Objective**: Improve the previous step `F1` score and get at least 80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?????"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> While in this notebook we optimized each task on its own, in production we'll optimize all at once."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
