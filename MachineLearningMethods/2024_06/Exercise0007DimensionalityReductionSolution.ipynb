{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Fixel Algorithms](https://fixelalgorithms.co/images/CCExt.png)](https://fixelalgorithms.gitlab.io/)\n",
    "\n",
    "# Machine Learning Methods\n",
    "\n",
    "## Exercise 007 - Dimensionality Reduction\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 0.1.000 | 03/03/2023 | Royi Avital | First version                                                      |\n",
    "|         |            |             |                                                                    |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/MachineLearningMethods/2023_01/Exercise0007DimensionalityReductionSolution.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T09:30:06.492269Z",
     "start_time": "2022-02-02T09:30:06.220934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.manifold import Isomap, MDS, SpectralEmbedding, TSNE\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Miscellaneous\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "from platform import python_version\n",
    "import random\n",
    "import urllib.request\n",
    "import re\n",
    "\n",
    "# Typing\n",
    "from typing import Callable, List, Tuple\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython\n",
    "from IPython.display import Image, display\n",
    "from ipywidgets import Dropdown, FloatSlider, interact, IntSlider, Layout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# %matplotlib inline\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "# sns.set_theme() #>! Apply SeaBorn theme\n",
    "\n",
    "runInGoogleColab = 'google.colab' in str(get_ipython())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "FIG_SIZE_DEF    = (8, 8)\n",
    "ELM_SIZE_DEF    = 50\n",
    "CLASS_COLOR     = ('b', 'r')\n",
    "EDGE_COLOR      = 'k'\n",
    "MARKER_SIZE_DEF = 10\n",
    "LINE_WIDTH_DEF  = 2\n",
    "\n",
    "DATA_FILE_URL   = r'https://drive.google.com/uc?export=download&confirm=9iBg&id=1UXLdZgXwClgwZVszRq88UaaN2nvgMFiC'\n",
    "DATA_FILE_NAME  = r'BciData.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixel Algorithms Packages\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "In this exercise we'll use _Dimensionality Reduction_ as a feature engineering process.  \n",
    "We'll try 3 different approaches utilize the process: Linear, Non Linear and with manual feature engineering.\n",
    "\n",
    "In this exercise:\n",
    "\n",
    "1. We'll process real world EEG signals to identify movement of body parts.\n",
    "2. We'll use dimensionality reduction with a classifier to identify the part of the body.\n",
    "3. We'll optimize the combination of the dimensionality reduction and the classifier.\n",
    "4. Visualize the features in low dimension (2).\n",
    "\n",
    "* <font color='brown'>(**#**)</font> This exercise won't have single line completions. It will require coding the whole block according to instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "numRowsPlot = 3\n",
    "numColsPlot = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary Functions\n",
    "\n",
    "def PlotLabelsHistogram(vY: np.ndarray, hA = None, lClass = None, xLabelRot: int = None) -> plt.Axes:\n",
    "\n",
    "    if hA is None:\n",
    "        hF, hA = plt.subplots(figsize = (8, 6))\n",
    "    \n",
    "    vLabels, vCounts = np.unique(vY, return_counts = True)\n",
    "\n",
    "    hA.bar(vLabels, vCounts, width = 0.9, align = 'center')\n",
    "    hA.set_title('Histogram of Classes / Labels')\n",
    "    hA.set_xlabel('Class')\n",
    "    hA.set_ylabel('Number of Samples')\n",
    "    hA.set_xticks(vLabels)\n",
    "    if lClass is not None:\n",
    "        hA.set_xticklabels(lClass)\n",
    "    \n",
    "    if xLabelRot is not None:\n",
    "        for xLabel in hA.get_xticklabels():\n",
    "            xLabel.set_rotation(xLabelRot)\n",
    "\n",
    "    return hA\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate / Load Data\n",
    "\n",
    "In this exercise we'll use the Brain Computer Interfaces (BCI) Data from [BCI Competition IV](https://www.bbci.de/competition/iv/).  \n",
    "Specifically we'll use [data set 2a](https://www.bbci.de/competition/iv/#dataset2a) provided by the Institute for Knowledge Discovery (Laboratory of Brain-Computer Interfaces), Graz University of Technology, (Clemens Brunner, Robert Leeb, Gernot Müller-Putz, Alois Schlögl, Gert Pfurtscheller).  \n",
    "This is a recording of EEG signals while the subject is doing a cued movement of the left hand, right hand, feet or tongue.\n",
    "\n",
    "The data is composed of:\n",
    "\n",
    " * 22 EEG channels (0.5-100Hz; notch filtered).\n",
    " * 4 classes: left hand, right hand, feet, tongue.\n",
    " * 9 subjects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Data\n",
    "# This section downloads data from the given URL if needed.\n",
    "\n",
    "if not os.path.exists(DATA_FILE_NAME):\n",
    "    urllib.request.urlretrieve(DATA_FILE_URL, DATA_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading / Generating Data\n",
    "\n",
    "dData = np.load(DATA_FILE_NAME)\n",
    "mX    = dData['mX1']\n",
    "vY    = dData['vY1']\n",
    "\n",
    "# Sample: An observation of a subject.\n",
    "# Measurement: Sample in time of the EEG signal.\n",
    "# Channel: EEG Channel.\n",
    "numSamples, numMeasurements, numChannels = mX.shape \n",
    "lLabel = ['Left Hand', 'Right Hand', 'Foot', 'Tongue'] #<! The labels\n",
    "\n",
    "print(f'The data shape: {mX.shape}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre Process Data\n",
    "\n",
    "Scale the pixels into the [0, 1] range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre Process Data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Image\n",
    "\n",
    "numPlots = numRowsPlot * numColsPlot\n",
    "\n",
    "hF, hAs = plt.subplots(nrows = numRowsPlot, ncols = numColsPlot, figsize = (18, 10))\n",
    "hAs = hAs.flat\n",
    "\n",
    "vIdx = np.random.choice(numSamples, numPlots, replace = False)\n",
    "\n",
    "for sampleIdx, hA in zip(vIdx, hAs):\n",
    "    mXX = mX[sampleIdx, :, :]\n",
    "    hA.plot(mXX)\n",
    "    hA.set_title(f'EEG Signals of Sample {sampleIdx:04d} with Label {lLabel[vY[sampleIdx]]}')\n",
    "    hA.set_xlabel('Measurement Index')\n",
    "    hA.set_ylabel('Measurement Value')\n",
    "\n",
    "hF.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of Labels\n",
    "\n",
    "hA = PlotLabelsHistogram(vY, lClass = lLabel)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> What score would you use in the case above?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 001\n",
    "\n",
    "In this model we'll use a linear combination of the features with a classifier.  \n",
    "The cross validation is _leave one out_ using `cross_val_predict()`.\n",
    "\n",
    "1. Transform data into `(numSamples, numMeasurements x numChannels)`.\n",
    "2. Apply PCA with the optimal number of components.\n",
    "3. Use a **non ensemble** classifier of your choice.\n",
    "\n",
    "Objective, above 35% accuracy in _Leave One Out_ cross validation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> What's the maximum number of components of the PCA you may use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape Data\n",
    "mXX = np.reshape(mX, (numSamples, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization Parameters\n",
    "lModels     = [DecisionTreeClassifier(criterion = 'gini', max_leaf_nodes = 12), LogisticRegression(penalty = 'l2', C = 0.5, fit_intercept = True), SVC(C = 0.9, kernel = 'rbf')] #<! We may optimize the Model Hyper Parameters as well\n",
    "lNumComp    = list(range(10, 51))\n",
    "# lNumComp    = list(range(10, 12)) #<! For fast testing\n",
    "lModelsStr  = ['Decision Tree', 'Logistic Regression', 'SVC']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Data Frame\n",
    "\n",
    "numComb = len(lModels) * len(lNumComp)\n",
    "dData   = {'Model': [], 'Number of Components': [], 'Accuracy': [0.0] * numComb}\n",
    "\n",
    "for ii, numComp in enumerate(lNumComp):\n",
    "    for jj, modelStr in enumerate(lModelsStr):\n",
    "        dData['Model'].append(modelStr)\n",
    "        dData['Number of Components'].append(numComp)\n",
    "\n",
    "dfModelScore = pd.DataFrame(data = dData)\n",
    "dfModelScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the Model\n",
    "currNumComp = 0\n",
    "\n",
    "for ii in range(numComb):\n",
    "    modelStr   = dfModelScore.loc[ii, 'Model']\n",
    "    numComp    = dfModelScore.loc[ii, 'Number of Components']\n",
    "\n",
    "    if numComp != currNumComp:\n",
    "        mZ = PCA(n_components = numComp).fit_transform(mXX)\n",
    "        currNumComp = numComp\n",
    "\n",
    "\n",
    "    print(f'Processing model {ii + 1:03d} out of {numComb} with `Model` = {modelStr} and `Number of Components` = {numComp}.')\n",
    "\n",
    "    # Very slow in Pipeline, hence cache the PCA\n",
    "    # oPipeCls = Pipeline([('PCA', PCA(n_components = numComp)), ('Classifier', lModels[lModelsStr.index(modelStr)])])\n",
    "    # vYPred = cross_val_predict(oPipeCls, mXX, vY, cv = KFold(numSamples), n_jobs = 6)\n",
    "    \n",
    "    vYPred = cross_val_predict(lModels[lModelsStr.index(modelStr)], mZ, vY, cv = KFold(numSamples), n_jobs = 6)\n",
    "\n",
    "    accScore = np.mean(vY == vYPred)\n",
    "    dfModelScore.loc[ii, 'Accuracy'] = accScore\n",
    "    print(f'Finished processing model {ii + 1:03d} with `Accuracy = {accScore:0.2%}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Sorted Results (Descending)\n",
    "\n",
    "dfModelScore.sort_values(by = ['Accuracy'], ascending = False).head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 002\n",
    "\n",
    "In this model we'll use the covariance matrix of the data as a feature.  \n",
    "Basically we want to take advantage of the interaction between the different channels.\n",
    "\n",
    "Do the following steps:\n",
    "\n",
    "1. For each sample $\\boldsymbol{X}_{i}\\in\\mathbb{R}^{1000 \\times 22}$, compute the covariance matrix $\\boldsymbol{C}_{i}\\in\\mathbb{R}^{22\\times22}$.  \n",
    "   You may use [`np.cov()`](https://numpy.org/doc/stable/reference/generated/numpy.cov.html).\n",
    "2. Set $\\boldsymbol{c}_{i}\\in\\mathbb{R}^{22^{2}}$ as the columns stack version of each $\\boldsymbol{C}_{i}\\in\\mathbb{R}^{22 \\times 22}$.  \n",
    "   You may use [`np.reshape()`](https://numpy.org/doc/stable/reference/generated/numpy.reshape.html).\n",
    "4. Build the feature matrix based on $\\boldsymbol{c}_{i}$.\n",
    "5. Analyze the separation using a non linear dimensionality reduction model (Use `vY` for the colorization of data).\n",
    "6. Use an ensemble based classifier.\n",
    "\n",
    "You should optimize the combination of the classification model and the dimensionality reduction model.\n",
    "\n",
    "Objective, above 50% accuracy in _Leave One Out_ cross validation.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> You should also try no dimensionality reduction as well.\n",
    "* <font color='brown'>(**#**)</font> If you use `t-SNE`, you may want to apply `PCA` to reduce the data into ~50 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the Features\n",
    "\n",
    "mC = np.full((numSamples, numChannels, numChannels), np.nan)\n",
    "for ii in range(numSamples):\n",
    "    mXi          = mX[ii, :, :]\n",
    "    mC[ii, :, :] = np.cov(mXi.T)\n",
    "\n",
    "\n",
    "mXX = np.reshape(mC, (numSamples, -1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Low Dimensional Model\n",
    "\n",
    "lPlotModels     = [Isomap(), TSNE()]\n",
    "lPlotModelsStr  = ['IsoMap', 't-SNE']\n",
    "\n",
    "hF, hAs = plt.subplots(nrows = 1, ncols = len(lPlotModels))\n",
    "hAs = hAs.flat\n",
    "\n",
    "for ii, (plotModel, hA) in enumerate(zip(lPlotModels, hAs)):\n",
    "    plotModel = plotModel.set_params(n_components = 2)\n",
    "    mZ = plotModel.fit_transform(mXX)\n",
    "\n",
    "    hA.scatter(mZ[:, 0], mZ[:, 1], s = 50, c = vY, edgecolor = 'k')\n",
    "    hA.set_title(lPlotModelsStr[ii])\n",
    "    hA.axis('equal')\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization Parameters\n",
    "lModels     = [LGBMClassifier(num_leaves = 15, n_estimators = 25), XGBClassifier(n_estimators = 25, max_leaves = 15)] #<! We may optimize the Model Hyper Parameters as well\n",
    "lModelsStr  = ['LightGBM', 'XGBoost']\n",
    "lDrModel    = [ColumnTransformer(transformers = [('Pass', 'passthrough', list(range(numChannels * numChannels)))]), Isomap(n_components = 2), Isomap(n_components = 10), TSNE(n_components = 2), TSNE(n_components = 10, method = 'exact')]\n",
    "lDrModelStr = ['Pass', 'Isomap02', 'Isomap10', 'TSNE02', 'TSNE10']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Data Frame\n",
    "\n",
    "numComb = len(lModels) * len(lDrModel)\n",
    "dData   = {'CLS Model': [], 'DR Model': [], 'Accuracy': [0.0] * numComb}\n",
    "\n",
    "for ii, drModelStr in enumerate(lDrModelStr):\n",
    "    for jj, clsModelStr in enumerate(lModelsStr):\n",
    "        dData['CLS Model'].append(clsModelStr)\n",
    "        dData['DR Model'].append(drModelStr)\n",
    "\n",
    "dfModelScore = pd.DataFrame(data = dData)\n",
    "dfModelScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the Model\n",
    "currDrModelStr = 'Default'\n",
    "vYPred = np.full(shape = numSamples, fill_value = -1, dtype = np.int8)\n",
    "vF = np.full(shape = numSamples, fill_value = True)\n",
    "\n",
    "for ii in range(numComb):\n",
    "    clsModelStr   = dfModelScore.loc[ii, 'CLS Model']\n",
    "    drModelStr    = dfModelScore.loc[ii, 'DR Model']\n",
    "\n",
    "    if drModelStr != currDrModelStr:\n",
    "        mZ = lDrModel[lDrModelStr.index(drModelStr)].fit_transform(mXX)\n",
    "        currDrModelStr = drModelStr\n",
    "\n",
    "    print(f'Processing model {ii + 1:03d} out of {numComb} with `CLS Model` = {clsModelStr} and `DR Model` = {drModelStr}.')\n",
    "    \n",
    "    vYPred = cross_val_predict(lModels[lModelsStr.index(clsModelStr)], mZ, vY, cv = KFold(numSamples))\n",
    "\n",
    "    # Manual Leave One Out\n",
    "    # modelCls = lModels[lModelsStr.index(clsModelStr)]\n",
    "\n",
    "    # for jj in range(numSamples):\n",
    "    #     vF[jj] = False\n",
    "    #     modelCls = modelCls.fit(mZ[vF, :], vY[vF])\n",
    "    #     vYPred[jj] = modelCls.predict(np.atleast_2d(mZ[jj]).copy())\n",
    "    #     vF[jj] = True\n",
    "\n",
    "    accScore = np.mean(vY == vYPred)\n",
    "    dfModelScore.loc[ii, 'Accuracy'] = accScore\n",
    "    print(f'Finished processing model {ii + 1:03d} with `Accuracy = {accScore:0.2%}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Sorted Results (Descending)\n",
    "\n",
    "dfModelScore.sort_values(by = ['Accuracy'], ascending = False).head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 003\n",
    "\n",
    "In this model we'll use the covariance matrix of the data as a feature.  \n",
    "Yet, we'll add a covariance matrix specific metric, the SPD Metric (`SpdMetric`):\n",
    "\n",
    "$$d\\left(\\boldsymbol{C}_{i},\\boldsymbol{C}_{j}\\right)=\\sqrt{\\sum_{i=1}^{d}\\log^{2}\\left(\\lambda_{i}\\left(\\boldsymbol{C}_{i}^{-1}\\boldsymbol{C}_{j}\\right)\\right)}$$\n",
    "\n",
    "Where ${\\lambda}_{i} \\left( \\cdot \\right)$ extract the $i$ -th eigen value of the matrix.\n",
    "\n",
    "We'll use this metric to apply a metric aware dimensionality reduction.\n",
    "\n",
    "Do the following steps:\n",
    "\n",
    "1. Implement the `SpdMetric` metric.  \n",
    "   The function `SpdMetric()` input is 2 column stacked covariance matrix $\\boldsymbol{c}_{i}$ and $\\boldsymbol{c}_{j}$.  \n",
    "   The function reshapes them back to the two matrices $\\boldsymbol{C}_{i}$ and $\\boldsymbol{C}_{j}$ and returns $d\\left(\\boldsymbol{C}_{i},\\boldsymbol{C}_{j}\\right)$.\n",
    "2. Based covariance features from _Model 002_ build a distance matrix `mD` based on the metric above.  \n",
    "   Namely `mD[ii, jj]` should be the distance between the covariance matrix of the `ii` sample to the `jj` sample.\n",
    "5. Analyze the separation using a non linear dimensionality reduction model (Use `vY` for the colorization of data).  \n",
    "   Make sure to utilize the distance function above.\n",
    "6. Use an ensemble based classifier applied on the dimensionality reduced data.\n",
    "\n",
    "You should optimize the combination of the classification model and the dimensionality reduction model.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> You may use `sp.linalg.eigvalsh(mCi, mCj)` to calculate $ \\lambda_{i}\\left(\\boldsymbol{C}_{i}^{-1}\\boldsymbol{C}_{j}\\right) $.\n",
    "\n",
    "Objective, above 75% accuracy in _Leave One Out_ cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Covariance Matrix Metric\n",
    "\n",
    "def SpdMetric(vC1: np.ndarray, vC2: np.ndarray) -> float:\n",
    "\n",
    "    numRows = int(np.sqrt(len(vC1)))\n",
    "    \n",
    "    mC1 = np.reshape(vC1, (numRows, numRows))\n",
    "    mC2 = np.reshape(vC2, (numRows, numRows))\n",
    "    \n",
    "    vλ = sp.linalg.eigvalsh(mC1, mC2)\n",
    "    \n",
    "    return np.linalg.norm(np.log(vλ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance Matrix to Affinity Matrix\n",
    "\n",
    "def ConvertDistanceMatAffinityMat(mD: np.ndarray) -> np.ndarray:\n",
    "\n",
    "    return np.exp(mD / np.std(mD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the Distance Matrix\n",
    "\n",
    "mD = np.zeros(shape = (numSamples, numSamples))\n",
    "for ii in range(numSamples):\n",
    "    vCi = mXX[ii, :]\n",
    "    for jj in range(ii):            \n",
    "        vCj        = mXX[jj, :]\n",
    "        mD[ii, jj] = SpdMetric(vCi, vCj)\n",
    "        mD[jj, ii] = mD[ii, jj] #<! Symmetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mA = ConvertDistanceMatAffinityMat(mD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Low Dimensional Model\n",
    "\n",
    "lPlotModels     = [MDS(dissimilarity = 'precomputed', normalized_stress = False), SpectralEmbedding(affinity = 'precomputed'), TSNE(metric = 'precomputed', init = 'random')]\n",
    "lPlotModelsStr  = ['MDS', 'SpectralEmbedding', 't-SNE']\n",
    "\n",
    "hF, hAs = plt.subplots(nrows = 1, ncols = len(lPlotModels), figsize = (15, 8))\n",
    "hAs = hAs.flat\n",
    "\n",
    "for ii, (plotMode, hA) in enumerate(zip(lPlotModels, hAs)):\n",
    "    plotMode.set_params(n_components = 2)\n",
    "    if lPlotModelsStr[ii] == 'SpectralEmbedding':\n",
    "        mZ = plotMode.fit_transform(mA)\n",
    "    else:\n",
    "        mZ = plotMode.fit_transform(mD)\n",
    "\n",
    "    hA.scatter(mZ[:, 0], mZ[:, 1], s = 50, c = vY, edgecolor = 'k')\n",
    "    hA.set_title(lPlotModelsStr[ii])\n",
    "    hA.axis('equal')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization Parameters\n",
    "lModels     = [SVC(kernel = 'rbf'), LGBMClassifier(num_leaves = 15, n_estimators = 25), XGBClassifier(n_estimators = 25, max_leaves = 15)] #<! We may optimize the Model Hyper Parameters as well\n",
    "lModelsStr  = ['SVC', 'LightGBM', 'XGBoost']\n",
    "lDrModel    = [MDS(n_components = 2, dissimilarity = 'precomputed', normalized_stress = False), MDS(n_components = 10, dissimilarity = 'precomputed', normalized_stress = False), TSNE(n_components = 2, metric = 'precomputed', init = 'random'), TSNE(n_components = 10, metric = 'precomputed', init = 'random', method = 'exact')]\n",
    "lDrModelStr = ['MDS02', 'MDS10', 'TSNE02', 'TSNE10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Data Frame\n",
    "\n",
    "numComb = len(lModels) * len(lDrModel)\n",
    "dData   = {'CLS Model': [], 'DR Model': [], 'Accuracy': [0.0] * numComb}\n",
    "\n",
    "for ii, drModelStr in enumerate(lDrModelStr):\n",
    "    for jj, clsModelStr in enumerate(lModelsStr):\n",
    "        dData['CLS Model'].append(clsModelStr)\n",
    "        dData['DR Model'].append(drModelStr)\n",
    "\n",
    "dfModelScore = pd.DataFrame(data = dData)\n",
    "dfModelScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the Model\n",
    "currDrModelStr = 'Default'\n",
    "vYPred = np.full(shape = numSamples, fill_value = -1, dtype = np.int8)\n",
    "vF = np.full(shape = numSamples, fill_value = True)\n",
    "\n",
    "for ii in range(numComb):\n",
    "    clsModelStr   = dfModelScore.loc[ii, 'CLS Model']\n",
    "    drModelStr    = dfModelScore.loc[ii, 'DR Model']\n",
    "\n",
    "    if drModelStr != currDrModelStr:\n",
    "        mZ = lDrModel[lDrModelStr.index(drModelStr)].fit_transform(mD)\n",
    "        currDrModelStr = drModelStr\n",
    "\n",
    "    print(f'Processing model {ii + 1:03d} out of {numComb} with `CLS Model` = {clsModelStr} and `DR Model` = {drModelStr}.')\n",
    "    \n",
    "    vYPred = cross_val_predict(lModels[lModelsStr.index(clsModelStr)], mZ, vY, cv = KFold(numSamples))\n",
    "\n",
    "    accScore = np.mean(vY == vYPred)\n",
    "    dfModelScore.loc[ii, 'Accuracy'] = accScore\n",
    "    print(f'Finished processing model {ii + 1:03d} with `Accuracy = {accScore:0.2%}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Sorted Results (Descending)\n",
    "\n",
    "dfModelScore.sort_values(by = ['Accuracy'], ascending = False).head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Why does the SVC classifier performs as good as the tree based ensembles?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
