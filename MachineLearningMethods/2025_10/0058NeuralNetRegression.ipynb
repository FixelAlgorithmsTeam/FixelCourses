{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Fixel Algorithms](https://fixelalgorithms.co/images/CCExt.png)](https://fixelalgorithms.gitlab.io)\n",
    "\n",
    "# Machine Learning Methods\n",
    "\n",
    "## Neural Network - Regression\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 1.0.000 | 03/12/2025 | Royi Avital | First version                                                      |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/AIProgram/2024_02/0040ClassifierKernelSVM.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T09:30:06.492269Z",
     "start_time": "2022-02-02T09:30:06.220934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "\n",
    "# Miscellaneous\n",
    "from platform import python_version\n",
    "import random\n",
    "\n",
    "# Typing\n",
    "from typing import Callable, Dict, List, Optional, Set, Tuple, Union\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought.\n",
    "\n",
    "Code Notations:\n",
    "\n",
    "```python\n",
    "someVar    = 2; #<! Notation for a variable\n",
    "vVector    = np.random.rand(4) #<! Notation for 1D array\n",
    "mMatrix    = np.random.rand(4, 3) #<! Notation for 2D array\n",
    "tTensor    = np.random.rand(4, 3, 2, 3) #<! Notation for nD array (Tensor)\n",
    "tuTuple    = (1, 2, 3) #<! Notation for a tuple\n",
    "lList      = [1, 2, 3] #<! Notation for a list\n",
    "dDict      = {1: 3, 2: 2, 3: 1} #<! Notation for a dictionary\n",
    "oObj       = MyClass() #<! Notation for an object\n",
    "dfData     = pd.DataFrame() #<! Notation for a data frame\n",
    "dsData     = pd.Series() #<! Notation for a series\n",
    "hObj       = plt.Axes() #<! Notation for an object / handler / function handler\n",
    "```\n",
    "\n",
    "### Code Exercise\n",
    "\n",
    " - Single line fill\n",
    "\n",
    "```python\n",
    "valToFill = ???\n",
    "```\n",
    "\n",
    " - Multi Line to Fill (At least one)\n",
    "\n",
    "```python\n",
    "# You need to start writing\n",
    "?????\n",
    "```\n",
    "\n",
    " - Section to Fill\n",
    "\n",
    "```python\n",
    "#===========================Fill This===========================#\n",
    "# 1. Explanation about what to do.\n",
    "# !! Remarks to follow / take under consideration.\n",
    "mX = ???\n",
    "\n",
    "?????\n",
    "#===============================================================#\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# %matplotlib inline\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "# Matplotlib default color palette\n",
    "lMatPltLibclr = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "# sns.set_theme() #>! Apply SeaBorn theme\n",
    "\n",
    "runInGoogleColab = 'google.colab' in str(get_ipython())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "FIG_SIZE_DEF    = (8, 8)\n",
    "ELM_SIZE_DEF    = 50\n",
    "CLASS_COLOR     = ('b', 'r')\n",
    "EDGE_COLOR      = 'k'\n",
    "MARKER_SIZE_DEF = 10\n",
    "LINE_WIDTH_DEF  = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courses Packages\n",
    "\n",
    "from DataVisualization import PlotRegressionResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Auxiliary Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron (MLP) for Regression\n",
    "\n",
    "In this exercise we'll apply the Cross Validation automatically to find the optimal hyper parameters for the model.  \n",
    "In order to achieve this we'll do a [Grid Search for Hyper Parameters Optimization](https://en.wikipedia.org/wiki/Hyperparameter_optimization).\n",
    "\n",
    "1. Load the [Fashion MNIST Data Set](https://github.com/zalandoresearch/fashion-mnist) manually (Done by the notebook).\n",
    "2. Train a baseline _Logistic Regression_ model.\n",
    "3. Find the optimal parameters of the model using Grid Search.\n",
    "4. Extract the optimal model.\n",
    "5. Plot the Confusion Matrix of the best model on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The MLP Regressor Model\n",
    "\n",
    "![](https://i.imgur.com/qNvZJX8.png)\n",
    "<!-- ![](https://i.postimg.cc/hjNXppkW/Diagrams-Multi-Layer-Perceptron-(MLP)-Regression.png) -->\n",
    "\n",
    "The above model is given by:\n",
    "\n",
    "$$\n",
    "{\\color{red}{\\boldsymbol{y}}} = {\\color{orange} \\boldsymbol{W}_{3}} \\sigma \\left( {\\color{orange} \\boldsymbol{W}_{2}} \\sigma \\left( {\\color{orange} \\boldsymbol{W}_{1}} {\\color{cyan} \\boldsymbol{x}} + {\\color{959516} \\boldsymbol{b}_{1}} \\right) + {\\color{959516} \\boldsymbol{b}_{2}} \\right)\n",
    "$$\n",
    "\n",
    "Where \n",
    " - The vector ${\\color{cyan} \\boldsymbol{x}}$ is the input layer.\n",
    " - The values ${\\color{00B050} \\boldsymbol{z}_{1}} = \\sigma \\left( {\\color{orange} \\boldsymbol{W}_{1}} {\\color{cyan} \\boldsymbol{x}} + {\\color{959516} \\boldsymbol{b}_{1}} \\right), \\; {\\color{00B050} \\boldsymbol{z}_{2}} = \\sigma \\left( {\\color{orange} \\boldsymbol{W}_{2}} {\\color{00B050} \\boldsymbol{z}_{1}} + {\\color{959516} \\boldsymbol{b}_{2}} \\right)$ are the _Hidden Layers_.\n",
    " - The function $\\sigma \\left( \\cdot \\right)$ is the _Activation Layer_.\n",
    " - The vector ${\\color{red}{\\boldsymbol{y}}}$ is the output layer.\n",
    " - $ \\left( {\\color{orange} \\boldsymbol{W}_{1}}, {\\color{959516} \\boldsymbol{b}_{1}} \\right), \\left( {\\color{orange} \\boldsymbol{W}_{2}}, {\\color{959516} \\boldsymbol{b}_{2}} \\right), \\left( {\\color{orange} \\boldsymbol{W}_{3}} \\right) $ are the _Model Parameters_.\n",
    "\n",
    "</br>\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The MLP can be scaled with more hidden layers.  \n",
    "  Commonly the 1st section expands the number of features and 2nd decreases according to the number of outputs.\n",
    "* <font color='brown'>(**#**)</font> In the case above the output layer is a single item vector. Yet it may be a vector in order to approximate a _Vector Function_.\n",
    "* <font color='brown'>(**#**)</font> In case the output should be bounded, one may use a _Sigmoid_ / _Hyperbolic Tangent_ as the last activation layer.  \n",
    "  In such case, it is common to add a bias term for the output layer as well.\n",
    "* <font color='brown'>(**#**)</font> Without using _Deep Learning_ techniques, the depth potential is limited.\n",
    "* <font color='brown'>(**#**)</font> The control over the training phase in SciKit Learn is limited. For more control one may use [Skorch](https://github.com/skorch-dev/skorch) or directly use [PyTorch](https://github.com/pytorch/pytorch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# Data\n",
    "csvFileUrl    = r'https://github.com/FixelAlgorithmsTeam/FixelCourses/raw/refs/heads/master/DataSets/UCIConcreteCompressiveStrength.csv'\n",
    "trainSetRatio = 0.85\n",
    "\n",
    "# Linear Regression (Baseline Model)\n",
    "polynomDeg = 2 #<! Baseline\n",
    "α          = 0.05\n",
    "\n",
    "# MLP Regressor\n",
    "numFold = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate / Load Data\n",
    "\n",
    "The data (Features) description:\n",
    "\n",
    "| Variable Name                 | Role    | Type       | Description | Units  | Missing Values |\n",
    "|-------------------------------|---------|------------|-------------|--------|----------------|\n",
    "| Cement                        | Feature | Continuous |             | kg/m^3 | no             |\n",
    "| Blast Furnace Slag            | Feature | Integer    |             | kg/m^3 | no             |\n",
    "| Fly Ash                       | Feature | Continuous |             | kg/m^3 | no             |\n",
    "| Water                         | Feature | Continuous |             | kg/m^3 | no             |\n",
    "| Superplasticizer              | Feature | Continuous |             | kg/m^3 | no             |\n",
    "| Coarse Aggregate              | Feature | Continuous |             | kg/m^3 | no             |\n",
    "| Fine Aggregate                | Feature | Continuous |             | kg/m^3 | no             |\n",
    "| Age                           | Feature | Integer    |             | day    | no             |\n",
    "| Concrete Compressive Strength | Target  | Continuous |             | MPa    | no             |\n",
    "\n",
    "The target variable is `Concrete Compressive Strength`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data \n",
    "\n",
    "dfData = pd.read_csv(csvFileUrl)\n",
    "\n",
    "dfData.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Summary\n",
    "\n",
    "dfData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Description\n",
    "\n",
    "dfData.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair Plot\n",
    "\n",
    "sns.pairplot(data = dfData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Matrix\n",
    "mCorr = np.abs(dfData.corr())\n",
    "\n",
    "hF, hA = plt.subplots(figsize = (6, 4))\n",
    "\n",
    "sns.heatmap(mCorr, annot = True, fmt = '0.2f', cmap = 'coolwarm', ax = hA)\n",
    "hA.xaxis.set_tick_params(rotation = 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Which feature is the most important?\n",
    "* <font color='red'>(**?**)</font> If one feature must be dropped, which one would you drop?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Data\n",
    "\n",
    "dfX = dfData.copy()\n",
    "dfX = dfX.drop(columns = ['Compressive Strength'])\n",
    "dsY = dfData['Compressive Strength'].copy()\n",
    "\n",
    "print(f'The features data shape: {dfX.shape}')\n",
    "print(f'The labels data shape: {dsY.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Validation Data Split\n",
    "\n",
    "dfXTrain, dfXVal, dsYTrain, dsYVal = train_test_split(dfX, dsY, train_size = trainSetRatio, random_state = seedNum, shuffle = True)\n",
    "\n",
    "print(f'The training features data shape  : {dfXTrain.shape}')\n",
    "print(f'The training labels data shape    : {dsYTrain.shape}')\n",
    "print(f'The validation features data shape: {dfXVal.shape}')\n",
    "print(f'The validation labels data shape  : {dsYVal.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Ridge Regression Regressor\n",
    "\n",
    "The _Ridge Regression_ model will function as the baseline regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression Linear Model\n",
    "#===========================Fill This===========================#\n",
    "# 1. Construct a baseline model pipeline user the _Hyper Parameters_ defined above:\n",
    "#   - Data Scaler: `StandardScaler()`.\n",
    "#   - Polynomial Features: `PolynomialFeatures()`.\n",
    "#   - Regressor: `Ridge()`.\n",
    "# 2. Train the model (Training set).\n",
    "# 3. Score the model using the R2 score (Validation set). Keep result in a variable named `modelScore`.\n",
    "\n",
    "# Pipeline\n",
    "\n",
    "oLinReg    = Pipeline([('DataScaler', StandardScaler()), ('PolyFeatures', PolynomialFeatures(degree = polynomDeg)), ('Regressor', Ridge(alpha= α))])\n",
    "oLinReg    = oLinReg.fit(dfXTrain, dsYTrain)\n",
    "modelScore = oLinReg.score(dfXVal, dsYVal)\n",
    "#===============================================================#\n",
    "\n",
    "print(f'The model score (R2) on the data: {modelScore:0.2f}') #<! Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Multi Layer Perceptron (MLP) Classifier\n",
    "\n",
    "This section trains an MLP model for classification.  \n",
    "Using _Grid Search_ it tunes the optimal hyper parameters of the model.  \n",
    "\n",
    "In case of the MLP the main _Hyper Parameters_ are:\n",
    "\n",
    " - Number of Hidden Layers.\n",
    " - Number of parameters in each Hidden Layer.\n",
    " - Activation Function.\n",
    " - Regularization Factor.\n",
    "\n",
    "The Grid Search requires defining:\n",
    "\n",
    "In order to use it we need to define:\n",
    " - The Model (`estimator`) - Which model is used.\n",
    " - The Parameters Grid (`param_grid`) - The set of parameter to try.\n",
    " - The Scoring (`scoring`) - The score used to define the best model.\n",
    " - The Cross Validation Iterator (`cv`) - The iteration to validate the model.\n",
    "\n",
    "\n",
    "* <font color='brown'>(**#**)</font> Pay attention to the expected run time. Using `verbose` is useful.\n",
    "* <font color='brown'>(**#**)</font> For large number of combinations, one may try [`RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) which in many is more efficient.\n",
    "* <font color='brown'>(**#**)</font> The `GridSearchCV()` is limited to one instance of an estimator.  \n",
    "  Yet using Pipelines we may test different types of estimators.\n",
    "* <font color='brown'>(**#**)</font> In production one would visualize the effect of each parameter on the model result. Then use it to fine tune farther the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Pipeline for MLP Regressor\n",
    "\n",
    "oMlpReg = Pipeline([('DataScaler', StandardScaler()), ('Regressor', MLPRegressor(solver = 'adam', max_iter = 15_000))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the Grid Search object \n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Set the parameters to iterate over and their values.\n",
    "# !! The parameters are of Pipeline format: Prefixed by the step name and a double underscore `__`.\n",
    "# !! One may use `n_jobs = 4` to speed up the search.\n",
    "dParams = {'Regressor__hidden_layer_sizes': [(40,), (20, 15), (10, 6, 4)], 'Regressor__activation': ['relu', 'logistic'], 'Regressor__alpha': [0.001, 0.01, 0.1]}\n",
    "dParams = {'Regressor__hidden_layer_sizes': [(50,), (25, 15)], 'Regressor__activation': ['relu', 'logistic'], 'Regressor__alpha': [0.001, 0.01, 0.1]}\n",
    "#===============================================================#\n",
    "\n",
    "oGsSvc = GridSearchCV(estimator = oMlpReg, param_grid = dParams, scoring = None, n_jobs = 4, cv = numFold, verbose = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> Better results can be generated with wider model with the cost of run time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameter Optimization\n",
    "# Training the model with each combination of hyper parameters.\n",
    "# Should take ~3 minute on a decent machine.\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. The model trains on the train data using Stratified K Fold cross validation.\n",
    "oGsSvc = oGsSvc.fit(dfXTrain, dsYTrain) #<! It may take few minutes\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Model\n",
    "# Extract the attributes of the best model.\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Extract the best score.\n",
    "# 2. Extract a dictionary of the parameters.\n",
    "# !! Use the attributes of the `oGsSvc` object.\n",
    "bestScore   = oGsSvc.best_score_\n",
    "dBestParams = oGsSvc.best_params_\n",
    "#===============================================================#\n",
    "\n",
    "print(f'The best model had the following parameters: {dBestParams} with the CV score: {bestScore:0.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> In production one would visualize the effect of each parameter on the model result. Then use it to fine tune farther the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Best Model\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Extract the best model.\n",
    "# 2. Score the best model on the test data set.\n",
    "oBestModel = oGsSvc.best_estimator_\n",
    "modelScore = oBestModel.score(dfXVal, dsYVal)\n",
    "#===============================================================#\n",
    "\n",
    "print(f'The model score (Accuracy) on the data: {modelScore:0.2%}') #<! Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Is the value above exactly as the value from the best model of the grid search? If so, look at the `refit` parameter of `GridSearchCV`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics / Scores\n",
    "\n",
    "This section analyzes the model using regression score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Regression Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Regression Plot\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Plot the Regression Plot for the best model.\n",
    "# Results on the Validation Set\n",
    "\n",
    "hF, hA = plt.subplots(figsize = (8, 6))\n",
    "hA = PlotRegressionResults(dsYVal.to_numpy(), oBestModel.predict(dfXVal), hA = hA)\n",
    "hA.set_title(f'Validation Set Score: {modelScore:0.2f}, CV Score: {bestScore:0.2f}');\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Explain the graph. Specifically, how can teh results of Multi Variate regression be displayed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
