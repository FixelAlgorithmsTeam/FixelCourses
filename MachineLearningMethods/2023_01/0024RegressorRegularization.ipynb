{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Fixel Algorithms](https://fixelalgorithms.co/images/CCExt.png)](https://fixelalgorithms.gitlab.io/)\n",
    "\n",
    "# Machine Learning Methods\n",
    "\n",
    "## Supervised Learning - Regression - Linear Regression with ${L}_{1}$ / ${L}_{2}$ Regularization\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 0.1.000 | 12/02/2023 | Royi Avital | First version                                                      |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/MachineLearningMethods/2023_01/0024RegressorRegularization.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T09:30:06.492269Z",
     "start_time": "2022-02-02T09:30:06.220934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.linear_model import lars_path, Lasso, lasso_path\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Miscellaneous\n",
    "import os\n",
    "from platform import python_version\n",
    "import random\n",
    "\n",
    "# Typing\n",
    "from typing import Tuple\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython\n",
    "from IPython.display import Image, display\n",
    "from ipywidgets import Dropdown, FloatSlider, interact, IntSlider, Layout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# %matplotlib inline\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "# sns.set_theme() #>! Apply SeaBorn theme\n",
    "\n",
    "runInGoogleColab = 'google.colab' in str(get_ipython())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "FIG_SIZE_DEF    = (8, 8)\n",
    "ELM_SIZE_DEF    = 50\n",
    "CLASS_COLOR     = ('b', 'r')\n",
    "EDGE_COLOR      = 'k'\n",
    "MARKER_SIZE_DEF = 10\n",
    "LINE_WIDTH_DEF  = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixel Algorithms Packages\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with LASSO (${L}_{1}$) / Ridge (${L}_{2}$) Regularization\n",
    "\n",
    "Regularization is a simple and effective way to balance the adaptation of the model to the data.  \n",
    "It allows us to use complex model yet to tune it to prevent overfit.\n",
    "\n",
    "The models of optimization:\n",
    "\n",
    " - LASSO: $\\arg \\min_{\\boldsymbol{w}} \\frac{1}{2} {\\left\\| X \\boldsymbol{w} - \\boldsymbol{y} \\right\\|}_{2}^{2} + \\lambda {\\left\\| \\boldsymbol{w} \\right\\|}_{1}$.  \n",
    "   Promotes sparsity, modeled by a Laplace prior of the coefficients of $\\boldsymbol{w}$.\n",
    " - Ridge: $\\arg \\min_{\\boldsymbol{w}} \\frac{1}{2} {\\left\\| X \\boldsymbol{w} - \\boldsymbol{y} \\right\\|}_{2}^{2} + \\lambda {\\left\\| \\boldsymbol{w} \\right\\|}_{2}^{2}$.  \n",
    "   Promotes damped coefficients, modeled by a Gaussian prior of the coefficients of $\\boldsymbol{w}$.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The LASSO model is implemented by the `Lasso` class in SciKit Learn.\n",
    "* <font color='brown'>(**#**)</font> The Ridge model is implemented by the `Ridge` class in SciKit Learn.\n",
    "* <font color='brown'>(**#**)</font> Both `Lasso` and `Ridge` (As well as `LinearRegression`) have the `positive` parameter to enforce non negative values for $\\boldsymbol{w}$.\n",
    "* <font color='brown'>(**#**)</font> The SciKit Learn classes use $\\alpha$ instead of $\\lambda$ in their optimization function.\n",
    "\n",
    "In this notebook we'll show the effect of the regularization on the fitness level of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# Data Generation\n",
    "numSamples  = 30\n",
    "noiseStd    = 17.5\n",
    "\n",
    "vP = np.array([0.5, 0.25, 2, 5])\n",
    "\n",
    "# Model\n",
    "lPolyDeg = [3, 5, 7, 9]\n",
    "lλ       = list(np.linspace(0, 25, 500)) #<! Pay attention that for λ = 0 it is better to use LinearRegression model (Numerical reasons)\n",
    "numSamplesTrain = 20\n",
    "numSamplesTest  = numSamples - numSamplesTrain\n",
    "\n",
    "# Data Visualization\n",
    "gridNoiseStd = 0.05\n",
    "numGridPts   = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary Functions\n",
    "\n",
    "def PlotRegressionData( mX: np.ndarray, vY: np.ndarray, hA:plt.Axes = None, figSize: Tuple[int, int] = FIG_SIZE_DEF, elmSize: int = ELM_SIZE_DEF, classColor: Tuple[str, str] = CLASS_COLOR, axisTitle: str = None ) -> plt.Axes:\n",
    "\n",
    "    if hA is None:\n",
    "        hF, hA = plt.subplots(figsize = figSize)\n",
    "    else:\n",
    "        hF = hA.get_figure()\n",
    "    \n",
    "    if np.ndim(mX) == 1:\n",
    "        mX = np.reshape(mX, (mX.size, 1))\n",
    "\n",
    "    numSamples = len(vY)\n",
    "    numDim     = mX.shape[1]\n",
    "    if (numDim > 2):\n",
    "        raise ValueError(f'The features data must have at most 2 dimensions')\n",
    "    \n",
    "    # Work on 1D, Add support for 2D when needed\n",
    "    # See https://matplotlib.org/stable/api/toolkits/mplot3d.html\n",
    "    hA.scatter(mX[:, 0], vY, s = elmSize, color = classColor[0], edgecolor = 'k', label = f'Samples')\n",
    "    hA.axvline(x = 0, color = 'k')\n",
    "    hA.axhline(y = 0, color = 'k')\n",
    "    hA.set_xlabel('${x}_{1}$')\n",
    "    # hA.axis('equal')\n",
    "    if axisTitle is not None:\n",
    "        hA.set_title(axisTitle)\n",
    "    hA.legend()\n",
    "    \n",
    "    return hA\n",
    "\n",
    "def PlotPolyFit( vX: np.ndarray, vY: np.ndarray, vP: np.ndarray = None, P: int = 1, numGridPts: int = 1001, hA:plt.Axes = None, figSize: Tuple[int, int] = FIG_SIZE_DEF, markerSize: int = MARKER_SIZE_DEF, lineWidth: int = LINE_WIDTH_DEF, axisTitle: str = None ):\n",
    "\n",
    "    if hA is None:\n",
    "        hF, hA = plt.subplots(1, 2, figsize = figSize)\n",
    "    else:\n",
    "        hF = hA[0].get_figure()\n",
    "\n",
    "    numSamples = len(vY)\n",
    "\n",
    "    # Polyfit\n",
    "    vW    = np.polyfit(vX, vY, P)\n",
    "    \n",
    "    # MSE\n",
    "    vHatY = np.polyval(vW, vX)\n",
    "    MSE   = (np.linalg.norm(vY - vHatY) ** 2) / numSamples\n",
    "    \n",
    "    # Plot\n",
    "    xx  = np.linspace(np.floor(np.min(vX)), np.ceil(np.max(vX)), numGridPts)\n",
    "    yy  = np.polyval(vW, xx)\n",
    "\n",
    "    hA[0].plot(vX, vY, '.r', ms = 10, label = '$y_i$')\n",
    "    hA[0].plot(xx, yy, 'b',  lw = 2,  label = '$\\hat{f}(x)$')\n",
    "    hA[0].set_title (f'$P = {P}$\\nMSE = {MSE}')\n",
    "    hA[0].set_xlabel('$x$')\n",
    "    # hA[0].axis(lAxis)\n",
    "    hA[0].grid()\n",
    "    hA[0].legend()\n",
    "    \n",
    "    hA[1].stem(vW[::-1], label = 'Estimated')\n",
    "    if vP is not None:\n",
    "        hA[1].stem(vP[::-1], linefmt = 'C1:', markerfmt = 'D', label = 'Ground Truth')\n",
    "    numTicks = len(vW) if vP is None else max(len(vW), len(vP))\n",
    "    hA[1].set_xticks(range(numTicks))\n",
    "    hA[1].set_title('Coefficients')\n",
    "    hA[1].set_xlabel('$w$')\n",
    "    hA[1].legend()\n",
    "\n",
    "    # return hA\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate / Load Data\n",
    "\n",
    "In the following we'll generate data according to the following model:\n",
    "\n",
    "$$ y_{i} = f \\left( x_{i} \\right) + \\epsilon_{i} $$\n",
    "\n",
    "Where\n",
    "\n",
    "$$ f \\left( x \\right) = \\frac{1}{2} {x}^{3} + \\frac{1}{4} {x}^{2} + 2 x + 5 $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Generating Function\n",
    "def f( vX: np.ndarray, vP: np.ndarray ):\n",
    "    \n",
    "    return np.polyval(vP, vX)\n",
    "\n",
    "\n",
    "hF = lambda vX: f(vX, vP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading / Generating Data\n",
    "\n",
    "vX = np.linspace(-5, 5, numSamples, endpoint = True) + (gridNoiseStd * np.random.randn(numSamples))\n",
    "vN = noiseStd * np.random.randn(numSamples)\n",
    "vY = hF(vX) + vN\n",
    "\n",
    "print(f'The features data shape: {vX.shape}')\n",
    "print(f'The labels data shape: {vY.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Data\n",
    "\n",
    "PlotRegressionData(vX, vY)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split\n",
    "\n",
    "In order to show the importance of the regularization, we'll apply a split on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "\n",
    "mX = np.reshape(vX, (-1, 1))\n",
    "mXTrain, mXTest, vYTrain, vYTest = train_test_split(mX, vY, test_size = numSamplesTest, train_size = numSamplesTrain, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> What does `stratify` mean in the context of splitting data for regression? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Regularized Polyfit Regressor\n",
    "\n",
    "The regularized PolyFit optimization problem is given by:\n",
    "\n",
    "$$ \\arg \\min_{\\boldsymbol{w}} {\\left\\| \\boldsymbol{X} \\boldsymbol{w} - \\boldsymbol{y} \\right|}_{2}^{2} + \\lambda R \\left( \\boldsymbol{w} \\right) $$\n",
    "\n",
    "Where\n",
    "\n",
    "$$\n",
    "\\boldsymbol{X} = \\begin{bmatrix} 1 & x_{1} & x_{1}^{2} & \\cdots & x_{1}^{p} \\\\\n",
    "1 & x_{2} & x_{2}^{2} & \\cdots & x_{2}^{p} \\\\\n",
    "\\vdots & \\vdots & \\vdots &  & \\vdots \\\\\n",
    "1 & x_{N} & x_{N}^{2} & \\cdots & x_{N}^{p}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And $R \\left( \\boldsymbol{w} \\right)$ is the regularization function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipelines\n",
    "\n",
    "# We could use the same instance of polynomial features object for both\n",
    "pPolyFitL1 = Pipeline([('PolyFeatures', PolynomialFeatures(include_bias = False)), ('Regressor', Lasso(fit_intercept = True))])\n",
    "pPolyFitL2 = Pipeline([('PolyFeatures', PolynomialFeatures(include_bias = False)), ('Regressor', Ridge(fit_intercept = True))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score Data Frame\n",
    "\n",
    "numComb = len(lPolyDeg) * len(lλ)\n",
    "dData   = {'Poly Degree': [], 'λ': [], 'Train R2 L1 Regularization': [0.0] * numComb, 'Test R2 L1 Regularization': [0.0] * numComb, 'Train R2 L2 Regularization': [0.0] * numComb, 'Test R2 L2 Regularization': [0.0] * numComb}\n",
    "\n",
    "for ii, polyDeg in enumerate(lPolyDeg):\n",
    "    for jj, paramλ in enumerate(lλ):\n",
    "        dData['Poly Degree'].append(polyDeg)\n",
    "        dData['λ'].append(paramλ)\n",
    "#===============================================================#\n",
    "\n",
    "dfModelScore = pd.DataFrame(data = dData)\n",
    "dfModelScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring the Models\n",
    "\n",
    "for ii in range(numComb):\n",
    "    polyDeg  = dfModelScore.loc[ii, 'Poly Degree']\n",
    "    paramλ   = dfModelScore.loc[ii, 'λ']\n",
    "\n",
    "    print(f'Processing model {ii + 1:03d} out of {numComb} with `degree` = {polyDeg} and `λ` = {paramλ}.')\n",
    "\n",
    "    # The `__` Trick for Pipelines\n",
    "    pPolyFitL1.set_params(PolyFeatures__degree = polyDeg)\n",
    "    pPolyFitL1.set_params(Regressor__alpha = paramλ)\n",
    "    pPolyFitL2.set_params(PolyFeatures__degree = polyDeg)\n",
    "    pPolyFitL2.set_params(Regressor__alpha = paramλ)\n",
    "\n",
    "    pPolyFitL1.fit(mXTrain, vYTrain)\n",
    "    pPolyFitL2.fit(mXTrain, vYTrain)\n",
    "\n",
    "    dfModelScore.loc[ii, 'Train R2 L1 Regularization'] = pPolyFitL1.score(mXTrain, vYTrain)\n",
    "    dfModelScore.loc[ii, 'Test R2 L1 Regularization'] = pPolyFitL1.score(mXTest, vYTest)\n",
    "    dfModelScore.loc[ii, 'Train R2 L2 Regularization'] = pPolyFitL2.score(mXTrain, vYTrain)\n",
    "    dfModelScore.loc[ii, 'Test R2 L2 Regularization'] = pPolyFitL2.score(mXTest, vYTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfModelScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numFigures = len(lPolyDeg)\n",
    "numRows    = 2\n",
    "numCols    = int(np.ceil(numFigures / numRows))\n",
    "\n",
    "hF, hA = plt.subplots(nrows = numRows, ncols = numCols, figsize = (18, 10))\n",
    "hA = hA.flat\n",
    "\n",
    "for ii, ax in enumerate(hA):\n",
    "    if ii >= len(lPolyDeg):\n",
    "        continue\n",
    "    dsIdx = dfModelScore['Poly Degree'] == lPolyDeg[ii]\n",
    "    ax.plot(lλ, dfModelScore.loc[dsIdx, 'Train R2 L1 Regularization'], color = 'C0', ls = ':', label = 'Train L1')\n",
    "    ax.plot(lλ, dfModelScore.loc[dsIdx, 'Test R2 L1 Regularization'], color = 'C0', ls = '-', label = 'Test L1')\n",
    "    ax.plot(lλ, dfModelScore.loc[dsIdx, 'Train R2 L2 Regularization'], color = 'C1', ls = ':', label = 'Train L2')\n",
    "    ax.plot(lλ, dfModelScore.loc[dsIdx, 'Test R2 L2 Regularization'], color = 'C1', ls = '-', label = 'Test L2')\n",
    "\n",
    "    # ax.set_ylim((0.45, 1.00))\n",
    "\n",
    "    ax.set_title(f'Regression with Polynomial of Degree {lPolyDeg[ii]}')\n",
    "    ax.set_xlabel('$\\lambda$')\n",
    "    ax.set_ylabel('${R}^{2}$')\n",
    "\n",
    "    ax.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> There is a regularization which use both norms (${L}_{1}$ and ${L}_{2}$) for regularization. It is called Elastic Net. See SciKit Learn's [`ElasticNet`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
