{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Fixel Algorithms](https://fixelalgorithms.co/images/CCExt.png)](https://fixelalgorithms.gitlab.io/)\n",
    "\n",
    "# Machine Learning Methods\n",
    "\n",
    "## Supervised Learning - Regression - Polynomial Fit with LASSO Regularization\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 0.1.000 | 09/02/2023 | Royi Avital | First version                                                      |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/MachineLearningMethods/2023_01/0025RegressorPolynomialFitLasso.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T09:30:06.492269Z",
     "start_time": "2022-02-02T09:30:06.220934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.linear_model import lars_path, Lasso, lasso_path\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Miscellaneous\n",
    "import os\n",
    "from platform import python_version\n",
    "import random\n",
    "\n",
    "# Typing\n",
    "from typing import Tuple\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython\n",
    "from IPython.display import Image, display\n",
    "from ipywidgets import Dropdown, FloatSlider, interact, IntSlider, Layout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "%matplotlib inline\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "# sns.set_theme() #>! Apply SeaBorn theme\n",
    "\n",
    "runInGoogleColab = 'google.colab' in str(get_ipython())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "FIG_SIZE_DEF    = (8, 8)\n",
    "ELM_SIZE_DEF    = 50\n",
    "CLASS_COLOR     = ('b', 'r')\n",
    "EDGE_COLOR      = 'k'\n",
    "MARKER_SIZE_DEF = 10\n",
    "LINE_WIDTH_DEF  = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixel Algorithms Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# Data Generation\n",
    "numSamples  = 50\n",
    "noiseStd    = 0.3\n",
    "vP = np.array([0.5, 2, 5])\n",
    "\n",
    "# Model\n",
    "polyDeg = 2\n",
    "λ       = 0.1\n",
    "\n",
    "# Data Visualization\n",
    "gridNoiseStd = 0.05\n",
    "numGridPts = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary Functions\n",
    "\n",
    "def PlotRegressionData( mX: np.ndarray, vY: np.ndarray, hA:plt.Axes = None, figSize: Tuple[int, int] = FIG_SIZE_DEF, elmSize: int = ELM_SIZE_DEF, classColor: Tuple[str, str] = CLASS_COLOR, axisTitle: str = None ) -> plt.Axes:\n",
    "\n",
    "    if hA is None:\n",
    "        hF, hA = plt.subplots(figsize = figSize)\n",
    "    else:\n",
    "        hF = hA.get_figure()\n",
    "    \n",
    "    if np.ndim(mX) == 1:\n",
    "        mX = np.reshape(mX, (mX.size, 1))\n",
    "\n",
    "    numSamples = len(vY)\n",
    "    numDim     = mX.shape[1]\n",
    "    if (numDim > 2):\n",
    "        raise ValueError(f'The features data must have at most 2 dimensions')\n",
    "    \n",
    "    # Work on 1D, Add support for 2D when needed\n",
    "    # See https://matplotlib.org/stable/api/toolkits/mplot3d.html\n",
    "    hA.scatter(mX[:, 0], vY, s = elmSize, color = classColor[0], edgecolor = 'k', label = f'Samples')\n",
    "    hA.axvline(x = 0, color = 'k')\n",
    "    hA.axhline(y = 0, color = 'k')\n",
    "    hA.set_xlabel('${x}_{1}$')\n",
    "    # hA.axis('equal')\n",
    "    if axisTitle is not None:\n",
    "        hA.set_title(axisTitle)\n",
    "    hA.legend()\n",
    "    \n",
    "    return hA\n",
    "\n",
    "\n",
    "def PlotPolyFitLasso( vX: np.ndarray, vY: np.ndarray, vP: np.ndarray = None, P: int = 1, λ: float = 0.0, numGridPts: int = 1001, hA:plt.Axes = None, figSize: Tuple[int, int] = FIG_SIZE_DEF, markerSize: int = MARKER_SIZE_DEF, lineWidth: int = LINE_WIDTH_DEF, axisTitle: str = None ):\n",
    "\n",
    "    if hA is None:\n",
    "        hF, hA = plt.subplots(1, 2, figsize = figSize)\n",
    "    else:\n",
    "        hF = hA[0].get_figure()\n",
    "\n",
    "    numSamples = len(vY)\n",
    "\n",
    "    # Polyfit\n",
    "    if λ == 0:\n",
    "        # No Lasso (Classic Polyfit)\n",
    "        vW  = np.polyfit(vX, vY, P)\n",
    "    else:\n",
    "        # Lasso\n",
    "        mX   = PolynomialFeatures(degree = P, include_bias = False).fit_transform(vX[:, None])\n",
    "        oMdl = Lasso(alpha = λ, fit_intercept = True, max_iter = 500000).fit(mX, vY)\n",
    "        # Lasso coefficients\n",
    "        vW   = np.r_[oMdl.coef_[::-1], oMdl.intercept_]\n",
    "    \n",
    "    # R2 Score\n",
    "    vHatY = np.polyval(vW, vX)\n",
    "    R2    = r2_score(vY, vHatY)\n",
    "    \n",
    "    # Plot\n",
    "    xx  = np.linspace(np.around(np.min(vX), decimals = 1) - 0.1, np.around(np.max(vX), decimals = 1) + 0.1, numGridPts)\n",
    "    yy  = np.polyval(vW, xx)\n",
    "\n",
    "    hA[0].plot(vX, vY, '.r', ms = 10, label = '$y_i$')\n",
    "    hA[0].plot(xx, yy, 'b',  lw = 2,  label = '$\\hat{f}(x)$')\n",
    "    hA[0].set_title (f'P = {P}, R2 = {R2}')\n",
    "    hA[0].set_xlabel('$x$')\n",
    "    hA[0].set_xlim(left = xx[0], right = xx[-1])\n",
    "    hA[0].set_ylim(bottom = np.floor(np.min(vY)), top = np.ceil(np.max(vY)))\n",
    "    hA[0].grid()\n",
    "    hA[0].legend()\n",
    "    \n",
    "    hA[1].stem(vW[::-1], label = 'Estimated')\n",
    "    if vP is not None:\n",
    "        hA[1].stem(vP[::-1], linefmt = 'g', markerfmt = 'gD', label = 'Ground Truth')\n",
    "    hA[1].set_title('Coefficients')\n",
    "    hA[1].set_xlabel('$w$')\n",
    "    hA[1].set_ylim(bottom = -2, top = 6)\n",
    "    hA[1].legend()\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate / Load Data\n",
    "\n",
    "In the following we'll generate data according to the following model:\n",
    "\n",
    "$$ y_{i} = f \\left( x_{i} \\right) + \\epsilon_{i} $$\n",
    "\n",
    "Where\n",
    "\n",
    "$$ f \\left( x \\right) = \\frac{1}{2} x^{2} + 2x + 5 $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Generating Function\n",
    "def f( vX: np.ndarray, vP: np.ndarray ):\n",
    "    # return 0.25 * (vX ** 2) + 2 * vX + 5\n",
    "    return np.polyval(vP, vX)\n",
    "\n",
    "\n",
    "hF = lambda vX: f(vX, vP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading / Generating Data\n",
    "\n",
    "vX = np.linspace(-2, 2, numSamples, endpoint = True) + (gridNoiseStd * np.random.randn(numSamples))\n",
    "vN = noiseStd * np.random.randn(numSamples)\n",
    "vY = hF(vX) + vN\n",
    "\n",
    "print(f'The features data shape: {vX.shape}')\n",
    "print(f'The labels data shape: {vY.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Data\n",
    "\n",
    "PlotRegressionData(vX, vY)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Polyfit Regressor with LASSO Regularization\n",
    "\n",
    "The ${L}_{1}$ regularized PolyFit optimization problem is given by:\n",
    "\n",
    "$$ \\arg \\min_{\\boldsymbol{w}} \\frac{1}{2} {\\left\\| X \\boldsymbol{w} - \\boldsymbol{y} \\right\\|}_{2}^{2} + \\lambda {\\left\\| \\boldsymbol{w} \\right\\|}_{1} $$\n",
    "\n",
    "Where\n",
    "\n",
    "$$\n",
    "\\boldsymbol{X} = \\begin{bmatrix} 1 & x_{1} & x_{1}^{2} & \\cdots & x_{1}^{p} \\\\\n",
    "1 & x_{2} & x_{2}^{2} & \\cdots & x_{2}^{p} \\\\\n",
    "\\vdots & \\vdots & \\vdots &  & \\vdots \\\\\n",
    "1 & x_{N} & x_{N}^{2} & \\cdots & x_{N}^{p}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This regularization is called _Least Absolute Shrinkage and Selection Operator_ (LASSO).  \n",
    "Since the ${L}_{1}$ norm promotes sparsity we basically have a feature selector built in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial Fit with Lasso Regularization\n",
    "\n",
    "mX         = PolynomialFeatures(degree = polyDeg, include_bias = False).fit_transform(vX[:, None]) #<! Build the model matrix\n",
    "oLinRegL1  = Lasso(alpha = λ, fit_intercept = True, max_iter = 30000).fit(mX, vY)\n",
    "vW         = np.r_[oLinRegL1.coef_[::-1], oLinRegL1.intercept_]\n",
    "vW"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Regressor for Various Regularization (λ) Values\n",
    "\n",
    "Let's see the effect of the strength of the regularization on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hPolyFitLasso = lambda λ: PlotPolyFitLasso(vX, vY, vP = vP, P = 15, λ = λ)\n",
    "lamSlider = FloatSlider(min = 0, max = 1, step = 0.001, value = 0, readout_format = '.4f', layout = Layout(width = '30%'))\n",
    "interact(hPolyFitLasso, λ = lamSlider)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> How do you expect the ${R}^{2}$ score to behave with increasing $\\lambda$?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Path for Feature Importance\n",
    "\n",
    "The _rise_ of a feature is similar to the correlation of the feature.  \n",
    "Hence we cen use the _Lasso Path_ for feature selection / significance.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The LASSO checks the conditional correlation. Namely the specific combination of the features.  \n",
    "  While selection based on correlation is based on marginal correlation. Namely the value of specific feature (Its mean or other statistics).  \n",
    "  In practice, LASSO potentially can make a good selection when there are inter correlations between the features.\n",
    "* <font color='brown'>(**#**)</font> See [Partial / Conditional Correlation vs. Marginal Correlation](https://stats.stackexchange.com/questions/77318)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data from https://gist.github.com/seankross/a412dfbd88b3db70b74b\n",
    "# mpg - Miles per Gallon\n",
    "# cyl - # of cylinders\n",
    "# disp - displacement, in cubic inches\n",
    "# hp - horsepower\n",
    "# drat - driveshaft ratio\n",
    "# wt - weight\n",
    "# qsec - 1/4 mile time; a measure of acceleration\n",
    "# vs - 'V' or straight - engine shape\n",
    "# am - transmission; auto or manual\n",
    "# gear - # of gears\n",
    "# carb - # of carburetors\n",
    "dfMpg = pd.read_csv('https://github.com/FixelAlgorithmsTeam/FixelCourses/raw/master/DataSets/mtcars.csv')\n",
    "dfMpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for Analysis\n",
    "dfX = dfMpg.drop(columns = ['model', 'mpg'], inplace = False)\n",
    "dfX = (dfX - dfX.mean()) / dfX.std() #<! Normalize\n",
    "dsY = dfMpg['mpg'].copy() #<! Data Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LASSO Path Analysis\n",
    "\n",
    "alphasPath, coefsPath, *_ = lasso_path(dfX, dsY)\n",
    "# alphasPath, coefsPath, *_ = lars_path(dfX, dsY, method = 'lasso')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hF, hA = plt.subplots(figsize = (16, 8))\n",
    "hA.plot(alphasPath, np.abs(coefsPath.T), lw = 2, label = dfX.columns.to_list())\n",
    "hA.set_title('The Lasso Path')\n",
    "hA.set_xlabel('$\\lambda$')\n",
    "hA.set_ylabel('Coefficient Value')\n",
    "hA.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
