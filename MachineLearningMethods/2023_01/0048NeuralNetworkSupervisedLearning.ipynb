{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Fixel Algorithms](https://fixelalgorithms.co/images/CCExt.png)](https://fixelalgorithms.gitlab.io/)\n",
    "\n",
    "# Machine Learning Methods\n",
    "\n",
    "## Neural Networks - Supervised Learning\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 0.1.000 | 09/03/2023 | Royi Avital | First version                                                      |\n",
    "|         |            |             |                                                                    |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/MachineLearningMethods/2023_01/0046AnomalyDetectionIsolationForest.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T09:30:06.492269Z",
     "start_time": "2022-02-02T09:30:06.220934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Miscellaneous\n",
    "import os\n",
    "import math\n",
    "from platform import python_version\n",
    "import random\n",
    "\n",
    "# Typing\n",
    "from typing import Callable, List, Tuple, Union\n",
    "\n",
    "# Visualization\n",
    "from matplotlib.colors import LogNorm, Normalize, PowerNorm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython\n",
    "from IPython.display import Image, display\n",
    "from ipywidgets import Dropdown, FloatSlider, interact, IntSlider, Layout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "%matplotlib inline\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "# sns.set_theme() #>! Apply SeaBorn theme\n",
    "\n",
    "runInGoogleColab = 'google.colab' in str(get_ipython())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "FIG_SIZE_DEF    = (8, 8)\n",
    "ELM_SIZE_DEF    = 50\n",
    "CLASS_COLOR     = ('b', 'r')\n",
    "EDGE_COLOR      = 'k'\n",
    "MARKER_SIZE_DEF = 10\n",
    "LINE_WIDTH_DEF  = 2\n",
    "\n",
    "DATA_FILE_URL = r'https://raw.githubusercontent.com/nsethi31/Kaggle-Data-Credit-Card-Fraud-Detection/master/creditcard.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixel Algorithms Packages\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning with Neural Networks\n",
    "\n",
    "The concept Neural Networks have been developed since ~1960.  \n",
    "The basic idea is chaining a basic atoms.  \n",
    "The most popular _atom_ is based on linear regression + non linear activation.  \n",
    "\n",
    "* <font color='brown'>(**#**)</font> [3Blue1Brown - But What Is a Neural Network](https://www.youtube.com/watch?v=aircAruvnKk).\n",
    "* <font color='brown'>(**#**)</font> The [TensorFlow Play Ground](https://playground.tensorflow.org/).\n",
    "* <font color='brown'>(**#**)</font> Isolation Forest is a tree based model (Ensemble).\n",
    "\n",
    "In this notebook we'll reproduce the _Hello World_ of Neural Networks: Solve the MNIST data set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disclaimer: Neural Networks  $\\ne$  Deep Learning\n",
    "\n",
    "To start off I'd like to emphasize that neural networks and deep learning are not one and the same.  \n",
    "Neural networks are a general and powerful machine learning model, while deep learning is the art of using them when they have a certain form.\n",
    "\n",
    "> **Note:** **NN** stands for Neural Network, but you will see many specific NN types:\n",
    "> * **ANN** and **DNN** are basically synonyms of NN and they stand for **Artificial Neural Networks** and **Deep Neural Networks**.\n",
    "> * **RNN** and  **CNN** stand for **Recurrent Neural Networks** and **Convolutional Neural Networks**, which are networks with specific kind of layers.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> Actually _DNN_ is for Deep Neural Networks. Deep means the architecture is very long (Many \"Nets\" stacked)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "\n",
    "As we already know from our experience with machine learning challenges, it is easier to fit (Also overfit) the model to the data when your model has many degrees of freedom.\n",
    "**Neural networks have many degrees of freedom.** In fact, they have so many degrees of freedom, they can actually fit anything, given enough data and time.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> One might want to read about the [Universal Approximation Theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# Data\n",
    "numSamplesTrain = 20_000\n",
    "numSamplesVal   = 5_000\n",
    "\n",
    "\n",
    "# Model\n",
    "lHiddenLayers   = [60, 30, 10]\n",
    "activationFun   = 'logistic'\n",
    "solverType      = 'adam'\n",
    "regFctr         = 0.0001 #<! L2 Regularization\n",
    "batchSize       = 100\n",
    "lrPolicy        = 'adaptive' #<! Works only if `solverType = sgd`\n",
    "lrInit          = 0.001 #<! Works only if `solverType = sgd` or `solverType = adam`\n",
    "numEpoch        = 100\n",
    "stopTol         = 1e-4\n",
    "earlyStopping   = True #<! Works only if `solverType = sgd` or `solverType = adam`\n",
    "valRatio        = 0.15\n",
    "\n",
    "\n",
    "# Visualization\n",
    "numRows = 3\n",
    "numCols = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary Functions\n",
    "\n",
    "def PlotMnistImages(mX: np.ndarray, vY: np.ndarray = None, numRows: int = 1, numCols: int = 1, imgSize = (28, 28), randomChoice = True, hF = None):\n",
    "\n",
    "    numSamples  = mX.shape[0]\n",
    "\n",
    "    numImg = numRows * numCols\n",
    "\n",
    "    # tFigSize = (numRows * 3, numCols * 3)\n",
    "    tFigSize = (numCols * 3, numRows * 3)\n",
    "\n",
    "    if hF is None:\n",
    "        hF, hA = plt.subplots(numRows, numCols, figsize = tFigSize)\n",
    "    else:\n",
    "        hA = hF.axis\n",
    "    \n",
    "    hA = np.atleast_1d(hA) #<! To support numImg = 1\n",
    "    hA = hA.flat\n",
    "\n",
    "    \n",
    "    for kk in range(numImg):\n",
    "        if randomChoice:\n",
    "            idx = np.random.choice(numSamples)\n",
    "        else:\n",
    "            idx = kk\n",
    "        mI  = np.reshape(mX[idx, :], imgSize)\n",
    "    \n",
    "        hA[kk].imshow(mI, cmap = 'gray')\n",
    "        hA[kk].tick_params(axis = 'both', left = False, top = False, right = False, bottom = False, labelleft = False, labeltop = False, labelright = False, labelbottom = False)\n",
    "        labelStr = f', Label = {vY[idx]}' if vY is not None else ''\n",
    "        hA[kk].set_title(f'Index = {idx}' + labelStr)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def PlotScatterData(mX: np.ndarray, vL: np.ndarray = None, hA:plt.Axes = None, figSize: Tuple[int, int] = FIG_SIZE_DEF, markerSize: int = MARKER_SIZE_DEF, edgeColor: int = EDGE_COLOR, axisTitle: str = None):\n",
    "\n",
    "    if hA is None:\n",
    "        hF, hA = plt.subplots(figsize = figSize)\n",
    "    else:\n",
    "        hF = hA.get_figure()\n",
    "    \n",
    "    numSamples = mX.shape[0]\n",
    "\n",
    "    if vL is None:\n",
    "        vL = np.zeros(numSamples)\n",
    "    \n",
    "    vU = np.unique(vL)\n",
    "    numClusters = len(vU)\n",
    "\n",
    "    for ii in range(numClusters):\n",
    "        vIdx = vL == vU[ii]\n",
    "        hA.scatter(mX[vIdx, 0], mX[vIdx, 1], s = markerSize, edgecolor = edgeColor, label = ii)\n",
    "    \n",
    "    hA.set_xlabel('${{x}}_{{1}}$')\n",
    "    hA.set_ylabel('${{x}}_{{2}}$')\n",
    "    if axisTitle is not None:\n",
    "        hA.set_title(axisTitle)\n",
    "    hA.grid()\n",
    "    hA.legend()\n",
    "\n",
    "    return hA\n",
    "\n",
    "\n",
    "def PlotLabelsHistogram(vY: np.ndarray, hA = None, lClass = None, xLabelRot: int = None) -> plt.Axes:\n",
    "\n",
    "    if hA is None:\n",
    "        hF, hA = plt.subplots(figsize = (8, 6))\n",
    "    \n",
    "    vLabels, vCounts = np.unique(vY, return_counts = True)\n",
    "\n",
    "    hA.bar(vLabels, vCounts, width = 0.9, align = 'center')\n",
    "    hA.set_title('Histogram of Classes / Labels')\n",
    "    hA.set_xlabel('Class')\n",
    "    hA.set_ylabel('Number of Samples')\n",
    "    hA.set_xticks(vLabels)\n",
    "    if lClass is not None:\n",
    "        hA.set_xticklabels(lClass)\n",
    "    \n",
    "    if xLabelRot is not None:\n",
    "        for xLabel in hA.get_xticklabels():\n",
    "            xLabel.set_rotation(xLabelRot)\n",
    "\n",
    "    return hA\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate / Load Data\n",
    "\n",
    "In this notebook we'll use the [`MNIST`](https://en.wikipedia.org/wiki/MNIST_database) data set.\n",
    "\n",
    "It features 60,000 train images and 10,000 test images of size `28x28`.  \n",
    "The images are `INT64` with values in the range `{0, 1, 2, ..., 255}` (Like `UINT8`).\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The MNIST is the data set used by Yann LeCun in the ~1990 to show the ability of Neural Networks.\n",
    "* <font color='brown'>(**#**)</font> In the [MNIST WebSite](http://yann.lecun.com/exdb/mnist/) one can watch the performance improvement over the years using different approaches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading / Generating Data\n",
    "mX, vY = fetch_openml('mnist_784', version = 1, return_X_y = True, as_frame = False, parser = 'auto')\n",
    "\n",
    "print(f'The features data shape: {mX.shape}')\n",
    "print(f'The labels data shape: {vY.shape}')\n",
    "print(f'The unique values of the labels: {np.unique(vY)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre Processing\n",
    "\n",
    "# The image is in the range {0, 1, ..., 255}\n",
    "# We scale it into [0, 1]\n",
    "\n",
    "mX = mX / 255.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Data\n",
    "\n",
    "PlotMnistImages(mX, vY, numRows, numCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of Labels\n",
    "\n",
    "hA = PlotLabelsHistogram(vY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Test Split\n",
    "\n",
    "mXTrain, mXTest, vYTrain, vYTest =  train_test_split(mX, vY, train_size = numSamplesTrain, test_size = numSamplesVal, stratify = vY)\n",
    "\n",
    "print(f'The number of training data samples: {mXTrain.shape[0]}')\n",
    "print(f'The number of training features per sample: {mXTrain.shape[1]}') \n",
    "\n",
    "\n",
    "print(f'The number of test data samples: {mXTest.shape[0]}')\n",
    "print(f'The number of test features per sample: {mXTest.shape[1]}') "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Classifier\n",
    "\n",
    "As a classifier we'll use SciKit Learn's [`MLPClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html).  \n",
    "It basically implements the Multi Layer Perceptron architecture:\n",
    "\n",
    "![](https://i.imgur.com/FATOA17.png`)\n",
    "\n",
    "\n",
    "\n",
    "Some notes about the configuration:\n",
    "\n",
    " * The model allows us to define the depth of the model and the activation layer.\n",
    " * The model allows us selecting the optimizer using `solver`.\n",
    " * The model allows us setting the scheduling policy using `learning_rate`.\n",
    " * The model has the option of _early stopping_ based on validation sub set of the data.  \n",
    "   This assists in preventing overfit of the model.\n",
    "\n",
    "\n",
    "In order to have a control over each epoch, we'll us the `partial_fit()` method to apply a single epoch each time.  \n",
    "Per epoch we'll compare the permeance of the model on the train data vs. test data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> One of the options for `activation` is `identity`, namely no activation. In case of a multi layer model, what effect will it have?\n",
    "* <font color='red'>(**?**)</font> What kind of a parameter is the number of hidden layers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the Model\n",
    "\n",
    "oMlpCls = MLPClassifier(hidden_layer_sizes = lHiddenLayers, activation = activationFun, solver = solverType, alpha = regFctr, batch_size = batchSize)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Why does the length of `lHiddenLayers` is `n_layers - 2`? What are the missing layers?\n",
    "* <font color='red'>(**?**)</font> What would happen, in the context of classification, if the test labels will have a label which is not in the train set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch Loop\n",
    "\n",
    "lTrainLoss  = [] #<! Train set loss\n",
    "lTrainScore = [] #<! Train set score\n",
    "lValScore   = [] #<! Validation set score\n",
    "\n",
    "for ii in range(numEpoch):\n",
    "    print(f'Processing epoch #{ii:03d} out of {numEpoch} epochs.')\n",
    "    \n",
    "    oMlpCls = oMlpCls.partial_fit(mXTrain, vYTrain, np.unique(vY)) #<! The method `partial_fit()` requires the classes on first call\n",
    "\n",
    "    # Accuracy Score\n",
    "    trainScore  = oMlpCls.score(mXTrain, vYTrain)\n",
    "    valScore    = oMlpCls.score(mXTest, vYTest)\n",
    "\n",
    "    lTrainLoss.append(oMlpCls.loss_)\n",
    "    lTrainScore.append(trainScore)\n",
    "    lValScore.append(valScore)\n",
    "    print(f'The train loss (Log Loss)      : {oMlpCls.loss_:0.2f}')\n",
    "    print(f'The train score (Accuracy)     : {trainScore:0.2%}')\n",
    "    print(f'The validation score (Accuracy): {valScore:0.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Results\n",
    "\n",
    "hF, hA = plt.subplots(figsize = FIG_SIZE_DEF)\n",
    "\n",
    "hA.plot(range(numEpoch), lTrainLoss, label = 'Train Loss')\n",
    "hA.plot(range(numEpoch), lTrainScore, label = 'Train Score')\n",
    "hA.plot(range(numEpoch), lValScore, label = 'Validation Score')\n",
    "\n",
    "hA.set_title('Score and Loss of the Training Loop')\n",
    "\n",
    "hA.set_xlabel('Epoch Index')\n",
    "hA.set_ylabel('Score / Loss')\n",
    "\n",
    "hA.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping\n",
    "\n",
    "Using a simple policy, called _early stopping_, one could prevent over fitting and stop the training process at the optimal phase.  \n",
    "The idea is to stop the training phase once the score of the validation stops improving.\n",
    "\n",
    "This is achieved using the `tol`, `early_stopping`, `validation_fraction` and `n_iter_no_change` parameters of the model.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The _early stopping_ technique is a regularization method to prevent over fit.\n",
    "* <font color='brown'>(**#**)</font> The _early stopping_ technique is optimal under the assumption once the policy breaks, the validation score won't ever improve. This is not always true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the Model\n",
    "# Setting `n_iter_no_change = numEpoch` to prevent early stopping.\n",
    "\n",
    "oMlpCls = MLPClassifier(hidden_layer_sizes = lHiddenLayers, activation = activationFun, solver = solverType, alpha = regFctr, batch_size = batchSize, max_iter = numEpoch, random_state = seedNum, early_stopping = earlyStopping, n_iter_no_change = numEpoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Model\n",
    "\n",
    "oMlpCls = oMlpCls.fit(mX, vY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Results\n",
    "\n",
    "hF, hA = plt.subplots(figsize = FIG_SIZE_DEF)\n",
    "\n",
    "hA.plot(range(oMlpCls.n_iter_), oMlpCls.loss_curve_, label = 'Train Loss')\n",
    "# hA.plot(range(oMlpCls.n_iter), lTrainScore, label = 'Train Score')\n",
    "hA.plot(range(oMlpCls.n_iter_), oMlpCls.validation_scores_, label = 'Validation Score')\n",
    "\n",
    "hA.set_title('Score and Loss of the Training Loop - Without Early Stopping')\n",
    "\n",
    "hA.set_xlabel('Epoch Index')\n",
    "hA.set_ylabel('Score / Loss')\n",
    "\n",
    "hA.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the Model\n",
    "# Setting `tol = 0` to prevent early stopping.\n",
    "\n",
    "oMlpCls = MLPClassifier(hidden_layer_sizes = lHiddenLayers, activation = activationFun, solver = solverType, alpha = regFctr, batch_size = batchSize, max_iter = numEpoch, random_state = seedNum, early_stopping = earlyStopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Model\n",
    "\n",
    "oMlpCls = oMlpCls.fit(mX, vY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Results\n",
    "\n",
    "hF, hA = plt.subplots(figsize = FIG_SIZE_DEF)\n",
    "\n",
    "hA.plot(range(oMlpCls.n_iter_), oMlpCls.loss_curve_, label = 'Train Loss')\n",
    "# hA.plot(range(oMlpCls.n_iter), lTrainScore, label = 'Train Score')\n",
    "hA.plot(range(oMlpCls.n_iter_), oMlpCls.validation_scores_, label = 'Validation Score')\n",
    "\n",
    "hA.set_title('Score and Loss of the Training Loop - With Early Stopping')\n",
    "\n",
    "hA.set_xlabel('Epoch Index')\n",
    "hA.set_ylabel('Score / Loss')\n",
    "\n",
    "hA.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, indeed the training stopped earlier as the score / loss stopped improving."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='green'>(**@**)</font> Manually recreate the _inference_ model using the model's `coefs_` and `intercepts_` attributes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
