{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Fixel Algorithms](https://fixelalgorithms.co/images/CCExt.png)](https://fixelalgorithms.gitlab.io/)\n",
    "\n",
    "# Machine Learning Methods\n",
    "\n",
    "## Supervised Learning - Decision Tree Classifier - Exercise Solution\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 0.1.000 | 04/02/2023 | Royi Avital | First version                                                      |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/MachineLearningMethods/2023_01/0021DecisionTreeExerciseSolution.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T09:30:06.492269Z",
     "start_time": "2022-02-02T09:30:06.220934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Miscellaneous\n",
    "import os\n",
    "from platform import python_version\n",
    "import random\n",
    "\n",
    "# Typing\n",
    "from typing import Tuple\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm, Normalize\n",
    "import seaborn as sns\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython\n",
    "from IPython.display import Image, display\n",
    "from ipywidgets import Dropdown, FloatSlider, interact, IntSlider, Layout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "%matplotlib inline\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "# sns.set_theme() #>! Apply SeaBorn theme\n",
    "\n",
    "runInGoogleColab = 'google.colab' in str(get_ipython())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "FIG_SIZE_DEF = (8, 8)\n",
    "ELM_SIZE_DEF = 50\n",
    "CLASS_COLOR = ('b', 'r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixel Algorithms Packages\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "\n",
    "In this exercise we'll use the _Decision Tree_ model as a classifier.  \n",
    "The SciKit Learn library implement it with the [`DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) class.  \n",
    "\n",
    "We'll use the [Heart Disease Data Set](https://archive.ics.uci.edu/ml/datasets/Heart+Disease) (Also known as Cleveland Heard Disease).  \n",
    "The data set contains binary and categorical features which Decision Trees excel utilizing.  \n",
    "\n",
    "The data set has the following columns:\n",
    "\n",
    "- `age`: Age in years.\n",
    "- `sex`: Sex (`1`: male; `0`: female).\n",
    "- `cp`: Chest pain type: {`0`: typical angina, `1`: atypical angina, `2`: non-anginal pain, `3`: asymptomatic}.\n",
    "- `trestbps`: Resting blood pressure (in mm Hg on admission to the hospital).\n",
    "- `chol`: Serum cholestoral in mg/dl.\n",
    "- `fbs`: Check fasting blood sugar: {`1`: Above 120 mg/dl, `0`: Below 120 mg/dl}.\n",
    "- `restecg`: Resting electrocardiographic results: {`0`: normal, `1`: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV), `2`: showing probable or definite left ventricular hypertrophy by Estes' criteria}.\n",
    "- `thalach`: Maximum heart rate achieved.\n",
    "- `exang`: Exercise induced angina: {`1`: yes, `0`: no}.\n",
    "- `oldpeak` = ST depression induced by exercise relative to rest.\n",
    "- `slope`: The slope of the peak exercise ST segment: {`0`: upsloping, `1`: flat, `2`: downsloping}.\n",
    "- `ca`: Number of major vessels (0-3) colored by flourosopy.\n",
    "- `thal`: {`0`: normal, `1`: fixed defect, `2`: reversable defect}.\n",
    "- `num`: The target variable: {`0`: `<50` (No disease), `1`: `>50_1` (disease)}.\n",
    "\n",
    "The exercise will also show the process of handling real world data: Removing invalid data, mapping values, etc...\n",
    "\n",
    "I this exercise we'll do the following:\n",
    "\n",
    "1. Load the [Heart Disease Data Set](https://archive.ics.uci.edu/ml/datasets/Heart+Disease) using `fetch_openml()` with id .\n",
    "2. Validate data.\n",
    "3. Convert text to numerical data (Though still as categorical data).\n",
    "4. Train a decision tree.\n",
    "5. Optimize the parameters: `criterion` and `max_leaf_nodes` by the `f1` score.\n",
    "6. Train the optimal model on all data.\n",
    "7. Display the Confusion Matrix and extract the different types of predictions.\n",
    "8. Show the feature importance rank of the model.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> In order to let the classifier know the data is binary / categorical we'll use a **Data Frame** as the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Set the options for the `criterion` parameter (Use all options).\n",
    "# 2. Set the options for the `max_leaf_nodes` parameter.\n",
    "lCriterion   = ['gini', 'entropy', 'log_loss']\n",
    "lMaxLeaf     = list(range(5, 11))\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary Functions\n",
    "\n",
    "def PlotLabelsHistogram(vY: np.ndarray, hA = None):\n",
    "\n",
    "    if hA is None:\n",
    "        hF, hA = plt.subplots(figsize = (8, 6))\n",
    "    \n",
    "    vLabels, vCounts = np.unique(vY, return_counts = True)\n",
    "\n",
    "    hA.bar(vLabels, vCounts, width = 0.9, align = 'center')\n",
    "    hA.set_xticks(vLabels)\n",
    "    hA.set_title('Histogram of Classes / Labels')\n",
    "    hA.set_xlabel('Class')\n",
    "    hA.set_ylabel('Number of Samples')\n",
    "\n",
    "    return hA\n",
    "\n",
    "def PlotConfusionMatrix(vY: np.ndarray, vYPred: np.ndarray, normMethod: str = None, hA: plt.Axes = None, lLabels: list = None, dScore: dict = None, titleStr: str = 'Confusion Matrix') -> plt.Axes:\n",
    "\n",
    "    # Calculation of Confusion Matrix\n",
    "    mConfMat = confusion_matrix(vY, vYPred, normalize = normMethod)\n",
    "    oConfMat = ConfusionMatrixDisplay(mConfMat, display_labels = lLabels)\n",
    "    oConfMat = oConfMat.plot(ax = hA)\n",
    "    hA = oConfMat.ax_\n",
    "    if dScore is not None:\n",
    "        titleStr += ':'\n",
    "        for scoreName, scoreVal in  dScore.items():\n",
    "            titleStr += f' {scoreName} = {scoreVal:0.2},'\n",
    "        titleStr = titleStr[:-1]\n",
    "    hA.set_title(titleStr)\n",
    "    hA.grid(False)\n",
    "\n",
    "    return hA, mConfMat\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate / Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading / Generating Data\n",
    "\n",
    "dfData, dsY = fetch_openml('heart-c', version = 1, return_X_y = True, as_frame = True, parser = 'auto')\n",
    "\n",
    "print(f'The data shape: {dfData.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Process Data\n",
    "\n",
    "In this section we'll transform the data into features which the algorithms can work with.\n",
    "\n",
    "### Remove Missing / Undefined Values\n",
    "\n",
    "There are 3 main strategies with dealing with missing values:\n",
    "1. A model to interpolate them.\n",
    "2. Remove the sample.\n",
    "3. Remove the feature.\n",
    "\n",
    "The choice between (2) and (3) depends on the occurrence of the missing values.  \n",
    "If there is a feature which is dominated by missing values, we might want to consider remove it.  \n",
    "Otherwise, we'll remove samples with missing values.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> In case of large data set we might even build different models to different combinations of features.\n",
    "* <font color='brown'>(**#**)</font> If missing values can happen in production, we need to think of a strategy that holds in that case as well.\n",
    "* <font color='brown'>(**#**)</font> In practice, another factor to take into account is the importance of the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null / NA / NaN Matrix\n",
    "\n",
    "dfData['Positive'] = dsY #<! Merge data\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Calculate the logical map of invalid values using the method `isna()`.\n",
    "dfInvData = dfData.isna() #<! The logical matrix (DF) of invalid values\n",
    "#===============================================================#\n",
    "\n",
    "hF, hA = plt.subplots(figsize = FIG_SIZE_DEF)\n",
    "sns.heatmap(data = dfInvData, square = False, ax = hA)\n",
    "hA.set_title('Invalid Data Map')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Given the results above, would you remove a feature or few samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove NaN / Null Values\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Remove the NaN / Null values. Use `dropna()`.\n",
    "# !! Choose the correct policy (Remove samples or features) by `axis`.\n",
    "dfX = dfData.dropna(axis = 0)\n",
    "#===============================================================#\n",
    "\n",
    "print(f'The features data shape: {dfX.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Duplicate Rows\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Drop duplicate rows (Samples) using the method `drop_duplicates()`.\n",
    "# 2. Reset index using the method `reset_index()` .\n",
    "dfX = dfX.drop_duplicates()\n",
    "dfX = dfX.reset_index(drop = True)\n",
    "#===============================================================#\n",
    "dfX = dfX.astype(dtype = {'ca': np.int64}) #<! It's integer mistyped as Float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split X, y\n",
    "\n",
    "dsY = dfX['Positive']\n",
    "dfX = dfX.drop(columns = ['Positive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Labels\n",
    "\n",
    "hA = PlotLabelsHistogram(dsY)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Is the data balanced?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Data into Numeric Values\n",
    "\n",
    "Some of the categorical and binary data is given by text values.  \n",
    "It is better to convert them into numerical values (Though some models can work with them as is).  \n",
    "For some visualizations, the textual data is great, hence we keep it.\n",
    "\n",
    "\n",
    "* <font color='brown'>(**#**)</font> Usually this is done as part of the pipeline. See `OneHotEncoder`, `Binarizer` and `OrdinalEncoder` in the `sklearn.preprocessing` module.\n",
    "* <font color='brown'>(**#**)</font> Currently, the implementation of `DecisionTreeClassifier` doesn't support categorical values which are not ordinal. Hence we must use `OneHotEncoder`. See https://github.com/scikit-learn/scikit-learn/pull/12866.\n",
    "* <font color='brown'>(**#**)</font> The _One Hot Encoding_ is not perfect. See [Are Categorical Variables Getting Lost in Your Random Forests](https://web.archive.org/web/20200307172925/https://roamanalytics.com/2016/10/28/are-categorical-variables-getting-lost-in-your-random-forests/) (Also [Notebook - Are Categorical Variables Getting Lost in Your Random Forests](https://notebook.community/roaminsight/roamresearch/BlogPosts/Categorical_variables_in_tree_models/categorical_variables_post)). \n",
    "* <font color='brown'>(**#**)</font> The SciKit Learn has support for categorical features in the `HistGradientBoostingClassifier` class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the Type of the Features\n",
    "dfX.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists of the Features Type\n",
    "\n",
    "lBinaryFeature  = ['sex', 'fbs', 'exang']\n",
    "lCatFeature     = ['cp', 'restecg', 'slope', 'thal']\n",
    "lNumFeature     = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'ca']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Copy (Numerical)\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Create a copy (Not a view) using the method `copy()`.\n",
    "dfXNum = dfX.copy()\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Binary Categorical Features\n",
    "# Usually this is done using `Binarizer` and `OrdinalEncoder`.\n",
    "# Yet there is a defined mapping in the data description which will be used.\n",
    "\n",
    "dSex        = {'female': 0, 'male': 1}\n",
    "dCp         = {'typ_angina': 0, 'atyp_angina': 1, 'non_anginal': 2, 'asympt': 3}\n",
    "dFbs        = {'f': 0, 't': 1}\n",
    "dRestEcg    = {'normal': 0, 'st_t_wave_abnormality': 1, 'left_vent_hyper': 2}\n",
    "dExAng      = {'no': 0, 'yes': 1}\n",
    "dSlope      = {'up': 0, 'flat': 1, 'down': 2}\n",
    "dThal       = {'normal': 0, 'fixed_defect': 1, 'reversable_defect': 2}\n",
    "\n",
    "dMapper = {'sex': dSex, 'fbs': dFbs, 'exang': dExAng, 'cp': dCp, 'restecg': dRestEcg, 'slope': dSlope, 'thal': dThal}\n",
    "\n",
    "for colName in (lBinaryFeature + lCatFeature):\n",
    "    # dMapping = dMapper[colName]\n",
    "    dfXNum[colName] = dfXNum[colName].map(dMapper[colName])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the Labels\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Create a dictionary which maps the string `<50` to 0 and `>50_1` to 1.\n",
    "# 2. Apply a mapping on `dsY` using the method `map()`.\n",
    "dMapY   = {'<50': 0, '>50_1': 1}\n",
    "dsY     = dsY.map(dMapY)\n",
    "#===============================================================#\n",
    "\n",
    "dsY = dsY.rename('Positive')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA)\n",
    "\n",
    "This is the stage we're trying to infer insights on the data using visualizations.  \n",
    "This is a skill which requires experience and creativity.  \n",
    "\n",
    "We'll do some very basic operations for this data set.  \n",
    "We'll see the distribution of each feature for the 2 values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Data\n",
    "\n",
    "numFeatures = len(lBinaryFeature)\n",
    "hF, hA = plt.subplots(1, numFeatures, figsize = (24, 8))\n",
    "hA = hA.flat\n",
    "\n",
    "for ii, colName in enumerate(lBinaryFeature):\n",
    "    sns.histplot(data = dfX, x = colName, hue = dsY, discrete = True, multiple = 'dodge', ax = hA[ii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical Data\n",
    "\n",
    "numFeatures = len(lCatFeature)\n",
    "hF, hA = plt.subplots(1, numFeatures, figsize = (24, 8))\n",
    "hA = hA.flat\n",
    "\n",
    "for ii, colName in enumerate(lCatFeature):\n",
    "    sns.histplot(data = dfX, x = colName, hue = dsY, discrete = True, multiple = 'dodge', ax = hA[ii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical Data\n",
    "\n",
    "lDiscreteData = []\n",
    "\n",
    "numFeatures = len(lNumFeature)\n",
    "hF, hA = plt.subplots(1, numFeatures, figsize = (24, 8))\n",
    "hA = hA.flat\n",
    "\n",
    "for ii, colName in enumerate(lNumFeature):\n",
    "    # if pd.api.types.is_integer_dtype(dfX[colName]):\n",
    "    #     sns.histplot(data = dfX, x = colName, hue = dsY, discrete = True, multiple = 'dodge', ax = hA[ii])\n",
    "    if colName == 'ca':\n",
    "        sns.histplot(data = dfX, x = colName, hue = dsY, discrete = True, multiple = 'dodge', ax = hA[ii])\n",
    "    else:\n",
    "        sns.kdeplot(data = dfX, x = colName, hue = dsY, fill = True, common_norm = True, ax = hA[ii])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> How would you handle the case where a feature has a single value? Look at [`VarianceThreshold`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html#sklearn.feature_selection.VarianceThreshold).\n",
    "* <font color='brown'>(**#**)</font> Usually part of the work on feature includes a process to select the best of them. For example a brute force method is given by [`SelectKBest`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Decision Tree Model and Optimize Hyper Parameters\n",
    "\n",
    "In this section we'll optimize the model according to the `F1` score.  \n",
    "The `F1` score is the geometric mean of the _precision_ and _recall_.  \n",
    "Hence it can handle pretty well imbalanced data as well (Though this case is not really that imbalanced).\n",
    "\n",
    "We'll use the `f1_score()` function to calculate the measure.  \n",
    "The process to optimize the _Hyper Parameters_ will be as following:\n",
    "\n",
    "1. Build a data frame to keep the scoring of the different hyper parameters combination.\n",
    "2. Optimize the model:\n",
    "  - Construct a model using the current combination of _hyper parameters_.\n",
    "  - Apply a cross validation process to predict the data using `cross_val_predict()`.\n",
    "  - As the cross validation iterator (The `cv` parameter) use `KFold` to implement _Leave One Out_ policy.\n",
    "3. Calculate the `F1` score of the predicted classes.\n",
    "4. Store the result in the performance data frame.\n",
    "\n",
    "\n",
    "* <font color='brown'>(**#**)</font> Pay attention that while we optimize the hyper parameters according to the `F1` score, the model itself has a different _loss_ function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Data Frame\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Calculate the number of combinations.\n",
    "# 2. Create a nested loop to create the combinations between the parameters.\n",
    "# 3. Store the combinations as the columns of a data frame.\n",
    "\n",
    "# For Advanced Python users: Use iteration tools for create the cartesian product\n",
    "numComb = len(lCriterion) * len(lMaxLeaf)\n",
    "dData   = {'Criterion': [], 'Max Leaves': [], 'F1': [0.0] * numComb}\n",
    "\n",
    "for ii, paramCriteria in enumerate(lCriterion):\n",
    "    for jj, maxLeaf in enumerate(lMaxLeaf):\n",
    "        dData['Criterion'].append(paramCriteria)\n",
    "        dData['Max Leaves'].append(maxLeaf)\n",
    "#===============================================================#\n",
    "\n",
    "dfModelScore = pd.DataFrame(data = dData)\n",
    "dfModelScore\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the Model\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Iterate over each row of the data frame `dfModelScore`. Each row defines the hyper parameters.\n",
    "# 2. Construct the model.\n",
    "# 3. Train it on the Train Data Set.\n",
    "# 4. Calculate the score.\n",
    "# 5. Store the score into the data frame column.\n",
    "\n",
    "for ii in range(numComb):\n",
    "    paramCriteria    = dfModelScore.loc[ii, 'Criterion']\n",
    "    maxLeaf          = dfModelScore.loc[ii, 'Max Leaves']\n",
    "\n",
    "    print(f'Processing model {ii + 1:03d} out of {numComb} with `criterion` = {paramCriteria} and `max_leaf_nodes` = {maxLeaf}.')\n",
    "\n",
    "    oDecTreeCls = DecisionTreeClassifier(criterion = paramCriteria, max_leaf_nodes = maxLeaf)\n",
    "    \n",
    "    vYPred = cross_val_predict(oDecTreeCls, dfXNum, dsY, cv = KFold(n_splits = dfXNum.shape[0]))\n",
    "\n",
    "    f1Score = f1_score(dsY, vYPred)\n",
    "    dfModelScore.loc[ii, 'F1'] = f1Score\n",
    "    print(f'Finished processing model {ii + 1:03d} with `f1 = {f1Score}.')\n",
    "#===============================================================#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Should we use _Stratified K Fold_ above? Why?\n",
    "* <font color='brown'>(**#**)</font> Pay attention to the speed of the process. This is one of the main advantages of models based on trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Sorted Results (Descending)\n",
    "# Pandas allows sorting data by any column using the `sort_values()` method\n",
    "# The `head()` allows us to see only the the first values\n",
    "dfModelScore.sort_values(by = ['F1'], ascending = False).head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Do you find results satisfactory? How would you answer this?  \n",
    "   See the _Code 001_ below.\n",
    "* <font color='green'>(**@**)</font> Since the class `DecisionTreeClassifier` doesn't really support categorical features, you may use _One Hot Encoding_.  \n",
    "   See [Pandas' `get_dummies()`](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html). You may use _Code 002_ below.\n",
    "\n",
    "\n",
    "```python\n",
    "# Code 001\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "oCls = LogisticRegression(max_iter = 10_000)\n",
    "oCls = oCls.fit(dfXNum, dsY)\n",
    "f1_score(dsY, oCls.predict(dfXNum))\n",
    "```\n",
    "\n",
    "```python\n",
    "# Code 002\n",
    "dfXCat = dfXNum.copy()\n",
    "\n",
    "for colName in lCatFeature:\n",
    "    dfTmp = pd.get_dummies(dfXNum[colName], prefix = colName)\n",
    "    dfXCat = pd.concat((dfXCat, dfTmp), axis = 1)\n",
    "    dfXCat = dfXCat.drop(columns = colName)\n",
    "\n",
    "dfXCat\n",
    "```\n",
    "\n",
    "\n",
    "<!-- from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "oCls = HistGradientBoostingClassifier(max_iter = 100, max_leaf_nodes = 20, categorical_features = lBinaryFeature + lCatFeature)\n",
    "oCls = oCls.fit(dfXNum, dsY)\n",
    "f1_score(dsY, oCls.predict(dfXNum)) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Train Data F1 as a Heat Map\n",
    "# We can pivot the data set created to have a 2D matrix of the `F1` as a function of `Criterion` and the `Max Leaves`.\n",
    "\n",
    "hA = sns.heatmap(data = dfModelScore.pivot(index = 'Max Leaves', columns = 'Criterion', values = 'F1'), robust = True, linewidths = 1, annot = True, fmt = '0.2%', norm = LogNorm())\n",
    "hA.set_title('F1 of the Cross Validation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Optimal Hyper Parameters\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Extract the index of row which maximizes the score.\n",
    "# 2. Use the index of the row to extract the hyper parameters which were optimized.\n",
    "\n",
    "#! You may find the `idxmax()` method of a Pandas data frame useful.\n",
    "idxArgMax = dfModelScore['F1'].idxmax()\n",
    "#===============================================================#\n",
    "\n",
    "optimalCriterion = dfModelScore.loc[idxArgMax, 'Criterion']\n",
    "optimalMaxLeaf   = dfModelScore.loc[idxArgMax, 'Max Leaves']\n",
    "\n",
    "print(f'The optimal hyper parameters are: `criterion` = {optimalCriterion}, `max_leaf_nodes` = {optimalMaxLeaf}')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Model\n",
    "\n",
    "In this section we'll extract the best model an retrain it on the whole data (`dfXNum`).  \n",
    "We need to export the model which has the best Test values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the Optimal Model & Train on the Whole Data\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Construct the model with the optimal hyper parameters.\n",
    "# 2. Fit the model on the whole data set.\n",
    "oDecTreeCls = DecisionTreeClassifier(criterion = optimalCriterion, max_leaf_nodes = optimalMaxLeaf)\n",
    "oDecTreeCls = oDecTreeCls.fit(dfXNum, dsY)\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Score (Accuracy)\n",
    "\n",
    "print(f'The model score (Accuracy) is: {oDecTreeCls.score(dfXNum, dsY):0.2%}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Confusion Matrix\n",
    "hF, hA = plt.subplots(figsize = (10, 10))\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "hA, mConfMat = PlotConfusionMatrix(dsY, oDecTreeCls.predict(dfXNum), hA = hA)\n",
    "#===============================================================#\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Calculate the TP, TN, FP and FN rates. \n",
    "* <font color='red'>(**?**)</font> Calculate the _precision_ and _recall_.\n",
    "* <font color='red'>(**?**)</font> Calculate the _precision_ and _recall_ assuming the labels `0` is the positive label."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Significance\n",
    "\n",
    "One advantage of the decision tree based models is having access to the significance of each feature during training.  \n",
    "We can access it using the `feature_importances_` attribute (Only after a applying training by the `fit()` method).\n",
    "\n",
    "* <font color='brown'>(**#**)</font> This ability is useful as a pre processing of data for any model with no restriction to trees.\n",
    "* <font color='brown'>(**#**)</font> The idea is measuring the total contribution of the feature to the reduction in loss.\n",
    "* <font color='brown'>(**#**)</font> This is a good metric for importance mainly for categorical features. For features with high number of unique values (Continuous), it might be not as accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Importance of the Features\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# Extract the feature importance using the `feature_importances_` attribute\n",
    "vFeatImportance = oDecTreeCls.feature_importances_\n",
    "#===============================================================#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature importance is normalized, hence we can display it like a discrete probability mass function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Feature Importance\n",
    "\n",
    "hF, hA = plt.subplots(figsize = (16, 8))\n",
    "hA.bar(x = dfXNum.columns, height = vFeatImportance)\n",
    "hA.set_title('Features Importance of the Model')\n",
    "hA.set_xlabel('Feature Name')\n",
    "hA.set_title('Importance')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> How many non zero values could we have? Look at the number of splits.\n",
    "* <font color='red'>(**?**)</font> What can be done with the features with low value?\n",
    "* <font color='brown'>(**#**)</font> Can you explain what you see with the EDA phase plots?\n",
    "* <font color='brown'>(**#**)</font> Pay attention, in the context of feature importance we may choose high number of splits even if it means overfit. It won't be a model for production, but will give a better view of the features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "2e25f61d437a570f4a5ebab9620676b76d9d78268156eb24f90e74ea13ca7ad4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
