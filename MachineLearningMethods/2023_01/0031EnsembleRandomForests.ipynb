{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Fixel Algorithms](https://fixelalgorithms.co/images/CCExt.png)](https://fixelalgorithms.gitlab.io/)\n",
    "\n",
    "# Machine Learning Methods\n",
    "\n",
    "## Supervised Learning - Ensemble Methods - Random Forests\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 0.1.000 | 19/02/2023 | Royi Avital | First version                                                      |\n",
    "|         |            |             |                                                                    |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/MachineLearningMethods/2023_01/0031EnsembleRandomForests.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T09:30:06.492269Z",
     "start_time": "2022-02-02T09:30:06.220934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Miscellaneous\n",
    "import os\n",
    "from platform import python_version\n",
    "import random\n",
    "\n",
    "# Typing\n",
    "from typing import Callable, List, Tuple\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm, Normalize\n",
    "import seaborn as sns\n",
    "# from bokeh.plotting import figure, show\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython\n",
    "from IPython.display import Image, display\n",
    "from ipywidgets import Dropdown, FloatSlider, interact, IntSlider, Layout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "%matplotlib inline\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "# sns.set_theme() #>! Apply SeaBorn theme\n",
    "\n",
    "runInGoogleColab = 'google.colab' in str(get_ipython())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "FIG_SIZE_DEF    = (8, 8)\n",
    "ELM_SIZE_DEF    = 50\n",
    "CLASS_COLOR     = ('b', 'r')\n",
    "EDGE_COLOR      = 'k'\n",
    "MARKER_SIZE_DEF = 10\n",
    "LINE_WIDTH_DEF  = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixel Algorithms Packages\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests Classification\n",
    "\n",
    "In this note book we'll use the Random Forests based classifier in the task of estimating whether a passenger on the Titanic will or will not survive.  \n",
    "We'll focus on basic pre processing of the data and analyzing the importance of features using the classifier.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> This is a very popular data set for classification. You may have a look for notebooks on teh need with a deeper analysis of the data set itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# Data\n",
    "trainRatio = 0.75\n",
    "\n",
    "# Model\n",
    "numEst = 150\n",
    "spliCrit = 'gini'\n",
    "maxLeafNodes = 20\n",
    "outBagScore = True\n",
    "\n",
    "# Feature Permutation\n",
    "numRepeats = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary Functions\n",
    "\n",
    "def PlotRegResults( vY, vYPred, hA:plt.Axes = None, figSize: Tuple[int, int] = FIG_SIZE_DEF, lineWidth: int = LINE_WIDTH_DEF, elmSize: int = ELM_SIZE_DEF, classColor: Tuple[str, str] = CLASS_COLOR, axisTitle: str = None ) -> plt.Axes:\n",
    "\n",
    "    if hA is None:\n",
    "        hF, hA = plt.subplots(figsize = figSize)\n",
    "    else:\n",
    "        hF = hA.get_figure()\n",
    "\n",
    "    numSamples = len(vY)\n",
    "    if (numSamples != len(vYPred)):\n",
    "        raise ValueError(f'The inputs `vY` and `vYPred` must have the same number of elements')\n",
    "    \n",
    "    \n",
    "    hA.plot(vY, vY, color = 'r', lw = lineWidth, label = 'Ground Truth')\n",
    "    hA.scatter(vY, vYPred, s = elmSize, color = classColor[0], edgecolor = 'k', label = f'Estimation')\n",
    "    hA.set_xlabel('Label Value')\n",
    "    hA.set_ylabel('Prediction Value')\n",
    "    # hA.axis('equal')\n",
    "    if axisTitle is not None:\n",
    "        hA.set_title(axisTitle)\n",
    "    hA.legend()\n",
    "    \n",
    "    return hA\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate / Load Data\n",
    "\n",
    "In this section we'll load the data.  \n",
    "We'll create a Data Frame of the data and later will separate it into features and labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading / Generating Data\n",
    "\n",
    "dfX, dsY = fetch_openml('titanic', version = 1, return_X_y = True, as_frame = True, parser = 'auto')\n",
    "\n",
    "print(f'The data shape: {dfX.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfX.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping Features\n",
    "# We'll drop the `name` and `ticket` features (Avoid 1:1 identification)\n",
    "\n",
    "dfX = dfX.drop(columns = ['name', 'ticket'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> Pay attention that we dropped the features, but a deeper analysis could extract information from them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsY.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Processing and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfData = pd.concat([dfX, dsY], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null / NA / NaN Matrix\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Calculate the logical map of invalid values using the method `isna()`.\n",
    "mInvData = dfData.isna() #<! The logical matrix of invalid values\n",
    "#===============================================================#\n",
    "\n",
    "hF, hA = plt.subplots(figsize = FIG_SIZE_DEF)\n",
    "sns.heatmap(data = mInvData, square = False, ax = hA)\n",
    "hA.set_title('Invalid Data Map')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the features `cabin`, `boat`, `body` and `home.dest` have mostly non valid value we'll drop them as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Features with Invalid Values\n",
    "\n",
    "dfData = dfData.drop(columns = ['cabin', 'boat', 'body', 'home.dest'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Visualization\n",
    "# Basic EDA on the Data\n",
    "\n",
    "numCol = dfData.shape[1]\n",
    "lCols  = dfData.columns\n",
    "numAx  = int(np.ceil(np.sqrt(numCol)))\n",
    "\n",
    "hIsCatLikData = lambda dsX: (pd.api.types.is_categorical_dtype(dsX) or pd.api.types.is_bool_dtype(dsX) or pd.api.types.is_object_dtype(dsX) or pd.api.types.is_integer_dtype(dsX))\n",
    "\n",
    "hF, hAs = plt.subplots(nrows = numAx, ncols = numAx, figsize = (16, 12))\n",
    "hAs = hAs.flat\n",
    "\n",
    "for ii in range(numCol):\n",
    "    colName = dfData.columns[ii]\n",
    "    if hIsCatLikData(dfData[colName]):\n",
    "        sns.histplot(data = dfData, x = colName, hue = 'survived', stat = 'count', discrete = True, common_norm = True, multiple = 'dodge', ax = hAs[ii])\n",
    "    else:\n",
    "        sns.kdeplot(data = dfData, x = colName, hue = 'survived', fill = True, common_norm = True, ax = hAs[ii])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Have a look on the features and try to estimate their importance for the estimation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling Missing Data\n",
    "\n",
    "For the rest of the missing values we'll use a simple method of interpolation:\n",
    "\n",
    " - Categorical Data: Using the mode value.\n",
    " - Numeric Data: Using the median / mean.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> We could employ much more delicate and sophisticated data. For instance, use the mean value of the same `pclass`. Namely profiling the data by other features to interpolate the missing feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Value by Dictionary\n",
    "dNaNs = {'embarked': dfData['embarked'].mode()[0], 'age': dfData['age'].median(), 'fare': dfData['fare'].median()}\n",
    "\n",
    "dfData = dfData.fillna(value = dNaNs, inplace = False) #<! We can use the `inplace` for efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null / NA / NaN Matrix\n",
    "\n",
    "mInvData = dfData.isna() #<! The logical matrix of invalid values\n",
    "\n",
    "hF, hA = plt.subplots(figsize = FIG_SIZE_DEF)\n",
    "sns.heatmap(data = mInvData, square = False, ax = hA)\n",
    "hA.set_title('Invalid Data Map')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion of Categorical Data\n",
    "\n",
    "In this notebook we'll use the `RandomForestClassifier` implementation of Random Forests.  \n",
    "At the moment, it doesn't support Categorical Data, hence we'll use Dummy Variables (One Hot Encoding).\n",
    "\n",
    "* <font color='brown'>(**#**)</font> Pay attention that one hot encoding is an inferior approach to having a real support for categorical data.\n",
    "* <font color='brown'>(**#**)</font> For 2 values categorical data (Binary feature) there is no need fo any special support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features Encoding\n",
    "# 1. The feature 'embarked' -> One Hot Encoding.\n",
    "# 1. The feature 'sex' -> Mapping (Female -> 0, Male -> 1).\n",
    "dfData = pd.get_dummies(dfData, columns = ['embarked'], drop_first = False)\n",
    "dfData['sex'] = dfData['sex'].map({'female': 0, 'male': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Data Type\n",
    "dfData = dfData.astype(dtype = {'pclass': np.uint8, 'sex': np.uint8, 'sibsp': np.uint8, 'parch': np.uint8, 'survived': np.uint8})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfX = dfData.drop(columns = ['survived'])\n",
    "dsY = dfData['survived']\n",
    "\n",
    "print(f'The features data shape: {dfX.shape}')\n",
    "print(f'The labels data shape: {dsY.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Data\n",
    "\n",
    "dfXTrain, dfXTest, dsYTrain, dsYTest = train_test_split(dfX, dsY, train_size = trainRatio, random_state = seedNum, shuffle = True, stratify = dsY)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Random Forests Model\n",
    "\n",
    "The random forests models basically creates weak classifiers by limiting their access to the data and features.  \n",
    "This basically also limits their correlation which means we can use their mean in order to reduce the variance of the estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the Model & Train\n",
    "\n",
    "oRndForestsCls = RandomForestClassifier(n_estimators = numEst, criterion = spliCrit, max_leaf_nodes = maxLeafNodes, oob_score = outBagScore)\n",
    "oRndForestsCls = oRndForestsCls.fit(dfXTrain, dsYTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores of the Model\n",
    "\n",
    "print(f'The train accuracy     : {oRndForestsCls.score(dfXTrain, dsYTrain):0.2%}')\n",
    "print(f'The out of bag accuracy: {oRndForestsCls.oob_score_:0.2%}')\n",
    "print(f'The test accuracy      : {oRndForestsCls.score(dfXTest, dsYTest):0.2%}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='blue'>(**!**)</font> Try different values for the model's hyper parameter (Try defaults as well)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contribution to Impurity Decrease\n",
    "\n",
    "This is the default method for feature importance of decision trees based methods.  \n",
    "It basically sums the amount of impurity reduced by splits by each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Feature Importance\n",
    "vFeatImp = oRndForestsCls.feature_importances_\n",
    "vIdxSort = np.argsort(vFeatImp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Feature Importance\n",
    "hF, hA = plt.subplots(figsize = (16, 8))\n",
    "hA.bar(x = dfXTrain.columns[vIdxSort], height = vFeatImp[vIdxSort])\n",
    "hA.set_title('Features Importance of the Model')\n",
    "hA.set_xlabel('Feature Name')\n",
    "hA.set_title('Importance')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Do we need all 3: `embarked_C`, `embarked_Q` and `embarked_S`? Look at the options of `get_dummies()`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation Effect\n",
    "\n",
    "This is a more general method to measure the importance of a feature.  \n",
    "We basically replace the values with \"noise\" to see how much performance has been deteriorated.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> This is highly time consuming operation. Hence the speed of decision trees based methods creates a synergy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Permutations\n",
    "oFeatImpPermTrain = permutation_importance(oRndForestsCls, dfXTrain, dsYTrain, n_repeats = numRepeats)\n",
    "oFeatImpPermTest  = permutation_importance(oRndForestsCls, dfXTest, dsYTest, n_repeats = numRepeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Data Frame\n",
    "\n",
    "dT = {'Feature': [], 'Importance': [], 'Data': []}\n",
    "\n",
    "for (dataName, mScore) in [('Train', oFeatImpPermTrain.importances), ('Test', oFeatImpPermTest.importances)]:\n",
    "    for ii, featName in enumerate(dfX.columns):\n",
    "        for jj in range(numRepeats):\n",
    "            dT['Feature'].append(featName)\n",
    "            dT['Importance'].append(mScore[ii, jj])\n",
    "            dT['Data'].append(dataName)\n",
    "\n",
    "dfFeatImpPerm = pd.DataFrame(dT)\n",
    "dfFeatImpPerm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Results\n",
    "hF, hA = plt.subplots(figsize = (12, 8))\n",
    "sns.boxplot(data = dfFeatImpPerm, x = 'Feature', y = 'Importance', hue = 'Data', ax = hA)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='green'>(**@**)</font> Try extracting better results by using the dropped features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
