{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Fixel Algorithms](https://fixelalgorithms.co/images/CCExt.png)](https://fixelalgorithms.gitlab.io/)\n",
    "\n",
    "# Machine Learning Methods\n",
    "\n",
    "## Supervised Learning - Regression - Kernel Regression - Exercise Solution\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 0.1.000 | 18/02/2023 | Royi Avital | First version                                                      |\n",
    "|         |            |             |                                                                    |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/MachineLearningMethods/2023_01/0028RegressorKernelExerciseSolution.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T09:30:06.492269Z",
     "start_time": "2022-02-02T09:30:06.220934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Miscellaneous\n",
    "import os\n",
    "from platform import python_version\n",
    "import random\n",
    "\n",
    "# Typing\n",
    "from typing import Callable, List, Tuple\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm, Normalize\n",
    "import seaborn as sns\n",
    "# from bokeh.plotting import figure, show\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython\n",
    "from IPython.display import Image, display\n",
    "from ipywidgets import Dropdown, FloatSlider, interact, IntSlider, Layout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "%matplotlib inline\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "# sns.set_theme() #>! Apply SeaBorn theme\n",
    "\n",
    "runInGoogleColab = 'google.colab' in str(get_ipython())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "FIG_SIZE_DEF    = (8, 8)\n",
    "ELM_SIZE_DEF    = 50\n",
    "CLASS_COLOR     = ('b', 'r')\n",
    "EDGE_COLOR      = 'k'\n",
    "MARKER_SIZE_DEF = 10\n",
    "LINE_WIDTH_DEF  = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixel Algorithms Packages\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Regression\n",
    "\n",
    "In this exercise we'll build an estimator with the Sci Kit Learn API.  \n",
    "It will be based on the concept of Kernel Regression.\n",
    "\n",
    "\n",
    "We'll us the [Boston House Prices Dataset](https://www.openml.org/search?type=data&status=active&id=531) (See also [Kaggle - Boston House Prices](https://www.kaggle.com/datasets/vikrishnan/boston-house-prices)).  \n",
    "It has 13 features and one target. 2 of the features are categorical features.\n",
    "\n",
    "The objective is to estimate the `MDEV` of the estimation by optimizing the following hyper parameters:\n",
    " - The type of the kernel\n",
    " - The `h` parameter.\n",
    "\n",
    "I this exercise we'll do the following:\n",
    "\n",
    "1. Load the `Boston House Prices Dataset` data set using `fetch_openml()`.\n",
    "2. Create a an estimator (Regressor) class using SciKit API:\n",
    "  - Implement the constructor.\n",
    "  - Implement the `fit()`, `predict()` and `score()` methods.\n",
    "3. Optimize hyper parameters using _Leave One Out_ cross validation.\n",
    "4. Display the output of the model.\n",
    "\n",
    "We should get an _R2_ score above 0.75.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> In order to set the `h` parameter, one should have a look on the distance matrix of the data to get the relevant Dynamic Range of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "lKernelType = ['Cosine', 'Gaussian', 'Triangular', 'Uniform']\n",
    "#===========================Fill This===========================#\n",
    "# 1. Set the range of values of `h` (Bandwidth).\n",
    "lH          = list(np.linspace(0.1, 5, 40))\n",
    "#===============================================================#\n",
    "\n",
    "# Data Visualization\n",
    "gridNoiseStd = 0.05\n",
    "numGridPts = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary Functions\n",
    "\n",
    "def PlotRegResults( vY, vYPred, hA:plt.Axes = None, figSize: Tuple[int, int] = FIG_SIZE_DEF, lineWidth: int = LINE_WIDTH_DEF, elmSize: int = ELM_SIZE_DEF, classColor: Tuple[str, str] = CLASS_COLOR, axisTitle: str = None ) -> plt.Axes:\n",
    "\n",
    "    if hA is None:\n",
    "        hF, hA = plt.subplots(figsize = figSize)\n",
    "    else:\n",
    "        hF = hA.get_figure()\n",
    "\n",
    "    numSamples = len(vY)\n",
    "    if (numSamples != len(vYPred)):\n",
    "        raise ValueError(f'The inputs `vY` and `vYPred` must have the same number of elements')\n",
    "    \n",
    "    \n",
    "    hA.plot(vY, vY, color = 'r', lw = lineWidth, label = 'Ground Truth')\n",
    "    hA.scatter(vY, vYPred, s = elmSize, color = classColor[0], edgecolor = 'k', label = f'Estimation')\n",
    "    hA.set_xlabel('Label Value')\n",
    "    hA.set_ylabel('Prediction Value')\n",
    "    # hA.axis('equal')\n",
    "    if axisTitle is not None:\n",
    "        hA.set_title(axisTitle)\n",
    "    hA.legend()\n",
    "    \n",
    "    return hA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Kernels\n",
    "\n",
    "def CosineKernel( vU: np.ndarray ):\n",
    "    return (np.abs(vU) < 1) * (1 + np.cos(np.pi * vU))\n",
    "\n",
    "def GaussianKernel( vU: np.ndarray ):\n",
    "    return np.exp(-0.5 * np.square(vU))\n",
    "\n",
    "def TriangularKernel( vU: np.ndarray ):\n",
    "    return (np.abs(vU) < 1) * (1 - np.abs(vU))\n",
    "\n",
    "def UniformKernel( vU: np.ndarray ):\n",
    "    return 1 * (np.abs(vU) < 0.5)\n",
    "\n",
    "lKernels = [('Cosine', CosineKernel), ('Gaussian', GaussianKernel), ('Triangular', TriangularKernel), ('Uniform', UniformKernel)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate / Load Data\n",
    "\n",
    "In this section we'll load the data form the provided URL.  \n",
    "We'll create a Data Frame of the data and later will separate it into features and labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading / Generating Data\n",
    "\n",
    "dfX, dsY = fetch_openml('boston', version = 1, return_X_y = True, as_frame = True, parser = 'auto')\n",
    "\n",
    "print(f'The data shape: {dfX.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfX.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsY.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Visualization\n",
    "# We'll display the correlation matrix of the data.\n",
    "# We'll add the target variable as the last variable (`MEDV`).\n",
    "\n",
    "dfData = pd.concat([dfX, dsY], axis = 1)\n",
    "\n",
    "hF, hA = plt.subplots(figsize = (10, 7))\n",
    "sns.heatmap(dfData.corr(numeric_only = True).abs(), annot = True, cmap = 'viridis', ax = hA)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Would you use the above to drop some features?\n",
    "* <font color='red'>(**?**)</font> Do we see all features above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfX.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Training Data \n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Convert the `dfX` data frame into a matrix `mX`. Drop the categorical columns.\n",
    "# 2. Convert the `dsY` data frame into a vector `vY`.\n",
    "mX = dfX.drop(columns = ['CHAS', 'RAD']).to_numpy() #<! Drop the categorical data\n",
    "vY = dsY.to_numpy()\n",
    "#===============================================================#\n",
    "\n",
    "print(f'The features data shape: {mX.shape}')\n",
    "print(f'The labels data shape: {vY.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> We dropped the `CHAS` feature which is binary.\n",
    "* <font color='brown'>(**#**)</font> Binary features are like _One Hot Encoding_, so we could keep it.\n",
    "* <font color='red'>(**?**)</font> Why are binary features good as input while multi value categorical are not? Think about metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the Data\n",
    "# Since we use `h` it makes sense to keep the dynamic range fo values in tact.\n",
    "# In this case we'll center the data and normalize to have a unit standard deviation.\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Construct the scaler using `StandardScaler`.\n",
    "# 2. Apply the scaler on the data.\n",
    "stdScaler = StandardScaler()\n",
    "mX = stdScaler.fit_transform(mX)\n",
    "#===============================================================#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Regressor\n",
    "\n",
    "The kernel regression operation is defined by:\n",
    "\n",
    "$$\\boxed{\\hat{f} \\left( x \\right) = \\frac{\\sum_{i = 1}^{N} w_{x} \\left( {x}_{i} \\right) {y}_{i}}{\\sum_{i = 1}^{N} {w}_{x} \\left( {x}_{i} \\right)}}$$\n",
    "\n",
    "Where ${w}_{x} \\left( {x}_{i} \\right) = k \\left( \\frac{ x - x_{i} }{ h } \\right)$.\n",
    "\n",
    "In this exercise we'll use Leave One Out validation policy with the `cross_val_predict()` function.  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Regression Estimator\n",
    "\n",
    "We could create the linear polynomial fit estimator using a `Pipeline` of `PolynomialFeatures` and `LinearRegression`.  \n",
    "Yet since this is a simple task it is a good opportunity to exercise the creation of a _SciKit Estimator_.\n",
    "\n",
    "We need to provide 4 main methods:\n",
    "\n",
    "1. The `__init()__` Method: The constructor of the object. It should set the kernel type and parameter `h`.\n",
    "2. The `fit()` Method: The pre processing phase. It keeps a **copy** of the fit data. It should set the `h` parameter if not set.\n",
    "3. The `predict()` Method: Prediction of the values.\n",
    "4. The `score()` Method: Calculates the _R2_ score.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> Make sure you read and understand the `ApplyKernelRegression()` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Kernel Regression\n",
    "# Applies the regression given a callable kernel.\n",
    "# It avoids division by 0 in case no reference points are given within the kernel domain.\n",
    "\n",
    "def ApplyKernelRegression( hKernel: Callable[np.ndarray, np.ndarray], paramH: float, mG: np.ndarray, vY: np.ndarray, mX: np.ndarray, metricType = 'euclidean', zeroThr: float = 1e-9 ) -> np.ndarray:\n",
    "\n",
    "    mD = sp.spatial.distance.cdist(mX, mG, metric = metricType)\n",
    "    mW = hKernel(mD / paramH)\n",
    "    vK = mW @ vY #<! For numerical stability, removing almost zero values\n",
    "    vW = np.sum(mW, axis = 1)\n",
    "    vI = np.abs(vW) < zeroThr #<! Calculate only when there's real data\n",
    "    vK[vI] = 0\n",
    "    vW[vI] = 1 #<! Remove cases of dividing by 0\n",
    "    vYPred = vK / vW\n",
    "\n",
    "    return vYPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelRegressor(RegressorMixin, BaseEstimator):\n",
    "    def __init__(self, kernelType:s str = 'Gaussian', paramH: float = None, metricType: str = 'euclidean', lKernels: List = lKernels):\n",
    "        #===========================Fill This===========================#\n",
    "        # 1. Add `kernelType` as an attribute of the object.\n",
    "        # 2. Define the kernel from `lKernels` as `self.hKernel`.\n",
    "        # 3. Add `paramH` as an attribute of the object.\n",
    "\n",
    "        # !! Verify the input string of the kernel is within `lKernels`.\n",
    "        self.kernelType = kernelType\n",
    "        hKernel = None\n",
    "        for tKernel in lKernels:\n",
    "            if tKernel[0] == kernelType:\n",
    "                hKernel = tKernel[1]\n",
    "                break\n",
    "        if hKernel is not None:\n",
    "            self.hKernel = hKernel\n",
    "        else:\n",
    "            raise ValueError(f'The kernel in kernelType = {kernelType} is not in lKernels.')\n",
    "        self.paramH     = paramH\n",
    "        #===============================================================#\n",
    "        # We must set all input parameters as attributes\n",
    "        self.metricType = metricType\n",
    "        self.lKernels   = lKernels\n",
    "        \n",
    "    \n",
    "    def fit(self, mX, vY):\n",
    "        \n",
    "        if np.ndim(mX) != 2:\n",
    "            raise ValueError(f'The input `mX` must be an array like of size (n_samples, n_features) !')\n",
    "        \n",
    "        if mX.shape[0] !=  vY.shape[0]:\n",
    "            raise ValueError(f'The input `mX` must be an array like of size (n_samples, n_features) and `vY` must be (n_samples) !')\n",
    "        \n",
    "        #===========================Fill This===========================#\n",
    "        # 1. Extract the number of samples.\n",
    "        # 2. Set the bandwidth using Silverman's rule of thumb if it is not set (`None`).\n",
    "        # 3. Keep a copy of `mX` as a reference grid of features `mG`.\n",
    "        # 4. Keep a copy of `vY` as a reference values.\n",
    "        numSamples = mX.shape[0]\n",
    "        if self.paramH is None:\n",
    "            # Using Silverman's rule of thumb.\n",
    "            # It is optimized for Density Estimation for Univariate Gaussian like data.\n",
    "            σ = np.sqrt(np.sum(np.sqaure(mX - np.mean(mX, axis = 0))))\n",
    "            self.paramH = 1.06 * σ * (numSamples ** (-0.2))\n",
    "        \n",
    "        self.mG = mX.copy()\n",
    "        self.vY = vY.copy()\n",
    "        #===============================================================#\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, mX):\n",
    "\n",
    "        if np.ndim(mX) != 2:\n",
    "            raise ValueError(f'The input `mX` must be an array like of size (n_samples, n_features) !')\n",
    "\n",
    "        if mX.shape[1] != self.mG.shape[1]:\n",
    "            raise ValueError(f'The input `mX` must be an array like of size (n_samples, n_features) where `n_features` matches the number of feature in `fit()` !')\n",
    "\n",
    "        return ApplyKernelRegression(self.hKernel, self.paramH, self.mG, self.vY, mX, self.metricType)\n",
    "    \n",
    "    def score(self, mX, vY):\n",
    "        # Return the R2 as the score\n",
    "\n",
    "        if (np.size(vY) != np.size(mX, axis = 0)):\n",
    "            raise ValueError(f'The number of samples in `mX` must match the number of labels in `vY`.')\n",
    "\n",
    "        #===========================Fill This===========================#\n",
    "        # 1. Apply the prediction on the input features.\n",
    "        # 2. Calculate the R2 score (You may use `r2_score()`).\n",
    "        vYPred  = self.predict(mX)\n",
    "        valR2   = r2_score(vY, vYPred)\n",
    "        #===============================================================#\n",
    "\n",
    "        return valR2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Model and Optimize Hyper Parameters\n",
    "\n",
    "In this section we'll optimize the model according to the `R2` score.  \n",
    "\n",
    "We'll use the `r2_score()` function to calculate the score.  \n",
    "The process to optimize the _Hyper Parameters_ will be as following:\n",
    "\n",
    "1. Build a data frame to keep the scoring of the different hyper parameters combination.\n",
    "2. Optimize the model:\n",
    "  - Construct a model using the current combination of _hyper parameters_.\n",
    "  - Apply a cross validation process to predict the data using `cross_val_predict()`.\n",
    "  - As the cross validation iterator (The `cv` parameter) use `KFold` to implement _Leave One Out_ policy.\n",
    "3. Calculate the score of the predicted classes.\n",
    "4. Store the result in the performance data frame.\n",
    "\n",
    "\n",
    "* <font color='red'>(**?**)</font> While the `R2` score is used to optimize the Hyper Parameter, what loss is used to optimize the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Data Frame\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Calculate the number of combinations.\n",
    "# 2. Create a nested loop to create the combinations between the parameters.\n",
    "# 3. Store the combinations as the columns of a data frame.\n",
    "\n",
    "# For Advanced Python users: Use iteration tools for create the cartesian product\n",
    "numComb = len(lKernelType) * len(lH)\n",
    "dData   = {'Kernel Type': [], 'h': [], 'R2': [0.0] * numComb}\n",
    "\n",
    "for ii, kernelType in enumerate(lKernelType):\n",
    "    for jj, paramH in enumerate(lH):\n",
    "        dData['Kernel Type'].append(kernelType)\n",
    "        dData['h'].append(paramH)\n",
    "#===============================================================#\n",
    "\n",
    "dfModelScore = pd.DataFrame(data = dData)\n",
    "dfModelScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the Model\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Iterate over each row of the data frame `dfModelScore`. Each row defines the hyper parameters.\n",
    "# 2. Construct the model.\n",
    "# 3. Train it on the Train Data Set.\n",
    "# 4. Calculate the score.\n",
    "# 5. Store the score into the data frame column.\n",
    "\n",
    "for ii in range(numComb):\n",
    "    kernelType = dfModelScore.loc[ii, 'Kernel Type']\n",
    "    paramH     = dfModelScore.loc[ii, 'h']\n",
    "\n",
    "    print(f'Processing model {ii + 1:03d} out of {numComb} with `Kernel Type` = {kernelType} and `h` = {paramH}.')\n",
    "\n",
    "    oKerReg = KernelRegressor(kernelType = kernelType, paramH = paramH)\n",
    "    \n",
    "    vYPred = cross_val_predict(oKerReg, mX, vY, cv = KFold(n_splits = mX.shape[0]))\n",
    "\n",
    "    scoreR2 = r2_score(vY, vYPred)\n",
    "    dfModelScore.loc[ii, 'R2'] = scoreR2\n",
    "    print(f'Finished processing model {ii + 1:03d} with `R2 = {scoreR2}.')\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Sorted Results (Descending)\n",
    "# Pandas allows sorting data by any column using the `sort_values()` method\n",
    "# The `head()` allows us to see only the the first values\n",
    "dfModelScore.sort_values(by = ['R2'], ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Train Data F1 as a Heat Map\n",
    "# We can pivot the data set created to have a 2D matrix of the score as a function of parameters.\n",
    "\n",
    "hF, hA = plt.subplots(figsize = (9, 9))\n",
    "\n",
    "# hA = sns.heatmap(data = dfModelScore.pivot(index = 'h', columns = 'Kernel Type', values = 'R2'), robust = True, linewidths = 1, annot = True, fmt = '0.2f', norm = LogNorm(), ax = hA)\n",
    "hA = sns.heatmap(data = dfModelScore.pivot(index = 'h', columns = 'Kernel Type', values = 'R2'), robust = True, linewidths = 1, annot = True, fmt = '0.2f', cmap = 'viridis', ax = hA)\n",
    "hA.set_title('R2 of the Cross Validation')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> What's the actual model in production for Kernel Regression?\n",
    "* <font color='brown'>(**#**)</font> In production we'd extract the best hyper parameters and then train again on the whole data.\n",
    "* <font color='brown'>(**#**)</font> Usually, for best hype parameters, it is better to use cross validation with low number of folds. Using Leave One Out is better for estimating real world performance. The logic is that the best hyper parameters should be selected when they are tested with low correlation of the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized Cross Validation for Kernel Regression\n",
    "\n",
    "One way to optimize the process is by pre calculating the distance matrix for the whole data.  \n",
    "Then using a sub set of it according to the subset for training.\n",
    "\n",
    "For instance, let's recreate the _Leave One Out_:\n",
    "\n",
    "1. Calculate the Distance Matrix $\\boldsymbol{D}_{x} \\in \\mathbb{R}^{N \\times N}$ such that $ \\boldsymbol{D}_{x} \\left[ i, j \\right] = \\left\\| \\boldsymbol{x}_{i}-\\boldsymbol{x}_{j} \\right\\| _{2}$.\n",
    "2. Calculate the weights matrix $\\boldsymbol{W} \\in \\mathbb{R}^{N \\times N}$ such that $\\boldsymbol{W} \\left[ i, j \\right] = k \\left( \\frac{1}{h} \\boldsymbol{D}_{x} \\left[ i, j \\right] \\right)$.\n",
    "3. Estimate $\\boldsymbol{x}_{i}$ without using $\\boldsymbol{x}_{i}$ we set $\\boldsymbol{W} \\left[ i, i \\right] = 0$.\n",
    "4. Apply kernel regression $\\hat{\\boldsymbol{y}} = \\left( \\boldsymbol{W} \\boldsymbol{y} \\right) \\oslash \\left( \\boldsymbol{W} \\boldsymbol{1} \\right)$. Where $\\oslash$ is element wise division."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorized Leave One Out Kernel Regression\n",
    "\n",
    "numSamples = len(vY) #<! Number of Samples\n",
    "hK = GaussianKernel #<! Kernel\n",
    "paramH = 0.3 #<! Bandwidth\n",
    "mD = sp.spatial.distance.cdist(mX, mX, metric = 'euclidean') #<! Distance Matrix\n",
    "mW = hK(mD / paramH) #<! Weights matrix\n",
    "\n",
    "# Zeroing the diagonal to prevent the weight of the sample\n",
    "mW[range(numSamples), range(numSamples)] = 0 #<! Leave One Out\n",
    "vYPred = (mW @ vY) / np.sum(mW, axis = 1) #<! Kernel Regression\n",
    "\n",
    "print(f'The Leave One Out R2 Score for Kernel Type = {hK} and Bandwidth = {paramH} is {r2_score(vY, vYPred) }.')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='green'>(**@**)</font> Plot the regression of the best model (See previous notebooks)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
