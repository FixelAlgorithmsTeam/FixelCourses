{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Fixel Algorithms](https://fixelalgorithms.co/images/CCExt.png)](https://fixelalgorithms.gitlab.io/)\n",
    "\n",
    "# Machine Learning Methods\n",
    "\n",
    "## Supervised Learning - Logistic Regression - Exercise\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 0.1.000 | 02/02/2023 | Royi Avital | First version                                                      |\n",
    "|         |            |             |                                                                    |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/MachineLearningMethods/2023_01/0019LogisticRegressionExercise.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T09:30:06.492269Z",
     "start_time": "2022-02-02T09:30:06.220934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Miscellaneous\n",
    "import os\n",
    "from platform import python_version\n",
    "import random\n",
    "\n",
    "# Typing\n",
    "from typing import Tuple\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm, Normalize\n",
    "import seaborn as sns\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython\n",
    "from IPython.display import Image, display\n",
    "from ipywidgets import Dropdown, FloatSlider, interact, IntSlider, Layout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "%matplotlib inline\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "# sns.set_theme() #>! Apply SeaBorn theme\n",
    "\n",
    "runInGoogleColab = 'google.colab' in str(get_ipython())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "FIG_SIZE_DEF = (8, 8)\n",
    "ELM_SIZE_DEF = 50\n",
    "CLASS_COLOR = ('b', 'r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixel Algorithms Packages\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "In this exercise we'll use the Logistic Regression model as a classifier.  \n",
    "The SciKit Learn library implement it with the [`LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) class.\n",
    "\n",
    "I this exercise we'll do the following:\n",
    "\n",
    "1. Load the [MNIST Data set](https://en.wikipedia.org/wiki/MNIST_database) using `fetch_openml()`.\n",
    "2. Train a Logistic Regression model on the training data.\n",
    "3. Optimize the parameters: `penalty` and `C` by the `roc_auc` score.\n",
    "4. Interpret the model using its weights.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The model is a linear model, hence its weights are easy to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "numSamplesTrain = 1_500\n",
    "numSamplesTest  = 1_000\n",
    "\n",
    "numRows = 3\n",
    "numCols = 3\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Set the options for the `penalty` parameter (Use: 'l1' and 'l2').\n",
    "# 2. Set the options for the `C` parameter (~25 values, According to computer speed).\n",
    "lPenalty    = ???\n",
    "lC          = ???\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary Functions\n",
    "\n",
    "def PlotMnistImages(mX, vY, numRows, numCols, imgSize = (28, 28), randomChoice = True, hF = None):\n",
    "\n",
    "    numSamples  = mX.shape[0]\n",
    "    numPx       = mX.shape[1]\n",
    "\n",
    "    numImg = numRows * numCols\n",
    "\n",
    "    # tFigSize = (numRows * 3, numCols * 3)\n",
    "    tFigSize = (numCols * 3, numRows * 3)\n",
    "\n",
    "    if hF is None:\n",
    "        hF, hA = plt.subplots(numRows, numCols, figsize = tFigSize)\n",
    "    else:\n",
    "        hA = hF.axis\n",
    "    \n",
    "    hA = np.atleast_1d(hA) #<! To support numImg = 1\n",
    "    hA = hA.flat\n",
    "\n",
    "    \n",
    "    for kk in range(numImg):\n",
    "        if randomChoice:\n",
    "            idx = np.random.choice(numSamples)\n",
    "        else:\n",
    "            idx = kk\n",
    "        mI  = np.reshape(mX[idx, :], imgSize)\n",
    "    \n",
    "        hA[kk].imshow(mI, cmap = 'gray')\n",
    "        hA[kk].tick_params(axis = 'both', left = False, top = False, right = False, bottom = False, labelleft = False, labeltop = False, labelright = False, labelbottom = False)\n",
    "        hA[kk].set_title(f'Index = {idx}, Label = {vY[idx]}')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def PlotLabelsHistogram(vY: np.ndarray, hA = None):\n",
    "\n",
    "    if hA is None:\n",
    "        hF, hA = plt.subplots(figsize = (8, 6))\n",
    "    \n",
    "    vLabels, vCounts = np.unique(vY, return_counts = True)\n",
    "\n",
    "    hA.bar(vLabels, vCounts, width = 0.9, align = 'center')\n",
    "    hA.set_title('Histogram of Classes / Labels')\n",
    "    hA.set_xlabel('Class')\n",
    "    hA.set_ylabel('Number of Samples')\n",
    "\n",
    "    return hA\n",
    "\n",
    "def PlotConfusionMatrix(vY: np.ndarray, vYPred: np.ndarray, hA: plt.Axes = None, lLabels: list = None, dScore: dict = None, titleStr: str = 'Confusion Matrix'):\n",
    "\n",
    "    # Calculation of Confusion Matrix\n",
    "    mConfMat = confusion_matrix(vY, vYPred)\n",
    "    oConfMat = ConfusionMatrixDisplay(mConfMat, display_labels = lLabels)\n",
    "    oConfMat = oConfMat.plot(ax = hA)\n",
    "    hA = oConfMat.ax_\n",
    "    if dScore is not None:\n",
    "        titleStr += ':'\n",
    "        for scoreName, scoreVal in  dScore.items():\n",
    "            titleStr += f' {scoreName} = {scoreVal:0.2},'\n",
    "        titleStr = titleStr[:-1]\n",
    "    hA.set_title(titleStr)\n",
    "    hA.grid(False)\n",
    "\n",
    "    return hA\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate / Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading / Generating Data\n",
    "\n",
    "mX, vY = fetch_openml('mnist_784', version = 1, return_X_y = True, as_frame = False, parser = 'auto')\n",
    "\n",
    "print(f'The features data shape: {mX.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre Processing\n",
    "\n",
    "# The image is in the range {0, 1, ..., 255}\n",
    "# We scale it into [0, 1]\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Scale the values into the [0, 1] range.\n",
    "mX = ???\n",
    "\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Split the data such that the Train Data has `numSamplesTrain`.\n",
    "# 2. Split the data such that the Test Data has `numSamplesTest`.\n",
    "# 3. The distribution of the classes must match the original data.\n",
    "\n",
    "mXTrain, mXTest, vYTrain, vYTest = train_test_split(???)\n",
    "\n",
    "#===============================================================#\n",
    "\n",
    "print(f'The training features data shape: {mXTrain.shape}')\n",
    "print(f'The training labels data shape: {vYTrain.shape}')\n",
    "print(f'The test features data shape: {mXTest.shape}')\n",
    "print(f'The test labels data shape: {vYTest.shape}')\n",
    "print(f'The unique values of the labels: {np.unique(vY)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Data\n",
    "\n",
    "PlotMnistImages(mX, vY, numRows, numCols)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Labels\n",
    "\n",
    "When dealing with classification, it is important to know the balance between the labels within the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hA = PlotLabelsHistogram(vY)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "The _logistic regression_ can be derived in many forms.  \n",
    "We'll illustrate few of them.\n",
    "\n",
    "### Derivation 001\n",
    "\n",
    "One intuitive path is saying that we're after calculating the probability: $p \\left( y = 1 \\mid \\boldsymbol{x} \\right)$.  \n",
    "Since it is a probability function is must obey some rules. The first one being in the range $\\left[ 0, 1 \\right]$.  \n",
    "\n",
    "A function which maps $\\left( -\\infty, \\infty \\right) \\to \\left[0, 1 \\right]$ is the [Sigmoid Function](https://en.wikipedia.org/wiki/Sigmoid_function): $\\sigma \\left( z \\right) = \\frac{1}{1 + \\exp \\left( z \\right)}$.\n",
    "\n",
    "So now we can say that: $p \\left( y = 1 \\mid \\boldsymbol{x} \\right) = \\sigma \\left( {z}_{i} \\right)$.  \n",
    "Now the problem is modeling the parameter ${z}_{i}$. In which in a linear case will be modeled as ${z}_{i} = \\boldsymbol{w}_{i}^{T} \\boldsymbol{x} + {b}_{i}$.  \n",
    "Namely by a linear model, which in the choice of the Sigmoid Function means the objective function is Convex in $\\boldsymbol{w}_{i}$ and $b$:\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/c/cb/Exam_pass_logistic_curve.svg/640px-Exam_pass_logistic_curve.svg.png)\n",
    "\n",
    "* <font color='brown'>(**#**)</font> Actually it is convex only if the problem is not linear separable.\n",
    "\n",
    "If we expand the above to multi class we'll get the [Softmax Function](https://en.wikipedia.org/wiki/Softmax_function) as in slides.\n",
    "\n",
    "### Derivation 002\n",
    "\n",
    "By _Bayes Theorem_ for the $L$ classes model:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p \\left( y = {L}_{i} \\mid \\boldsymbol{x} \\right) & = \\frac{ p \\left( \\boldsymbol{x} \\mid y = {L}_{i} \\right) p \\left( y = {L}_{i} \\right) }{ p \\left( \\boldsymbol{x} \\right) } && \\text{} \\\\\n",
    "& = \\frac{ p \\left( \\boldsymbol{x} \\mid y = {L}_{i} \\right) p \\left( y = {L}_{i} \\right) }{ \\sum_{j = 1}^{L} p \\left( \\boldsymbol{x} \\mid y = {L}_{j} \\right) p \\left( y = {L}_{j} \\right) } && \\text{Expanding by law total probability} \\\\\n",
    "& = \\frac{ p \\left( \\boldsymbol{x} \\mid y = {L}_{i} \\right) p \\left( y = {L}_{i} \\right) }{ p \\left( \\boldsymbol{x} \\mid y = {L}_{i} \\right) p \\left( y = {L}_{i} \\right) + p \\left( \\boldsymbol{x} \\mid y \\neq {L}_{i} \\right) p \\left( y \\neq {L}_{i} \\right) } && \\text{Expanding by law total probability} \\\\\n",
    "& = \\frac{ 1 }{ 1 + \\frac{ p \\left( \\boldsymbol{x} \\mid y \\neq {L}_{i} \\right) p \\left( y \\neq {L}_{i} \\right)}{p \\left( \\boldsymbol{x} \\mid y = {L}_{i} \\right) p \\left( y = {L}_{i} \\right)} } && \\text{Dividing by $p \\left( \\boldsymbol{x} \\mid y = {L}_{i} \\right) p \\left( y = {L}_{i} \\right)$} \\\\\n",
    "& = \\frac{ 1 }{ 1 + {e}^{\\log \\frac{ p \\left( \\boldsymbol{x} \\mid y \\neq {L}_{i} \\right) p \\left( y \\neq {L}_{i} \\right)}{p \\left( \\boldsymbol{x} \\mid y = {L}_{i} \\right) p \\left( y = {L}_{i} \\right)}} } && \\text{for $x \\in \\left[ 0, \\infty \\right) \\Rightarrow x = \\exp \\log x $} \\\\\n",
    "& = \\frac{ 1 }{ 1 + {e}^{-\\log \\frac{ p \\left( \\boldsymbol{x} \\mid y = {L}_{i} \\right) p \\left( y = {L}_{i} \\right) }{ p \\left( \\boldsymbol{x} \\mid y \\neq {L}_{i} \\right) p \\left( y \\neq {L}_{i} \\right) }} } && \\text{$\\log x = - \\log \\frac{1}{x}$} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Now, if we model the log of likelihood ratio of the ${L}_{i}$ label with a linear model:\n",
    "\n",
    "$$ \\log \\frac{ p \\left( \\boldsymbol{x} \\mid y = {L}_{i} \\right) p \\left( y = {L}_{i} \\right) }{ p \\left( \\boldsymbol{x} \\mid y \\neq {L}_{i} \\right) p \\left( y \\neq {L}_{i} \\right) } = \\boldsymbol{w}_{i}^{T} \\boldsymbol{x} + {b}_{i} $$\n",
    "\n",
    "So we get:\n",
    "\n",
    "$$ p \\left( y = {L}_{i} \\mid \\boldsymbol{x} \\right) = \\frac{1}{ 1 + {e}^{- \\left( \\boldsymbol{w}_{i}^{T} \\boldsymbol{x} + {b}_{i} \\right)} } $$\n",
    "\n",
    "Yet, since $1 = {e}^{- \\log \\frac{p \\left( \\boldsymbol{x} \\mid y = {L}_{i} \\right) p \\left( y = {L}_{i} \\right)}{p \\left( \\boldsymbol{x} \\mid y = {L}_{i} \\right) p \\left( y = {L}_{i} \\right)}}$ the above can be written as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p \\left( y = {L}_{i} \\mid \\boldsymbol{x} \\right) = \\frac{1}{ 1 + {e}^{- \\left( \\boldsymbol{w}_{i}^{T} \\boldsymbol{x} + {b}_{i} \\right)} }\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Derivation 003\n",
    "\n",
    "By _Bayes Theorem_ for the $L$ classes model:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p \\left( y = {L}_{i} \\mid \\boldsymbol{x} \\right) & = \\frac{ p \\left( \\boldsymbol{x} \\mid y = {L}_{i} \\right) p \\left( y = {L}_{i} \\right) }{ p \\left( \\boldsymbol{x} \\right) } && \\text{} \\\\\n",
    "& = \\frac{ p \\left( \\boldsymbol{x} \\mid y = {L}_{i} \\right) p \\left( y = {L}_{i} \\right) }{ \\sum_{j = 1}^{L} p \\left( \\boldsymbol{x} \\mid y = {L}_{j} \\right) p \\left( y = {L}_{j} \\right) } && \\text{Expanding by law total probability} \\\\\n",
    "& = \\frac{ p \\left( \\boldsymbol{x} \\mid y = {L}_{i} \\right) p \\left( y = {L}_{i} \\right) }{ p \\left( \\boldsymbol{x} \\mid y = {L}_{k} \\right) p \\left( y = {L}_{k} \\right) + \\sum_{j \\neq k} p \\left( \\boldsymbol{x} \\mid y = {L}_{j} \\right) p \\left( y = {L}_{j} \\right) } && \\text{} \\\\\n",
    "& = \\frac{ \\frac{p \\left( \\boldsymbol{x} \\mid y = {L}_{i} \\right) p \\left( y = {L}_{i} \\right)}{p \\left( \\boldsymbol{x} \\mid y = {L}_{k} \\right) p \\left( y = {L}_{k} \\right)} }{ 1 + \\sum_{j \\neq k} \\frac{p \\left( \\boldsymbol{x} \\mid y = {L}_{j} \\right) p \\left( y = {L}_{j} \\right)}{p \\left( \\boldsymbol{x} \\mid y = {L}_{k} \\right) p \\left( y = {L}_{k} \\right)} } && \\text{Dividing by $p \\left( \\boldsymbol{x} \\mid y = {L}_{k} \\right) p \\left( y = {L}_{k} \\right)$} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "As in above, we may model the Log Likelihood Ratio by a linear function of $\\boldsymbol{x}$ then we'll get:\n",
    "\n",
    "$$ p \\left( y = {L}_{i} \\mid \\boldsymbol{x} \\right) = \\frac{ \\exp{\\left( \\boldsymbol{w}_{i}^{T} \\boldsymbol{x} \\right)} }{ 1 + \\sum_{j \\neq k} \\exp{\\left( \\boldsymbol{w}_{j}^{T} \\boldsymbol{x} \\right)}} $$\n",
    "\n",
    "Since $1 = \\exp{ \\left( \\log{ \\frac{p \\left( \\boldsymbol{x} \\mid y = {L}_{k} \\right) p \\left( y = {L}_{k} \\right)}{p \\left( \\boldsymbol{x} \\mid y = {L}_{k} \\right) p \\left( y = {L}_{k} \\right)} } \\right)}$ we can write:\n",
    "\n",
    "$$ p \\left( y = {L}_{i} \\mid \\boldsymbol{x} \\right) = \\frac{ \\exp{\\left( \\boldsymbol{w}_{i}^{T} \\boldsymbol{x} \\right)} }{ \\sum_{j} \\exp{\\left( \\boldsymbol{w}_{j}^{T} \\boldsymbol{x} \\right)}} $$\n",
    "\n",
    "### Derivation 004\n",
    "\n",
    "Given $L$ classes, we can chose a reference class: ${L}_{k}$. Then define the linear model of the log likelihood ratio compared to it:\n",
    "\n",
    "$$ \\log{ \\left( \\frac{ p \\left( y = {L}_{i} \\mid \\boldsymbol{x} \\right) }{ p \\left( {y} = {L}_{k} \\mid \\boldsymbol{x} \\right) } \\right) } = \\boldsymbol{w}_{i}^{T} \\boldsymbol{x} + {b}_{i} $$\n",
    "\n",
    "By definition $p \\left( y = {L}_{i} \\mid \\boldsymbol{x} \\right) = p \\left( y = {L}_{k} \\mid \\boldsymbol{x} \\right) \\exp{ \\left( \\boldsymbol{w}_{i}^{T} \\boldsymbol{x} + {b}_{i} \\right) }$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "1 - p \\left( y = {L}_{k} \\mid \\boldsymbol{x} \\right) & = \\sum_{j \\neq k} p \\left( y = {L}_{j} \\mid \\boldsymbol{x} \\right) && \\text{} \\\\\n",
    "& = \\sum_{j \\neq k} p \\left( y = {L}_{k} \\mid \\boldsymbol{x} \\right) \\exp{ \\left( \\boldsymbol{w}_{j}^{T} \\boldsymbol{x} + {b}_{j} \\right) } && \\text{Since $p \\left( y = {L}_{i} \\mid \\boldsymbol{x} \\right) = p \\left( y = {L}_{k} \\mid \\boldsymbol{x} \\right) \\exp{ \\left( \\boldsymbol{w}_{i}^{T} \\boldsymbol{x} + {b}_{i} \\right) }$} \\\\\n",
    "& = p \\left( y = {L}_{k} \\mid \\boldsymbol{x} \\right) \\sum_{j \\neq k} \\exp{ \\left( \\boldsymbol{w}_{j}^{T} \\boldsymbol{x} + {b}_{j} \\right) } && \\text{} \\\\\n",
    "& \\Rightarrow p \\left( y = {L}_{k} \\mid \\boldsymbol{x} \\right) = \\frac{1}{1 + \\sum_{j \\neq k} \\exp{ \\left( \\boldsymbol{w}_{j}^{T} \\boldsymbol{x} + {b}_{j} \\right) }} \\\\\n",
    "& \\Rightarrow p \\left( y = {L}_{i} \\mid \\boldsymbol{x} \\right) = \\frac{ \\exp{ \\left( \\boldsymbol{w}_{i}^{T} \\boldsymbol{x} + {b}_{i} \\right) } }{1 + \\sum_{j \\neq k} \\exp{ \\left( \\boldsymbol{w}_{j}^{T} \\boldsymbol{x} + {b}_{j} \\right) }} && \\text{}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Since $1 = \\exp{\\left( \\log{ \\frac{ p \\left( y = {L}_{k} \\mid \\boldsymbol{x} \\right) }{ p \\left( y = {L}_{k} \\mid \\boldsymbol{x} \\right) } } \\right)}$ we can write:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p \\left( y = {L}_{i} \\mid \\boldsymbol{x} \\right) & = \\frac{ \\exp{ \\left( \\boldsymbol{w}_{i}^{T} \\boldsymbol{x} + {b}_{i} \\right) } }{1 + \\sum_{j \\neq k} \\exp{ \\left( \\boldsymbol{w}_{j}^{T} \\boldsymbol{x} + {b}_{j} \\right) }} \\\\\n",
    "& = \\frac{ \\exp{ \\left( \\boldsymbol{w}_{i}^{T} \\boldsymbol{x} + {b}_{i} \\right) } }{\\exp{ \\left( \\boldsymbol{w}_{k}^{T} \\boldsymbol{x} + {b}_{k} \\right) } + \\sum_{j \\neq k} \\exp{ \\left( \\boldsymbol{w}_{j}^{T} \\boldsymbol{x} + {b}_{j} \\right) }} \\\\\n",
    "& = \\frac{ \\exp{ \\left( \\boldsymbol{w}_{i}^{T} \\boldsymbol{x} + {b}_{i} \\right) } }{ \\sum_{j} \\exp{ \\left( \\boldsymbol{w}_{j}^{T} \\boldsymbol{x} + {b}_{j} \\right) }}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Summary\n",
    "\n",
    "While there are many way to derive the logistic regression (for instance, also by assuming Binomial Distribution), the main motivation is its numerical properties.  \n",
    "Namely being convex with easy to calculate gradient.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The first \"Deep Learning\" model were actually chaining many logistic regression layers.\n",
    "* <font color='brown'>(**#**)</font> Most classification layers in Deep Learning models are basically Logistic Regression.\n",
    "* <font color='brown'>(**#**)</font> The concept of Logistic Regression can also be used as pure regression for continuous data bounded in the range $\\left[ a, b \\right]$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Grid Search Hyper Parameter Optimization\n",
    "\n",
    "1. Create a data frame with 3 columns:\n",
    "  - `Penalty` - The value of the `penalty` parameter.\n",
    "  - `C` - The value of `C` parameter.\n",
    "  - `ROC AUC` - The value of the `roc_auc()` of the model.\n",
    "   \n",
    "   The number of rows should match the number of combinations.\n",
    "2. Iterate over all combinations and measure the score on the test set.\n",
    "3. Plot an heatmap (2D) for the combination of hyper parameters and the resulted AUC.\n",
    "4. Extract the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Data Frame\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Calculate the number of combinations.\n",
    "# 2. Create a nested loop to create the combinations between the parameters.\n",
    "# 3. Store the combinations as the columns of a data frame.\n",
    "\n",
    "# For Advanced Python users: Use iteration tools for create the cartesian product\n",
    "numComb = ???\n",
    "dData   = ???\n",
    "\n",
    "for ii, paramPenalty in enumerate(lPenalty):\n",
    "    for jj, paramC in enumerate(lC):\n",
    "        ?????\n",
    "#===============================================================#\n",
    "\n",
    "dfModelScore = pd.DataFrame(data = dData)\n",
    "dfModelScore\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the Model\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Iterate over each row of the data frame `dfModelScore`. Each row defines the hyper parameters.\n",
    "# 2. Construct the model.\n",
    "# 3. Train it on the Train Data Set.\n",
    "# 4. Calculate its AUC ROC score on the train set, save it to the `ROC AUC Train`.\n",
    "# 5. Calculate its AUC ROC score on the test set, save it to the `ROC AUC Test`.\n",
    "\n",
    "#!! Make sure to chose the `saga` solver as it is the only one supporting all the regularization options.\n",
    "#!! Set the parameter `tol` to ~5e-3 to ensure convergence in a reasonable time.\n",
    "#!! Set the parameter `max_iter` to high value (10_000 or so) to make sure convergence of the model.\n",
    "#!! The score function `roc_auc_score()` requires confidence level of the labels, not the predicted labels.\n",
    "#!! Since we're dealing with multi class one must set the `multi_class` parameter of `roc_auc_score()`.\n",
    "for ii in range(numComb):\n",
    "    paramPenalty    = ???\n",
    "    paramC          = ???\n",
    "\n",
    "    if paramPenalty == 'None':\n",
    "        paramPenalty = None\n",
    "\n",
    "    print(f'Processing model {ii + 1:03d} out of {numComb} with `penalty` = {paramPenalty} and `C` = {paramC}.')\n",
    "\n",
    "    oLogRegCls = ???\n",
    "    oLogRegCls = ???\n",
    "\n",
    "    dfModelScore.loc[ii, 'ROC AUC Train']   = roc_auc_score(???)\n",
    "    dfModelScore.loc[ii, 'ROC AUC Test']    = roc_auc_score(???)\n",
    "#===============================================================#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> If one watches the timing of iterations above, he will see that higher regularization (Smaller `C`) will also be faster to calculate as the weights are less \"crazy\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the results sorted (Test)\n",
    "# Pandas allows sorting data by any column using the `sort_values()` method\n",
    "# The `head()` allows us to see only the the first values\n",
    "dfModelScore.sort_values(by = ['ROC AUC Test'], ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the results sorted (Train)\n",
    "dfModelScore.sort_values(by = ['ROC AUC Train'], ascending = False).head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Can you see cases of Under / Over Fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Train Data ROC AUC as a Heat Map\n",
    "# We can pivot the data set created to have a 2D matrix of the ROC AUC as a function of `C` and the `Penalty`.\n",
    "\n",
    "hA = sns.heatmap(data = dfModelScore.pivot(index = 'C', columns = 'Penalty', values = 'ROC AUC Train'), robust = True, linewidths = 1, annot = True, fmt = '0.2%', norm = LogNorm())\n",
    "hA.set_title('ROC AUC of the Train Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Test Data ROC AUC as a Heat Map\n",
    "# We can pivot the data set created to have a 2D matrix of the ROC AUC as a function of `C` and the `Penalty`.\n",
    "\n",
    "hA = sns.heatmap(data = dfModelScore.pivot(index = 'C', columns = 'Penalty', values = 'ROC AUC Test'), robust = True, linewidths = 1, annot = True, fmt = '0.2%', norm = LogNorm())\n",
    "hA.set_title('ROC AUC of the Test Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Optimal Hyper Parameters\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Extract the index of row of the maximum value of `ROC AUC Test`.\n",
    "# 2. Use the index of the row to extract the hyper parameters which were optimized.\n",
    "\n",
    "#! You may find the `idxmax()` method of a Pandas data frame useful.\n",
    "idxArgMax = ???\n",
    "#===============================================================#\n",
    "\n",
    "optimalPenalty  = dfModelScore.loc[idxArgMax, 'Penalty']\n",
    "optimalC        = dfModelScore.loc[idxArgMax, 'C']\n",
    "\n",
    "print(f'The optimal hyper parameters are: `penalty` = {optimalPenalty}, `C` = {optimalC}')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Model\n",
    "\n",
    "In this section we'll extract the best model an retrain it on the whole data (`mX`).  \n",
    "We need to export the model which has the best Test values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the Optimal Model & Train on the Whole Data\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Construct the logistic regression model. Use the same `tol`, `solver` and `max_itr` as above.\n",
    "# 2. Fit the model on the whole data set (mX).\n",
    "oLogRegCls = ???\n",
    "oLogRegCls = ???\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Score (Accuracy)\n",
    "\n",
    "print(f'The model score (Accuracy) is: {oLogRegCls.score(mX, vY):0.2%}.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Does it match the results above? Why?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain / Interpret the Model\n",
    "\n",
    "Linear models, which works mostly on correlation, are relatively easy to interpret / explain.  \n",
    "In this section we'll show how to interpret the weights of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Weights of the Classes\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# Extract the weights of the model using the `coef_` attribute.\n",
    "mW = ??? #<! The model weights (Without the biases)\n",
    "#===============================================================#\n",
    "\n",
    "print(f'The coefficients / weights matrix has the dimensions: {mW.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the weights basically match each pixel of the input image (As a vector) then we can display them as an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Weights as Images\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Convert the weights into the form ofn an image.\n",
    "# 2. Plot it using `imshow()` of Matplotlib.\n",
    "\n",
    "#! You may use `PlotMnistImages()` to do this for you, look at its code.\n",
    "???\n",
    "#===============================================================#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Could you explain the results and how the model works?\n",
    "* <font color='brown'>(**#**)</font> Usually, for linear models, it is important to have zero mean features.\n",
    "* <font color='blue'>(**!**)</font> Run the above using the `StandardScaler()` as part of the pipeline (Don't alter the images themselves!)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
