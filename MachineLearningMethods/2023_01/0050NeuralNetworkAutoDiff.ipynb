{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Fixel Algorithms](https://fixelalgorithms.co/images/CCExt.png)](https://fixelalgorithms.gitlab.io/)\n",
    "\n",
    "# Machine Learning Methods\n",
    "\n",
    "## Neural Networks - Auto Differentiation  \n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 0.1.000 | 11/03/2023 | Royi Avital | First version                                                      |\n",
    "|         |            |             |                                                                    |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/MachineLearningMethods/2023_01/0046AnomalyDetectionIsolationForest.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T09:30:06.492269Z",
     "start_time": "2022-02-02T09:30:06.220934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "import autograd.numpy as anp\n",
    "from autograd import grad\n",
    "from autograd import elementwise_grad as egrad\n",
    "\n",
    "# Miscellaneous\n",
    "import os\n",
    "import math\n",
    "from platform import python_version\n",
    "import random\n",
    "\n",
    "# Typing\n",
    "from typing import Callable, List, Tuple, Union\n",
    "\n",
    "# Visualization\n",
    "from matplotlib.colors import LogNorm, Normalize, PowerNorm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython\n",
    "from IPython.display import Image, display\n",
    "from ipywidgets import Dropdown, FloatSlider, interact, IntSlider, Layout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "%matplotlib inline\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "# sns.set_theme() #>! Apply SeaBorn theme\n",
    "\n",
    "runInGoogleColab = 'google.colab' in str(get_ipython())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "FIG_SIZE_DEF    = (8, 8)\n",
    "ELM_SIZE_DEF    = 50\n",
    "CLASS_COLOR     = ('b', 'r')\n",
    "EDGE_COLOR      = 'k'\n",
    "MARKER_SIZE_DEF = 10\n",
    "LINE_WIDTH_DEF  = 2\n",
    "\n",
    "DATA_FILE_URL = r'https://raw.githubusercontent.com/nsethi31/Kaggle-Data-Credit-Card-Fraud-Detection/master/creditcard.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixel Algorithms Packages\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto Differentiation\n",
    "\n",
    "Deep Learning / Neural Networks framework frameworks usually have the following 4 main ingredients:\n",
    "\n",
    " - Data Loaders  \n",
    "   Loading data in an optimized manner for the training work.\n",
    " - Layers  \n",
    "   The building bocks (Math functions) of the DL related operations.\n",
    " - Optimizers and Schedulers  \n",
    "   Algorithms for the optimization (Usually 1st derivative based) and schedulers to optimize the learning rate.\n",
    " - Auto Differentiation  \n",
    "   Used for the back propagation calculation.\n",
    "\n",
    "Most frameworks also pack the ayer which optimizes teh operation into hardware.  \n",
    "Currently, it mostly done to target GPU's (Mostly by NVIDIA).\n",
    "\n",
    "In this notebook we'll use a pretty modern auto differentiation framework called [`AutoGrad`](https://github.com/HIPS/autograd) for a simple example.  \n",
    "We'll try \n",
    "\n",
    "* <font color='brown'>(**#**)</font> This notebook doesn't cover teh ideas for _auto differentiation_. One might watch [What is Automatic Differentiation](https://www.youtube.com/watch?v=wG_nF1awSSY).\n",
    "* <font color='brown'>(**#**)</font> There are few approaches to auto differentiation. One of them is based on Dual Numbers.  \n",
    "  See A Hands On Introduction to Automatic Differentiation: [Part I](https://mostafa-samir.github.io/auto-diff-pt1), [Part II](https://mostafa-samir.github.io/auto-diff-pt2)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Balance wise, how do you expect the data to look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# Data\n",
    "numCoeff    = 3\n",
    "numSamples  = 50\n",
    "noiseStd    = 0.15\n",
    "\n",
    "# Model\n",
    "λ  = 0.1 #<! Regularization\n",
    "mD = -np.eye(numSamples - 1, numSamples, k = 0) + np.eye(numSamples - 1, numSamples, k = 1) #<! Finite Differences Matrix\n",
    "δ  = 1 #<! Huber Loss\n",
    "\n",
    "# Visualization\n",
    "numGrdiPts = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary Functions\n",
    "\n",
    "def PlotRegressionData( vX: np.ndarray, vY: np.ndarray, hA:plt.Axes = None, figSize: Tuple[int, int] = FIG_SIZE_DEF, elmSize: int = ELM_SIZE_DEF, elmColor = None, elmAlpha: float = 1.0, dataLabel: str = '_', axisTitle: str = None ) -> plt.Axes:\n",
    "\n",
    "    if hA is None:\n",
    "        hF, hA = plt.subplots(figsize = figSize)\n",
    "    else:\n",
    "        hF = hA.get_figure()\n",
    "    \n",
    "    hA.scatter(vX, vY, s = elmSize, c = elmColor, edgecolor = 'k', alpha = elmAlpha, label = dataLabel)\n",
    "    hA.set_xlabel('$x$')\n",
    "    hA.set_ylabel('$y$')\n",
    "    if axisTitle is not None:\n",
    "        hA.set_title(axisTitle)\n",
    "    hA.legend()\n",
    "    \n",
    "    return hA\n",
    "\n",
    "def SampleSum1Vec( numElements: int ) -> np.ndarray:\n",
    "    # Non negative, sum of 1 vector\n",
    "    vS = np.random.rand(numElements)\n",
    "    vS = vS / np.sum(vS)\n",
    "\n",
    "    return vS\n",
    "\n",
    "def GenMeanMatrix( numSamples: int, numCoeff: int = 5 ) -> np.ndarray:\n",
    "    # `numCoeff` - Must be odd!\n",
    "    # In practice, for large number of samples relative to coefficients it is better to use sparse matrix.\n",
    "    # This filter is not convolution as it always average `numCoeff` values. \n",
    "    # Hence at the borders it differs from a standard convolution.\n",
    "\n",
    "    if ((numCoeff % 2) == 0):\n",
    "        # The number is even\n",
    "        raise ValueError(f'The parameter `numCoeff` must be an odd positive integer. The input is {numCoeff}.')\n",
    "    \n",
    "    kernelRadius = int(numCoeff / 2)\n",
    "\n",
    "    coeffVal = 1 / numCoeff\n",
    "    mH = np.zeros(shape = (numSamples, numSamples))\n",
    "\n",
    "    # mH[:kernelRadius, :numCoeff] = coeffVal\n",
    "    # mH[-kernelRadius:, -numCoeff:] = coeffVal\n",
    "\n",
    "    mH[:kernelRadius, :numCoeff] = SampleSum1Vec(numCoeff)\n",
    "    mH[-kernelRadius:, -numCoeff:] = SampleSum1Vec(numCoeff)\n",
    "\n",
    "    for ii in range(kernelRadius, numSamples - kernelRadius):\n",
    "        # mH[ii, (ii - kernelRadius):(ii + kernelRadius + 1)] = coeffVal\n",
    "        mH[ii, (ii - kernelRadius):(ii + kernelRadius + 1)] = SampleSum1Vec(numCoeff)\n",
    "\n",
    "    return mH\n",
    "\n",
    "def PseudoHuber( vX: np.ndarray, δ: float = 1 ):\n",
    "    # Smooth approximation of the Abs\n",
    "\n",
    "    return δ * δ * (anp.sqrt(1 + anp.square(vX / δ)) - 1)\n",
    "\n",
    "def PseudoHuberGrad( vX: np.ndarray, δ: float = 1 ):\n",
    "\n",
    "    return vX / np.sqrt(1 + np.square(vX / δ))\n",
    "\n",
    "def ApproxL1Norm( vX: np.ndarray, δ: float = 1 ):\n",
    "\n",
    "    return anp.sum(PseudoHuber(vX, δ = δ))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate / Load Data\n",
    "\n",
    "The data will be a piece wise constant function which is filtered by a random mean filter.  \n",
    "The filtration will be applied by the known filter model matrix `mH`.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> While the filter is not applied by a convolution, it is a linear model.\n",
    "* <font color='brown'>(**#**)</font> We use random mean in order to stabilize the filter matrix (Condition Number).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading / Generating Data\n",
    "\n",
    "mH = GenMeanMatrix(numSamples, numCoeff = numCoeff) #<! Model Matrix\n",
    "# mH = np.eye(numSamples)\n",
    "vX = np.arange(numSamples)\n",
    "vY1 = np.piecewise(vX, [vX < numSamples, vX < 40, vX < 30, vX < 20, vX < 10], [0, 1, 2, 3, 4])\n",
    "vY2 = mH @ vY1\n",
    "vY = vY2 + (noiseStd * np.random.randn(numSamples))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Data\n",
    "hF, hA = plt.subplots(figsize = (8, 8))\n",
    "\n",
    "hA = PlotRegressionData(vX, vY1, elmColor = 'C0', dataLabel = 'RAW Data', hA = hA)\n",
    "hA = PlotRegressionData(vX, vY2, elmColor = 'C1', elmAlpha = 0.8, dataLabel = 'Filtered Data', hA = hA)\n",
    "hA = PlotRegressionData(vX, vY, elmColor = 'C2', elmAlpha = 0.6, dataLabel = 'Noisy Data (Measurement)', hA = hA)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model and Optimization Process\n",
    "\n",
    "We'll use the Total Variation model which is a good fit for Piece Wise Constant models:\n",
    "\n",
    "\n",
    "$$ \\arg \\min_{\\boldsymbol{w}} \\frac{1}{2} {\\left\\| H \\boldsymbol{w} - \\boldsymbol{y} \\right\\|}_{2}^{2} + \\lambda {\\left\\| D \\boldsymbol{w} \\right\\|}_{1} $$\n",
    "\n",
    "Where ${\\left\\| D \\boldsymbol{w} \\right\\|}_{1} = \\sum_{i = 1}^{N - 1} \\left| {w}_{i + 1} - {w}_{i} \\right|$ is the Total Variation of the samples.\n",
    "\n",
    "Since the ${L}_{1}$ norm is not smooth, we'll approximate it using the Pseudo Huber Loss:\n",
    "\n",
    "$$ \\arg \\min_{\\boldsymbol{w}} \\frac{1}{2} {\\left\\| H \\boldsymbol{w} - \\boldsymbol{y} \\right\\|}_{2}^{2} + \\lambda {L}_{\\delta} \\left( D \\boldsymbol{w} \\right) $$\n",
    "\n",
    "Where ${L}_{\\delta} \\left( D \\boldsymbol{w} \\right) = \\sum_{i = 1}^{N - 1} \\operatorname{PH}_{\\delta} \\left( {w}_{i + 1} - {w}_{i} \\right)$ with $\\operatorname{PH}_{\\delta} \\left( x \\right) = {\\delta}^{2} \\left( \\sqrt{1 + {\\left(\\frac{x}{\\delta}\\right)}^{2}} - 1 \\right)$.\n",
    "\n",
    "We'll use a vanilla gradient descent where the gradient will be calculated by the _auto differentiation_ framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smooth Absolute Value Function\n",
    "# Show the different approximations of the absolute value function\n",
    "vXX = np.linspace(-5, 5, numGrdiPts)\n",
    "\n",
    "hF, hA = plt.subplots(figsize = (8, 8))\n",
    "\n",
    "δ = 0.75\n",
    "\n",
    "hA.plot(vXX, np.abs(vXX), label = 'Abs')\n",
    "hA.plot(vXX, PseudoHuber(vXX, δ = δ), label = 'Pseudo Huber')\n",
    "# hA.plot(vXX, sp.special.pseudo_huber(δ, vXX), label = 'Pseudo Huber')\n",
    "hA.plot(vXX, sp.special.huber(δ, vXX), label = 'Huber')\n",
    "\n",
    "hA.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Loss Function and Loss Function Gradient\n",
    "def LossFun(mH: np.ndarray, vW: np.ndarray, vY: np.ndarray, λ: float, mD: np.ndarray, δ: float):\n",
    "    vT = anp.dot(mH, vW) - vY\n",
    "    l2Loss = anp.dot(vT, vT)\n",
    "    vL = anp.dot(mD, vW)\n",
    "    huberLoss = ApproxL1Norm(vL, δ = δ)\n",
    "\n",
    "    return (0.5 * l2Loss) + (λ * huberLoss)\n",
    "\n",
    "def LossFunGrad( mH: np.ndarray, vW: np.ndarray, vY: np.ndarray, λ: float, mD: np.ndarray, δ: float ):\n",
    "    # Gradient with respect to `vW`\n",
    "\n",
    "    vL2LossGrad = mH.T @ ((mH @ vW) - vY)\n",
    "    vD = mD @ vW\n",
    "    vRegGrad = mD.T @ np.array([PseudoHuberGrad(x, δ = δ) for x in vD])\n",
    "\n",
    "    return vL2LossGrad + (λ * vRegGrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Functions\n",
    "# Define the functions with respect to `vW`\n",
    "λ = 0.005\n",
    "hLossFun        = lambda vW: LossFun(mH, vW, vY, λ, mD, δ)\n",
    "hLossFunGrad    = grad(hLossFun)\n",
    "hLossFunGradAna = lambda vW: LossFunGrad(mH, vW, vY, λ, mD, δ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the Auto Differentiation\n",
    "\n",
    "vW = np.random.randn(numSamples)\n",
    "\n",
    "maxDev = np.max(np.abs(hLossFunGrad(vW) - hLossFunGradAna(vW)))\n",
    "\n",
    "print(f'The maximum deviation between the Auto Diff and analytic gradient: {maxDev}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent\n",
    "\n",
    "numIterations = 30_000\n",
    "stepSize = 0.0015\n",
    "\n",
    "mW = np.zeros(shape = (numSamples, numIterations)) #<! Estimation\n",
    "mW[:, 0] = vY\n",
    "\n",
    "mG = np.zeros(shape = (numSamples, numIterations)) #<! Gradient\n",
    "mG[:, 0] = hLossFun(mW[:, 0])\n",
    "\n",
    "vL = np.zeros(numIterations) #<! Loss Function\n",
    "vL[0] = hLossFun(mW[:, 0])\n",
    "\n",
    "for ii in range(1, numIterations):\n",
    "    vG = hLossFunGrad(mW[:, ii - 1])\n",
    "    # vG = hLossFunGradAna(mW[:, ii - 1])\n",
    "    mW[:, ii] = mW[:, ii - 1] - (stepSize * vG)\n",
    "    mG[:, ii] = vG\n",
    "    vL[ii] = hLossFun(mW[:, ii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hF, hAs = plt.subplots(nrows = 1, ncols = 3, figsize = (24, 8))\n",
    "hAs = hAs.flat\n",
    "\n",
    "hAs[0].scatter(vX, mW[:, -1], label = 'Prediction')\n",
    "hAs[0].scatter(vX, vY, label = 'Measurements')\n",
    "hAs[0].set_xlabel('Sample Index')\n",
    "hAs[0].set_ylabel('Sample Value')\n",
    "hAs[0].legend()\n",
    "\n",
    "hAs[1].plot(range(numIterations), np.linalg.norm(mG, axis = 0))\n",
    "hAs[1].set_xlabel('Iteration Index')\n",
    "hAs[1].set_ylabel('Gradient Norm')\n",
    "\n",
    "hAs[2].plot(range(numIterations), vL, label = 'By Optimization')\n",
    "hAs[2].plot(range(numIterations), hLossFun(vY1) * np.ones(numIterations), label = 'Ground Truth')\n",
    "hAs[2].set_xlabel('Iteration Index')\n",
    "hAs[2].set_ylabel('Loss Fun')\n",
    "hAs[2].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stability of mH (Condition Number)\n",
    "vSingularValues = sp.linalg.svdvals(mH)\n",
    "\n",
    "vSingularValues = np.abs(vSingularValues)\n",
    "\n",
    "np.max(vSingularValues) / np.min(vSingularValues) #<! Matches np.linalg.cond(mH)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
