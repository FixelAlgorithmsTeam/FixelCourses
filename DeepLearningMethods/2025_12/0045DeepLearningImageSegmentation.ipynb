{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Fixel Algorithms](https://fixelalgorithms.co/images/CCExt.png)](https://fixelalgorithms.gitlab.io)\n",
    "\n",
    "# Deep Learning Methods\n",
    "\n",
    "## Deep Learning - Image to Image - Image Segmentation with U-Net\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 1.0.002 | 02/02/2026 | Royi Avital | Expanded the information on the Inverted Residual Block            |\n",
    "| 1.0.001 | 01/02/2026 | Royi Avital | Simplified the classification head                                 |\n",
    "| 1.0.000 | 21/01/2026 | Royi Avital | First version                                                      |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/AIProgram/2024_02/0099DeepLearningObjectDetection.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T09:30:06.492269Z",
     "start_time": "2022-02-02T09:30:06.220934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Image Processing and Computer Vision\n",
    "import skimage as ski\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn            as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torchinfo\n",
    "\n",
    "from torchmetrics.functional.segmentation import mean_iou\n",
    "from torchmetrics.functional.classification import multiclass_f1_score\n",
    "\n",
    "import torchvista\n",
    "\n",
    "import torchvision\n",
    "from torchvision.io import decode_image\n",
    "from torchvision.transforms import v2 as TorchVisionTrns\n",
    "\n",
    "# Miscellaneous\n",
    "import os\n",
    "from platform import python_version\n",
    "import random\n",
    "import shutil\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# Typing\n",
    "from typing import Callable, Dict, Generator, List, Optional, Self, Set, Tuple, Union\n",
    "from numpy.typing import NDArray\n",
    "from torch import Tensor\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought.\n",
    "\n",
    "Code Notations:\n",
    "\n",
    "```python\n",
    "someVar    = 2; #<! Notation for a variable\n",
    "vVector    = np.random.rand(4) #<! Notation for 1D array\n",
    "mMatrix    = np.random.rand(4, 3) #<! Notation for 2D array\n",
    "tTensor    = np.random.rand(4, 3, 2, 3) #<! Notation for nD array (Tensor)\n",
    "tuTuple    = (1, 2, 3) #<! Notation for a tuple\n",
    "lList      = [1, 2, 3] #<! Notation for a list\n",
    "dDict      = {1: 3, 2: 2, 3: 1} #<! Notation for a dictionary\n",
    "oObj       = MyClass() #<! Notation for an object\n",
    "dfData     = pd.DataFrame() #<! Notation for a data frame\n",
    "dsData     = pd.Series() #<! Notation for a series\n",
    "hObj       = plt.Axes() #<! Notation for an object / handler / function handler\n",
    "```\n",
    "\n",
    "### Code Exercise\n",
    "\n",
    " - Single line fill\n",
    "\n",
    "```python\n",
    "valToFill = ???\n",
    "```\n",
    "\n",
    " - Multi Line to Fill (At least one)\n",
    "\n",
    "```python\n",
    "# You need to start writing\n",
    "?????\n",
    "```\n",
    "\n",
    " - Section to Fill\n",
    "\n",
    "```python\n",
    "#===========================Fill This===========================#\n",
    "# 1. Explanation about what to do.\n",
    "# !! Remarks to follow / take under consideration.\n",
    "mX = ???\n",
    "\n",
    "?????\n",
    "#===============================================================#\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# %matplotlib inline\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "# Matplotlib default color palette\n",
    "lMatPltLibclr = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "# sns.set_theme() #>! Apply SeaBorn theme\n",
    "\n",
    "runInGoogleColab = 'google.colab' in str(get_ipython())\n",
    "\n",
    "# Improve performance by benchmarking\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Reproducibility (Per PyTorch Version on the same device)\n",
    "# torch.manual_seed(seedNum)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark     = False #<! Makes things slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "FIG_SIZE_DEF    = (8, 8)\n",
    "ELM_SIZE_DEF    = 50\n",
    "CLASS_COLOR     = ('b', 'r')\n",
    "EDGE_COLOR      = 'k'\n",
    "MARKER_SIZE_DEF = 10\n",
    "LINE_WIDTH_DEF  = 2\n",
    "\n",
    "PROJECT_NAME     = 'FixelCourses'\n",
    "DATA_FOLDER_NAME = 'DataSets'\n",
    "BASE_FOLDER_PATH = os.getcwd()[:(len(os.getcwd()) - (os.getcwd()[::-1].lower().find(PROJECT_NAME.lower()[::-1])))]\n",
    "DATA_FOLDER_PATH = os.path.join(BASE_FOLDER_PATH, DATA_FOLDER_NAME)\n",
    "\n",
    "TENSOR_BOARD_BASE   = 'TB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Auxiliary Modules for Google Colab\n",
    "if runInGoogleColab:\n",
    "    !wget https://raw.githubusercontent.com/FixelAlgorithmsTeam/FixelCourses/master/AIProgram/2024_02/DataManipulation.py\n",
    "    !wget https://raw.githubusercontent.com/FixelAlgorithmsTeam/FixelCourses/master/AIProgram/2024_02/DataVisualization.py\n",
    "    !wget https://raw.githubusercontent.com/FixelAlgorithmsTeam/FixelCourses/master/AIProgram/2024_02/DeepLearningPyTorch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courses Packages\n",
    "\n",
    "from DataManipulation import DownloadKaggleDataset\n",
    "from DataVisualization import PlotLabelsHistogram\n",
    "from DeepLearningPyTorch import GenDataLoaders, GetBatch, TrainModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Auxiliary Functions\n",
    "\n",
    "def TensorImageNumpy( tZ: Tensor ) -> NDArray:\n",
    "    \"\"\"\n",
    "    Converts a PyTorch Tensor to a Numpy Array.\n",
    "    \"\"\"\n",
    "    mZ = tZ.squeeze()\n",
    "    mX = mZ.detach().cpu().numpy()\n",
    "\n",
    "    return mX\n",
    "\n",
    "def TensorImgNumpy( tI: Tensor ) -> NDArray:\n",
    "    \"\"\"\n",
    "    Converts a PyTorch Tensor Image to a Numpy Array Image.\n",
    "    \"\"\"\n",
    "    \n",
    "    mI = TensorImageNumpy(tI.permute(1, 2, 0))\n",
    "\n",
    "    return mI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image to Image Models\n",
    "\n",
    "_Image to Image_ models (Also called `Pix2Pix` / _Image to Image Translation_) are models which transform the input image into an output image.  \n",
    "There many applications of such models:\n",
    "\n",
    " - Feature Extraction    \n",
    "   Extract edges, corners, Mask, etc...\n",
    " - Color Adjustment  \n",
    "   Apply RGB to Gray / Gray to RGB transformations.  \n",
    "   Adjust White Balance / Tonal Curve.\n",
    " - Styling  \n",
    "   Style transfer, style application.\n",
    " - Modality Transformation  \n",
    "   RGB to IR / RGB to SAR and vice versa.  \n",
    "   Image to Map.\n",
    " - Deconvolution  \n",
    "   Super Resolution, Deblurring, Denoising.\n",
    "\n",
    "The applications drove many developments in the Deep Learning field:\n",
    "\n",
    " - Architectures  \n",
    "   U-Net, ViT.\n",
    " - Training Approach / Loss  \n",
    "   Variational Auto Encoder (VAE), Generative Adversarial Network (GAN), Diffusion Models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Segmentation\n",
    "\n",
    "_Image Segmentation_ is one of the most popular applications in the _Image to Image_ paradigm.  \n",
    "It is composed of 3 types of segmentation:\n",
    "\n",
    "| Type     \t| Example                              \t| Properties                                                                     |\n",
    "|----------\t|--------------------------------------\t|------------------------------------------------------------------------------- |\n",
    "| Semantic \t| ![](https://i.imgur.com/cnerHbN.png) \t| Classify each pixel by the class it represents                                 |\n",
    "| Instance \t| ![](https://i.imgur.com/qbAoMk3.png) \t| Classify pixels of countable objects, each object by its own label             |\n",
    "| Panoptic \t| ![](https://i.imgur.com/J99Rzpu.png) \t| Classify instances of countable objects and pixels of uncountable objects \t |\n",
    "\n",
    "This notebook demonstrates:\n",
    " - Working with real world data.\n",
    " - Augmentation of images and masks.\n",
    " - Building a model for _Image Segmentation_ based on U-Net.\n",
    " - Training a model with a composed objective.\n",
    " - Loading optimal weights for inference.\n",
    "\n",
    "</br>\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The U-Net model was original created in the context of medical imaging: [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# Data\n",
    "kggleUser       = 'girish17019'\n",
    "kaggleDataset   = 'mobile-phone-defect-segmentation-dataset'\n",
    "tmpFileName     = 'TMP.zip'\n",
    "dCls            = {0: 'None', 1: 'Scratch', 2: 'Stain', 3: 'Oil'} #<! Defect Classes\n",
    "lCls            = list(dCls.keys())\n",
    "numCls          = len(lCls) #<! Number of classes\n",
    "tuImgSize       = (320, 576) #<! Factor of 32 and similar aspect ratio to original images (1080, 1920)\n",
    "\n",
    "# Model\n",
    "numFiltersBase = 16\n",
    "weightSeg      = 3.0\n",
    "weightCls      = 1.0\n",
    "segThr         = 0.5\n",
    "\n",
    "# Training\n",
    "trainSampleRatio = 0.9 #<! Ratio of training images to total images\n",
    "valSampleRatio   = 1 - trainSampleRatio #<! Ratio of validation images to total images\n",
    "\n",
    "batchSize   = 8\n",
    "numWorkers  = 0 #<! Number of workers\n",
    "numEpochs   = 35\n",
    "\n",
    "# Optimizer\n",
    "ηOpt        = 1e-4        #<! Optimizer learning rate (Has no effect if using scheduler)\n",
    "tuβ         = (0.9, 0.99) #<! Betas for ADAM like Optimizer\n",
    "weightDecay = 5e-5        #<! Optimizer weight decay\n",
    "ηSch        = 7.5e-5      #<! Scheduler learning rate\n",
    "\n",
    "# Visualization\n",
    "numImg = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate / Load Data\n",
    "\n",
    "The data is the [Mobile Phone Screen Surface Defect Segmentation Dataset (MSD)](https://github.com/jianzhang96/MSD).  \n",
    "\n",
    "* <font color='brown'>(**#**)</font> The data is downloaded by the Kaggle version of the dataset: [Kaggle - Mobile Phone Screen Surface Defect Segmentation Dataset](https://www.kaggle.com/datasets/girish17019/mobile-phone-defect-segmentation-dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and Parse the Dataset\n",
    "\n",
    "DownloadKaggleDataset(kggleUser, kaggleDataset, tmpFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Files\n",
    "# Will create:\n",
    "# - `FixelCourses/DataSets/MSD/Images` - Contains all images.\n",
    "# - `FixelCourses/DataSets/MSD/Masks` - Contains all masks.\n",
    "# Files are: `<Class>_<ImgIdxCls>.png`\n",
    "\n",
    "datasetFolderPath     = os.path.join(DATA_FOLDER_PATH, 'MSD')\n",
    "imgDatasetFolderPath  = os.path.join(datasetFolderPath, 'Images')\n",
    "maskDatasetFolderPath = os.path.join(datasetFolderPath, 'Masks')\n",
    "\n",
    "# Delete existing folders if any\n",
    "if os.path.exists(datasetFolderPath):\n",
    "    shutil.rmtree(datasetFolderPath)\n",
    "\n",
    "os.makedirs(imgDatasetFolderPath, exist_ok = True)\n",
    "os.makedirs(maskDatasetFolderPath, exist_ok = True)\n",
    "\n",
    "with ZipFile(tmpFileName, 'r') as zipFile:\n",
    "    lFiles = zipFile.namelist()\n",
    "    for file in lFiles:\n",
    "        fileName = os.path.basename(file)\n",
    "        # Make the class explicit in the name\n",
    "        fileName = fileName.replace('Scr', 'Scratch')\n",
    "        fileName = fileName.replace('Sta', 'Stain')\n",
    "        if file.startswith('oil/') or file.startswith('scratch/') or file.startswith('stain/'):\n",
    "            # Using `extract()` keeps the folder structure\n",
    "            with open(os.path.join(imgDatasetFolderPath, fileName), 'wb') as hFile:\n",
    "                hFile.write(zipFile.read(file))\n",
    "        elif file.startswith('good/'):\n",
    "            # Do not have masks, create an empty mask\n",
    "            fileName = 'None_' + fileName\n",
    "            with open(os.path.join(imgDatasetFolderPath, fileName), 'wb') as hFile:\n",
    "                hFile.write(zipFile.read(file))\n",
    "            \n",
    "            mI = ski.io.imread(os.path.join(imgDatasetFolderPath, fileName))\n",
    "            mI = np.zeros_like(mI)\n",
    "            ski.io.imsave(os.path.join(maskDatasetFolderPath, fileName), mI, check_contrast = False)\n",
    "        elif file.startswith('ground_truth_1/') or file.startswith('ground_truth_2/'):\n",
    "            with open(os.path.join(maskDatasetFolderPath, fileName), 'wb') as hFile:\n",
    "                hFile.write(zipFile.read(file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Go through files using the OS's image viewer. Specifically the mask images. What can you say about the classes per image?\n",
    "\n",
    "<!-- Each image mask contain only a single class. Hence predicting the mask class can be done in global manner and not in a per pixel manner. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataSet\n",
    "\n",
    "Generate a `DataSet` class as a loader of the data.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> Since each image contains a single class, the mask can be represented as a binary mask for the defect area and a global class label by a classification head.\n",
    "* <font color='brown'>(**#**)</font> There images with no defects. Hence a `None` class should be added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Dataset Class\n",
    "\n",
    "class MSDDataset(Dataset):\n",
    "    def __init__( self, imgFolderPath: str, maskFolderPath: str, dCls: Dict[str, int], /, *, hTrns: Optional[Callable] = None, lImgFormats: List = ['jpg', 'jpeg', 'png'] ) -> None:\n",
    "        \"\"\"\n",
    "        Mobile Phone Defect Segmentation Dataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        imgFolderPath : str\n",
    "            Path to the folder containing images.\n",
    "        maskFolderPath : str\n",
    "            Path to the folder containing masks.\n",
    "        hTrns : Optional[Callable], optional\n",
    "            Transform to be applied on a sample, by default None. Must be TorchVision v2 transforms compatible.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        lImgFiles = os.listdir(imgFolderPath)\n",
    "        lImgFiles = [ff for ff in lImgFiles if (ff.split('.')[-1].lower() in lImgFormats) and (os.path.isfile(os.path.join(maskFolderPath, ff.split('.')[0] + '.png')))]\n",
    "        lMskFiles = [ff.split('.')[0] + '.png' for ff in lImgFiles] \n",
    "        numFiles  = len(lImgFiles)\n",
    "\n",
    "        dClsIdx = {v: k for k, v in dCls.items()}\n",
    "\n",
    "        self._imgFolderPath  = imgFolderPath\n",
    "        self._maskFolderPath = maskFolderPath\n",
    "        self._dCls           = dCls\n",
    "        self._dClsIdx        = dClsIdx\n",
    "        self._hTrns          = hTrns\n",
    "\n",
    "        # Must be after setting `self._dCls`\n",
    "        lImgCls   = [self._ParseCls(ff) for ff in lImgFiles]\n",
    "\n",
    "        self._lImgFiles = lImgFiles\n",
    "        self._lMskFiles = lMskFiles\n",
    "        self._lImgCls   = lImgCls\n",
    "        self._numFiles  = numFiles\n",
    "    \n",
    "    def __len__( self ) -> int:\n",
    "        \"\"\"\n",
    "        Returns the number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        return self._numFiles\n",
    "    \n",
    "    def __getitem__( self, idx: int ) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Returns the sample at index `idx`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : int\n",
    "            Index of the sample to be fetched.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[Tensor, int]\n",
    "            A tuple containing the image tensor and the class label.\n",
    "        \"\"\"\n",
    "        imgFile = self._lImgFiles[idx]\n",
    "        mskFile = self._lMskFiles[idx]\n",
    "\n",
    "        imgPath = os.path.join(self._imgFolderPath, imgFile)\n",
    "        mskPath = os.path.join(self._maskFolderPath, mskFile)\n",
    "\n",
    "        # TorchVision's `decode_image()` returns a tensor (C x H x W)\n",
    "        tI = decode_image(imgPath, mode = 'RGB') #<! Guarantees 3 channels\n",
    "        tM = decode_image(mskPath, mode = 'RGB')   #<! Single channel\n",
    "        clsLbl = self._lImgCls[idx]\n",
    "\n",
    "        # Binary Mask where defect area is 1, else 0\n",
    "        tB = (tM.sum(dim = 0) > 0).to(torch.long) #<! (H x W)\n",
    "        tM = clsLbl * tB #<! (H x W)\n",
    "\n",
    "        tI = torchvision.tv_tensors.Image(tI, dtype = torch.uint8)\n",
    "        tM = torchvision.tv_tensors.Mask(tM, dtype = torch.long)\n",
    "\n",
    "        if self._hTrns:\n",
    "            tI, tM = self._hTrns(tI, tM)\n",
    "\n",
    "        return tI, tM\n",
    "    \n",
    "    def _ParseCls( self, fileName: str ) -> int:\n",
    "        \"\"\"\n",
    "        Parses the class label from the file name.\n",
    "        Parameters\n",
    "        ----------\n",
    "        fileName : str\n",
    "            File name of the image.\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            Class label.\n",
    "        \"\"\"\n",
    "        clsStr = fileName.split('_')[0]\n",
    "        clsLbl = self._dClsIdx[clsStr]\n",
    "\n",
    "        return clsLbl\n",
    "    \n",
    "    def SetTransforms( self, hTrns: Callable ) -> None:\n",
    "        \"\"\"\n",
    "        Sets the transforms to be applied on a sample.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        hTrns : Callable\n",
    "            Transform to be applied on a sample. Must be TorchVision v2 transforms compatible.\n",
    "        \"\"\"\n",
    "        self._hTrns = hTrns\n",
    "    \n",
    "    def GetClasses( self ) -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        Returns the class dictionary.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Dict[str, int]\n",
    "            Class dictionary.\n",
    "        \"\"\"\n",
    "        \n",
    "        return self._dCls.copy()\n",
    "    \n",
    "    def GetLabels( self ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Returns the list of labels.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List[int]\n",
    "            List of labels.\n",
    "        \"\"\"\n",
    "        \n",
    "        return self._lImgCls.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Dataset\n",
    "\n",
    "dsData     = MSDDataset(imgDatasetFolderPath, maskDatasetFolderPath, dCls, hTrns = None)\n",
    "numSamples = len(dsData)\n",
    "\n",
    "print(f'Number of samples in the dataset: {numSamples}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Random Samples from the Dataset\n",
    "\n",
    "rndIdx = random.randint(0, numSamples - 1)\n",
    "tI, tM = dsData[rndIdx]\n",
    "\n",
    "tI = TensorImgNumpy(tI)\n",
    "tM = TensorImageNumpy(tM)\n",
    "\n",
    "hF, vHa = plt.subplots(1, 2, figsize = (12, 6))\n",
    "vHa = vHa.flat\n",
    "\n",
    "hA = vHa[0]\n",
    "hA.imshow(tI)\n",
    "hA.set_title('Image')\n",
    "hA.axis('off');\n",
    "\n",
    "hA = vHa[1]\n",
    "hA.imshow(tM, vmin = 0, vmax = numCls - 1, cmap = 'jet')\n",
    "hA.set_title(f'Mask, Class: {dCls[tM.max()]}')\n",
    "hA.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Labels Histogram\n",
    "\n",
    "lLablels = dsData.GetLabels()\n",
    "\n",
    "hF, hA = plt.subplots(figsize = FIG_SIZE_DEF)\n",
    "hA = PlotLabelsHistogram(lLablels, hA, lClass = list(dCls.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> In most cases _Image Segmentation_ deals with imbalanced classification.  \n",
    "  One of the effective ways to deal with it is using the Focal Loss in the binary case ([Extension to Multi Class](https://discuss.pytorch.org/t/61289)) or approaches like [Class-Balanced Loss Based on Effective Number of Samples](https://arxiv.org/abs/1901.05555)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loader Transform\n",
    "\n",
    "oTrns = TorchVisionTrns.Compose([\n",
    "    # TorchVisionTrns.ToImage(),\n",
    "    TorchVisionTrns.Resize(tuImgSize),\n",
    "    TorchVisionTrns.ToDtype(torch.float, scale = True),\n",
    "    TorchVisionTrns.RandomChoice([\n",
    "            TorchVisionTrns.RandomGrayscale(p = 1.0),\n",
    "            TorchVisionTrns.RandomHorizontalFlip(p = 1.0),\n",
    "            TorchVisionTrns.RandomVerticalFlip(p = 1.0),\n",
    "            TorchVisionTrns.RandomRotation(degrees = 10),\n",
    "            TorchVisionTrns.RGB(), #<! Identity for RGB Images\n",
    "        ], p = [0.15, 0.15, 0.15, 0.15, 0.40]),\n",
    "])\n",
    "\n",
    "dsData.SetTransforms(oTrns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='blue'>(**!**)</font> Add color related augmenation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Random Samples from the Dataset\n",
    "\n",
    "rndIdx = random.randint(0, numSamples - 1)\n",
    "tI, tM = dsData[rndIdx]\n",
    "\n",
    "tI = TensorImgNumpy(tI)\n",
    "tM = TensorImageNumpy(tM)\n",
    "\n",
    "hF, vHa = plt.subplots(1, 2, figsize = (12, 6))\n",
    "vHa = vHa.flat\n",
    "\n",
    "hA = vHa[0]\n",
    "hA.imshow(tI)\n",
    "hA.set_title('Image')\n",
    "hA.axis('off');\n",
    "\n",
    "hA = vHa[1]\n",
    "hA.imshow(tM, vmin = 0, vmax = numCls - 1, cmap = 'jet')\n",
    "hA.set_title(f'Mask, Class: {dCls[tM.max()]}')\n",
    "hA.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Training and Validation Datasets\n",
    "\n",
    "vIdxTrain, vIdxVal = train_test_split(np.arange(numSamples), test_size = valSampleRatio, train_size = trainSampleRatio, random_state = seedNum, shuffle = True, stratify = lLablels)\n",
    "\n",
    "dsTrain = torch.utils.data.Subset(dsData, vIdxTrain)\n",
    "dsVal   = torch.utils.data.Subset(dsData, vIdxVal)\n",
    "\n",
    "print(f'The training data set contains  : {len(dsTrain):4d} samples.')\n",
    "print(f'The validation data set contains: {len(dsVal):4d} samples.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> One could use negative values for the bounding box. The model will extrapolate the object dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader\n",
    "\n",
    "dlTrain, dlVal = GenDataLoaders(dsTrain, dsVal, batchSize, numWorkers = numWorkers, persWork = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Why are lists used instead of arrays for the labels and the bounding boxes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Element of the Data Set / Data Sample\n",
    "\n",
    "tX, valY = dsTrain[0]\n",
    "\n",
    "print(f'The features shape: {tX.shape}')\n",
    "print(f'The features type : {tX.dtype}')\n",
    "print(f'The labels shape  : {tM.shape}')\n",
    "print(f'The labels type   : {tM.dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Element of the Dataloader\n",
    "\n",
    "tX, tM = GetBatch(dlTrain)\n",
    "\n",
    "print(f'The batch of features shape: {tX.shape}')\n",
    "print(f'The batch of features type : {tX.dtype}')\n",
    "print(f'The batch of labels shape  : {tM.shape}')\n",
    "print(f'The batch of labels type   : {tM.dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> Since the labels are in the same contiguous container as the bounding box parameters, their type is `Float`.\n",
    "* <font color='brown'>(**#**)</font> The bounding box is using absolute values. In practice it is commonly normalized to the image dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model\n",
    "\n",
    "The U-Net Models rely heavily on the Transposed Convolution operator.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Transposed Convolution Operator\n",
    "\n",
    "The _Transposed Convolution Operator_ can be used as a learned upscaling operator.  \n",
    "The layers was crucial to achieve high quality upscaling for 1D and 2D signals.  \n",
    "As opposed to interpolation, the effect is not pre defined but learned from data.\n",
    "\n",
    "![](https://i.imgur.com/a9KBYKC.png)\n",
    "<!-- ![](https://i.postimg.cc/Qxjjwrhz/Diagrams-Transposed-Convolution.png) -->\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The Transposed Convolution operator is given by the [`ConvTranspose2d`](https://docs.pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html) layer.\n",
    "* <font color='brown'>(**#**)</font> There are 2 other views of the Transposed Convolution operator:\n",
    "  - If the Convolution Operator can be represented by a matrix $\\boldsymbol{W}$ such that: $\\boldsymbol{z} = \\boldsymbol{W} \\boldsymbol{x}$ then the Transposed Convolution is given by $\\boldsymbol{x} = \\boldsymbol{W}^{\\top} \\boldsymbol{z}$.\n",
    "  - The Transposed Convolution can be achieved by 2 steps (Similar to Signal Processing like approach):\n",
    "    - Upsample (Insert zeros) between the data samples.\n",
    "    - Apply _convolution_ as an _Low Pass Filter_ (LPF).\n",
    "* <font color='brown'>(**#**)</font> In depth discussions on convolutions layers: [Distill - Augustus Odena, Vincent Dumoulin - Deconvolution and Checkerboard Artifacts](https://distill.pub/2016/deconv-checkerboard/), [StackExchange Cross Validated - The Equivalence of Upsample Layer and Transposed Convolution Layer](https://stats.stackexchange.com/questions/252810).\n",
    "* <font color='brown'>(**#**)</font> Convolution Layer Visualization: [GitHub - Convolution Arithmetic](https://github.com/vdumoulin/conv_arithmetic), [Vincent Dumoulin, Francesco Visin - A guide to Convolution Arithmetic for Deep Learning](https://arxiv.org/abs/1603.07285)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverted Residual Block\n",
    "\n",
    "The concept of Inverted Residual Block was introduced in the papers on EfficientNet ([EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946)) and [MobileNet](https://en.wikipedia.org/wiki/MobileNet).\n",
    "\n",
    "While classic _Residual Block_ has the bottleneck in the middle: Wide -> Narrow -> Wide, the _Inverted Residual Block_ starts narrow: Narrow -> Wide -> Narrow.\n",
    "\n",
    "![](https://i.imgur.com/g1IhUaH.png)\n",
    "<!-- ![](https://i.postimg.cc/cH1zXvKx/Diagrams-Inverted-Residual-Block.png) -->\n",
    "\n",
    "The _Inverted Residual Block_ also uses Depthwise Convolution to reduce the number of parameters and removes the last activation layer to save computation time.\n",
    "\n",
    "In [MobileNet v4](https://arxiv.org/abs/2404.10518) it was generalized by the _Universal Inverted Residual Block_ (UIB):\n",
    "\n",
    "<img src=\"https://i.imgur.com/ARDgVh2.png\" width=720 >\n",
    "<!-- <img src=\"https://i.postimg.cc/sXppQmfP/image.png\" width=720 > -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverted Residual Block\n",
    "\n",
    "class InvertedResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Modern block structure: Expand -> Depth Wise Convolution -> Project.\n",
    "    Includes a skip connection if input and output shapes match.\n",
    "    \"\"\"\n",
    "    def __init__(self, numChnlIn: int, numChnlOut: int, expFctr: int = 4, strideSize: int = 1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.strideSize = strideSize\n",
    "        self.enableSkip = (strideSize == 1 and numChnlIn == numChnlOut)\n",
    "        hiddenDim       = numChnlIn * expFctr\n",
    "\n",
    "        self.oBlock = nn.Sequential(\n",
    "            # Expansion of Channels (Using 1x1 Convolution)\n",
    "            nn.Conv2d(numChnlIn, hiddenDim, 1, bias = False),\n",
    "            nn.BatchNorm2d(hiddenDim),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            # Depth Wise Convolution (3x3)\n",
    "            nn.Conv2d(hiddenDim, hiddenDim, 3, stride = strideSize, padding = 1, groups = hiddenDim, bias = False),\n",
    "            nn.BatchNorm2d(hiddenDim),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            # Projection (Using 1x1 Convolution) - Linear bottleneck (No activation at end)\n",
    "            nn.Conv2d(hiddenDim, numChnlOut, 1, bias = False),\n",
    "            nn.BatchNorm2d(numChnlOut),\n",
    "        )\n",
    "\n",
    "    def forward(self, tX: Tensor) -> Tensor:\n",
    "        \n",
    "        if self.enableSkip:\n",
    "            return tX + self.oBlock(tX)\n",
    "        else:\n",
    "            return self.oBlock(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depthwise Separable Convolution Block\n",
    "\n",
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    \"\"\"\n",
    "    The building block of efficient networks.\n",
    "    Splits a standard convolution into:\n",
    "    1. Depthwise: Spatial filtering (lightweight)\n",
    "    2. Pointwise: Channel mixing (1x1 conv)\n",
    "    \"\"\"\n",
    "    def __init__(self, numChnlIn: int, numChnlOut: int, strideSize: int = 1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.strideSize = strideSize\n",
    "        \n",
    "        # Depthwise Convolution (3x3) (Each kernel per channel)\n",
    "        self.oBlock001 = nn.Sequential(\n",
    "            nn.Conv2d(numChnlIn, numChnlIn, kernel_size = 3, padding = 1,  stride = strideSize, groups = numChnlIn, bias = False),\n",
    "            nn.BatchNorm2d(numChnlIn),\n",
    "            nn.SiLU(), #<! Modern activation (Swish)\n",
    "        )\n",
    "        # Pointwise Convolution (1x1) Projection over channels\n",
    "        self.oBlock002 = nn.Sequential(\n",
    "            nn.Conv2d(numChnlIn, numChnlOut, kernel_size = 1, bias = False),\n",
    "            nn.BatchNorm2d(numChnlOut),\n",
    "            nn.SiLU(), #<! Modern activation (Swish)\n",
    "        )\n",
    "\n",
    "    def forward(self, tX: Tensor) -> Tensor:\n",
    "        \n",
    "        tX = self.oBlock001(tX)\n",
    "        tX = self.oBlock002(tX)\n",
    "        \n",
    "        return tX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing the Model for the Case\n",
    "\n",
    "Since in the data above there is no case of 2 different non background in the same image the task can be separated into 2 simpler tasks:\n",
    " 1. Segment the support of the defect.\n",
    " 2. Classify the type of the defect.\n",
    "\n",
    "In order to achieve it, the model has 2 heads:\n",
    " - Per Pixel Binary Classification Head  \n",
    "   Per pixel calculates the probability of defect.\n",
    " - Global Multi Class classification Head  \n",
    "   Estimate the probability of the class of the defect.\n",
    "\n",
    "Both heads output the _logits_ of the probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNet: Encoder and Decoder Blocks\n",
    "\n",
    "class µSegmentor(nn.Module):\n",
    "    def __init__(self, numChnlIn: int, numCls: int, numFiltersBase: int = 32, *, useConvTrns: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder (Downsampling)\n",
    "        # Standard Feature Extractor block\n",
    "        self.oFeatExt = nn.Sequential(\n",
    "            nn.Conv2d(numChnlIn, numFiltersBase, 3, padding = 1, stride = 2, bias = False),\n",
    "            nn.BatchNorm2d(numFiltersBase),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "\n",
    "        # Encoder Stages (using Inverted Residuals)\n",
    "        # Note: Use `stride = 2` in the first block of a stage to downsample\n",
    "        self.oEnc001 = InvertedResidualBlock(numFiltersBase    , numFiltersBase * 2, strideSize = 2) #<! H/4\n",
    "        self.oEnc002 = InvertedResidualBlock(numFiltersBase * 2, numFiltersBase * 4, strideSize = 2) #<! H/8\n",
    "        self.oEnc003 = InvertedResidualBlock(numFiltersBase * 4, numFiltersBase * 8, strideSize = 2) #<! H/16\n",
    "\n",
    "        # Embedding / Bottleneck\n",
    "        self.oEmbed = InvertedResidualBlock(numFiltersBase * 8, numFiltersBase * 16, strideSize = 1) # H/16\n",
    "\n",
    "        # Decoder (Upsampling)\n",
    "        # Use simple Bilinear Upsampling + 1x1 Conv to reduce channels / Transposed Convolution\n",
    "        \n",
    "        # Process output of `oEnc002`\n",
    "        if useConvTrns:\n",
    "            self.oUp003  = nn.Sequential(\n",
    "                nn.ConvTranspose2d(numFiltersBase * 16, numFiltersBase * 16, kernel_size = 3, stride = 2, padding = 1, output_padding = 1, bias = False),\n",
    "                nn.BatchNorm2d(numFiltersBase * 16),\n",
    "                nn.SiLU(),\n",
    "            )\n",
    "        else:\n",
    "            self.oUp003  = nn.Upsample(scale_factor = 2, mode = 'bilinear', align_corners = False)\n",
    "\n",
    "        self.oDec003 = nn.Sequential(\n",
    "            nn.Conv2d(numFiltersBase * 16 + numFiltersBase * 4, numFiltersBase * 8, 1),\n",
    "            InvertedResidualBlock(numFiltersBase * 8, numFiltersBase * 8),\n",
    "        )\n",
    "\n",
    "        # Process output of `oEnc001`\n",
    "        if useConvTrns:\n",
    "            self.oUp002  = nn.Sequential(\n",
    "                nn.ConvTranspose2d(numFiltersBase * 8, numFiltersBase * 8, kernel_size = 3, stride = 2, padding = 1, output_padding = 1, bias = False),\n",
    "                nn.BatchNorm2d(numFiltersBase * 8),\n",
    "                nn.SiLU(),\n",
    "            )\n",
    "        else:\n",
    "            self.oUp002 = nn.Upsample(scale_factor = 2, mode = 'bilinear', align_corners = False)\n",
    "        self.oDec002 = nn.Sequential(\n",
    "            nn.Conv2d(numFiltersBase * 8 + numFiltersBase * 2, numFiltersBase * 4, 1),\n",
    "            InvertedResidualBlock(numFiltersBase * 4, numFiltersBase * 4),\n",
    "        )\n",
    "\n",
    "        # Process output of `oFeatExt`\n",
    "        if useConvTrns:\n",
    "            self.oUp001  = nn.Sequential(\n",
    "                nn.ConvTranspose2d(numFiltersBase * 4, numFiltersBase * 4, kernel_size = 3, stride = 2, padding = 1, output_padding = 1, bias = False),\n",
    "                nn.BatchNorm2d(numFiltersBase * 4),\n",
    "                nn.SiLU(),\n",
    "            )\n",
    "        else:\n",
    "            self.oUp001 = nn.Upsample(scale_factor = 2, mode = 'bilinear', align_corners = False)\n",
    "        self.oDec001 = nn.Sequential(\n",
    "            nn.Conv2d(numFiltersBase * 4 + numFiltersBase, numFiltersBase * 2, 1),\n",
    "            InvertedResidualBlock(numFiltersBase * 2, numFiltersBase * 2),\n",
    "        )\n",
    "        \n",
    "        # Segmentation Mask Head\n",
    "        # Final upsample to restore original resolution\n",
    "        if useConvTrns:\n",
    "            self.oHeadMask = nn.Sequential(\n",
    "                nn.ConvTranspose2d(numFiltersBase * 2, numFiltersBase * 2, kernel_size = 3, stride = 2, padding = 1, output_padding = 1, bias = False),\n",
    "                nn.BatchNorm2d(numFiltersBase * 2),\n",
    "                nn.SiLU(),\n",
    "                nn.Conv2d(numFiltersBase * 2, 1, 1), #<! Classify any class\n",
    "            )\n",
    "        else:\n",
    "            self.oHeadMask = nn.Sequential(\n",
    "                nn.Upsample(scale_factor = 2, mode = 'bilinear', align_corners = False),\n",
    "                # nn.Conv2d(numFiltersFeatExt * 2, numCls, 1), #<! Classify per class\n",
    "                nn.Conv2d(numFiltersBase * 2, 1, 1), #<! Classify any class\n",
    "            )\n",
    "\n",
    "        # Classification Head\n",
    "        self.oHeadCls = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(numFiltersBase, numCls),\n",
    "        )\n",
    "\n",
    "    def forward( self, tX: Tensor ) -> Tensor:\n",
    "        # tX: [B, 3, H, W]\n",
    "        # H and W must be even numbers\n",
    "        # If H / E are divisible by 16 no interpolation will be needed during skip connections\n",
    "        \n",
    "        # Encoder\n",
    "        tX0 = self.oFeatExt(tX) #<! H/2\n",
    "        tX1 = self.oEnc001(tX0) #<! H/4\n",
    "        tX2 = self.oEnc002(tX1) #<! H/8\n",
    "        tX3 = self.oEnc003(tX2) #<! H/16\n",
    "        \n",
    "        # Lowe Dim Embedding / Bottleneck\n",
    "        tEm = self.oEmbed(tX3) #<! H/16\n",
    "        \n",
    "        # Decoder\n",
    "        # D3: Upsample Bridge, concat with `oEnc002`\n",
    "        tD3 = self.oUp003(tEm)\n",
    "        # Handle potential padding issues if H/W aren't perfect powers of 2\n",
    "        if tD3.shape[2:] != tX2.shape[2:]:\n",
    "            tD3 = F.interpolate(tD3, size = tX2.shape[2:], mode = 'bilinear', align_corners = False)\n",
    "        tD3 = torch.cat([tD3, tX2], dim = 1)\n",
    "        tD3 = self.oDec003(tD3)\n",
    "        \n",
    "        # D2: Upsample D3, concat with `oEnc001`\n",
    "        tD2 = self.oUp002(tD3)\n",
    "        if tD2.shape[2:] != tX1.shape[2:]:\n",
    "            tD2 = F.interpolate(tD2, size = tX1.shape[2:], mode = 'bilinear', align_corners = False)\n",
    "        tD2 = torch.cat([tD2, tX1], dim = 1)\n",
    "        tD2 = self.oDec002(tD2)\n",
    "        \n",
    "        # D1: Upsample D2, concat with `oFeatExt`\n",
    "        tD1 = self.oUp001(tD2)\n",
    "        if tD1.shape[2:] != tX0.shape[2:]:\n",
    "            tD1 = F.interpolate(tD1, size = tX0.shape[2:], mode = 'bilinear', align_corners = False)\n",
    "        tD1 = torch.cat([tD1, tX0], dim = 1) # Concatenating with H/2 features\n",
    "        tD1 = self.oDec001(tD1)\n",
    "        \n",
    "        # Final Output\n",
    "        tO = self.oHeadMask(tD1) #<! Segmentation Mask Head\n",
    "        tY = self.oHeadCls(tEm)  #<! Classification Head\n",
    "            \n",
    "        return tO, tY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "oModel = µSegmentor(numChnlIn = tX.shape[1], numCls = numCls, numFiltersBase = numFiltersBase, useConvTrns = False) #<! Use `useConvTrns = False` for better run time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run device\n",
    "\n",
    "runDevice = torch.device('cuda:0' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu')) #<! The 1st CUDA device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Summary\n",
    "\n",
    "torchinfo.summary(oModel, tX.shape, col_names = ['input_size', 'output_size', 'num_params', 'kernel_size', 'trainable'], device = runDevice, row_settings = ['depth', 'var_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture\n",
    "\n",
    "torchvista.trace_model(oModel, tX.to(runDevice))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Class\n",
    "\n",
    "class SegmentationClsLoss(nn.Module):\n",
    "    def __init__( self, weightSeg: float = 1.0, weightCls: float = 1.0 ) -> None:\n",
    "        \"\"\"\n",
    "        Combined Segmentation and Classification Loss.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        weightSeg : float, optional\n",
    "            Weight for the segmentation loss, by default 1.0\n",
    "        weightCls : float, optional\n",
    "            Weight for the classification loss, by default 1.0\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.weightSeg = weightSeg\n",
    "        self.weightCls = weightCls\n",
    "        \n",
    "        self.oSegLoss = nn.BCEWithLogitsLoss()\n",
    "        self.oClsLoss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward( self, tuZ: Tuple[Tensor, Tensor], tMTgt: Tensor ) -> Tensor:\n",
    "        \"\"\"\n",
    "        Computes the combined loss.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        tPredMask : Tensor\n",
    "            Predicted segmentation mask (B x 1 x H x W).\n",
    "        tTrueMask : Tensor\n",
    "            True segmentation mask (B x H x W).\n",
    "        tPredCls : Tensor\n",
    "            Predicted class logits (B x numCls).\n",
    "        tTrueCls : Tensor\n",
    "            True class labels (B).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            Combined loss.\n",
    "        \"\"\"\n",
    "\n",
    "        tM   = tuZ[0]\n",
    "        mCls = tuZ[1]\n",
    "        tM   = tM.squeeze(1)\n",
    "\n",
    "        # Extract class from the mask\n",
    "        vClsTgt = tMTgt.view(tMTgt.shape[0], -1).amax(dim = 1) #<! (B,)\n",
    "        # Extract the binary mask\n",
    "        tMTgt   = tMTgt.not_equal(0).to(torch.float32) #<! (B, H, W)\n",
    "        \n",
    "        segLoss = self.oSegLoss(tM, tMTgt)\n",
    "        clsLoss = self.oClsLoss(mCls, vClsTgt)\n",
    "        \n",
    "        valLoss = self.weightSeg * segLoss + self.weightCls * clsLoss\n",
    "        \n",
    "        return valLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score Class\n",
    "\n",
    "class SegmentationClsScore(nn.Module):\n",
    "    def __init__( self, segThr: float, weightSeg: float = 1.0, weightCls: float = 1.0 ) -> None:\n",
    "        \"\"\"\n",
    "        Combined Segmentation and Classification Accuracy.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.segThr    = segThr\n",
    "        # Score Weights are normalized so the score is in [0, 1]\n",
    "        self.weightSeg = weightSeg / (weightSeg + weightCls)\n",
    "        self.weightCls = weightCls / (weightSeg + weightCls)\n",
    "    \n",
    "    def forward( self, tuZ: Tuple[Tensor, Tensor], tMTgt: Tensor ) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Computes the combined accuracy.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        tPredMask : Tensor\n",
    "            Predicted segmentation mask (B x 1 x H x W).\n",
    "        tTrueMask : Tensor\n",
    "            True segmentation mask (B x H x W).\n",
    "        tPredCls : Tensor\n",
    "            Predicted class logits (B x numCls).\n",
    "        tTrueCls : Tensor\n",
    "            True class labels (B).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[Tensor, Tensor]\n",
    "            Segmentation accuracy and classification accuracy.\n",
    "        \"\"\"\n",
    "\n",
    "        tM   = tuZ[0]\n",
    "        mCls = tuZ[1]\n",
    "        tB   = (torch.sigmoid(tM.squeeze(1)) > self.segThr).to(torch.long) #<! (B, H, W)\n",
    "        vCls = mCls.argmax(dim = 1) #<! (B,), No need for SoftMax as argmax is invariant to monotonic transforms\n",
    "\n",
    "        # Extract class from the mask\n",
    "        vClsTgt = tMTgt.view(tMTgt.shape[0], -1).amax(dim = 1) #<! (B,)\n",
    "        # Extract the binary mask\n",
    "        tBTgt   = tMTgt.not_equal(0).to(torch.long) #<! (B, H, W)\n",
    "\n",
    "        segScore = mean_iou(tB, tBTgt, num_classes = 2, include_background = True, per_class = False, input_format = 'index').mean() #<! Mean IoU over the batch\n",
    "        clsScore = multiclass_f1_score(vCls, vClsTgt, num_classes = numCls, average = 'macro', top_k = 1, multidim_average = 'global', ignore_index = None, validate_args = False, zero_division = 0) #<! Setting Macro so each class has the same weight\n",
    "        \n",
    "        valScore = self.weightSeg * segScore + self.weightCls * clsScore\n",
    "        \n",
    "        return valScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Score\n",
    "\n",
    "hL = SegmentationClsLoss(weightSeg, weightCls)\n",
    "hS = SegmentationClsScore(segThr = segThr, weightSeg = weightSeg, weightCls = weightCls)\n",
    "hL = hL.to(runDevice)\n",
    "hS = hS.to(runDevice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer Related\n",
    "\n",
    "oOpt = torch.optim.AdamW(oModel.parameters(), lr = ηOpt, betas = tuβ, weight_decay = weightDecay) #<! Define optimizer\n",
    "oSch = torch.optim.lr_scheduler.OneCycleLR(oOpt, max_lr = ηSch, total_steps = numEpochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "\n",
    "oModel = oModel.to(runDevice)\n",
    "_, lTrainLoss, lTrainScore, lValLoss, lValScore, lLearnRate = TrainModel(oModel, dlTrain, dlVal, oOpt, numEpochs, hL, hS, oSch = oSch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training Phase\n",
    "\n",
    "hF, vHa = plt.subplots(nrows = 1, ncols = 3, figsize = (18, 5))\n",
    "vHa = np.ravel(vHa)\n",
    "\n",
    "hA = vHa[0]\n",
    "hA.plot(lTrainLoss, lw = 2, label = 'Train')\n",
    "hA.plot(lValLoss, lw = 2, label = 'Validation')\n",
    "hA.set_title(f'Loss')\n",
    "hA.set_xlabel('Epoch')\n",
    "hA.set_ylabel('Loss')\n",
    "hA.legend()\n",
    "\n",
    "hA = vHa[1]\n",
    "hA.plot(lTrainScore, lw = 2, label = 'Train')\n",
    "hA.plot(lValScore, lw = 2, label = 'Validation')\n",
    "hA.set_title('Score')\n",
    "hA.set_xlabel('Epoch')\n",
    "hA.set_ylabel('Score')\n",
    "hA.legend()\n",
    "\n",
    "hA = vHa[2]\n",
    "hA.plot(lLearnRate, lw = 2)\n",
    "hA.set_title('Learn Rate Scheduler')\n",
    "hA.set_xlabel('Epoch')\n",
    "hA.set_ylabel('Learn Rate');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Best Model\n",
    "\n",
    "oModel.load_state_dict(torch.load('BestModel.pt')['Model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Samples through the Trained Model\n",
    "\n",
    "rndIdx = random.randint(0, len(dsVal) - 1)\n",
    "tX, tY = dsVal[rndIdx]\n",
    "\n",
    "oModel.eval()\n",
    "with torch.inference_mode():\n",
    "    tXb = tX.unsqueeze(0).to(runDevice) #<! Add batch dimension\n",
    "    tuZb = oModel(tXb)\n",
    "    tMb = tuZb[0].squeeze(0).cpu() #<! Remove batch dimension\n",
    "    tYb = tuZb[1].squeeze(0).cpu()\n",
    "\n",
    "    tMHat = torch.sigmoid(tMb) > segThr\n",
    "    tMHat = tMHat.to(torch.long)\n",
    "    valY  = tYb.argmax(dim = 0).item()\n",
    "\n",
    "tI    = TensorImgNumpy(tX)\n",
    "tM    = TensorImageNumpy(tY)\n",
    "tMHat = TensorImageNumpy(tMHat)\n",
    "\n",
    "hF, vHa = plt.subplots(1, 3, figsize = (12, 6))\n",
    "vHa = vHa.flat\n",
    "\n",
    "hA = vHa[0]\n",
    "hA.imshow(tI)\n",
    "hA.set_title('Image')\n",
    "hA.axis('off');\n",
    "\n",
    "hA = vHa[1]\n",
    "hA.imshow(tM, vmin = 0, vmax = numCls - 1, cmap = 'jet')\n",
    "hA.set_title(f'Mask, Class: {dCls[tM.max()]}')\n",
    "hA.axis('off');\n",
    "\n",
    "hA = vHa[2]\n",
    "hA.imshow(tMHat, vmin = 0, vmax = 1, cmap = 'gray')\n",
    "hA.set_title(f'Predicted Mask, Class: {dCls[valY]}')\n",
    "hA.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Which post process operation can be done to improve results?\n",
    "* <font color='green'>(**@**)</font> Add full analysis of the model performance on the validation set: Confusion Matrix for the classification head, Precision-Recall Curve for the segmentation head, etc.\n",
    "* <font color='green'>(**@**)</font> Plot the worst / best cases IoU wise. See the effect of the _segmentation threshold_.\n",
    "* <font color='green'>(**@**)</font> Improve the loss function based on [Loss functions for image segmentation](https://github.com/JunMa11/SegLossOdyssey) and [Losses Used in Segmentation Task](https://github.com/Nacriema/Loss-Functions-For-Semantic-Segmentation)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
