{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Fixel Algorithms](https://fixelalgorithms.co/images/CCExt.png)](https://fixelalgorithms.gitlab.io)\n",
    "\n",
    "# Deep Learning Methods\n",
    "\n",
    "## Deep Learning for Computer Vision - 1D Convolution Net for Audio Classification\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 1.0.000 | 21/12/2025 | Royi Avital | First version                                                      |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/AIProgram/2024_02/0085DeepLearning1DConvFreqEst.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T09:30:06.492269Z",
     "start_time": "2022-02-02T09:30:06.220934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn            as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "import torchinfo\n",
    "import torchvista\n",
    "\n",
    "# Miscellaneous\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import os\n",
    "from platform import python_version\n",
    "import random\n",
    "\n",
    "# Typing\n",
    "from typing import Callable, Dict, Generator, List, Literal, Optional, Self, Sequence, Set, Tuple, Union\n",
    "from numpy.typing import NDArray\n",
    "from torch import Tensor\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought.\n",
    "\n",
    "Code Notations:\n",
    "\n",
    "```python\n",
    "someVar    = 2; #<! Notation for a variable\n",
    "vVector    = np.random.rand(4) #<! Notation for 1D array\n",
    "mMatrix    = np.random.rand(4, 3) #<! Notation for 2D array\n",
    "tTensor    = np.random.rand(4, 3, 2, 3) #<! Notation for nD array (Tensor)\n",
    "tuTuple    = (1, 2, 3) #<! Notation for a tuple\n",
    "lList      = [1, 2, 3] #<! Notation for a list\n",
    "dDict      = {1: 3, 2: 2, 3: 1} #<! Notation for a dictionary\n",
    "oObj       = MyClass() #<! Notation for an object\n",
    "dfData     = pd.DataFrame() #<! Notation for a data frame\n",
    "dsData     = pd.Series() #<! Notation for a series\n",
    "hObj       = plt.Axes() #<! Notation for an object / handler / function handler\n",
    "```\n",
    "\n",
    "### Code Exercise\n",
    "\n",
    " - Single line fill\n",
    "\n",
    "```python\n",
    "valToFill = ???\n",
    "```\n",
    "\n",
    " - Multi Line to Fill (At least one)\n",
    "\n",
    "```python\n",
    "# You need to start writing\n",
    "?????\n",
    "```\n",
    "\n",
    " - Section to Fill\n",
    "\n",
    "```python\n",
    "#===========================Fill This===========================#\n",
    "# 1. Explanation about what to do.\n",
    "# !! Remarks to follow / take under consideration.\n",
    "mX = ???\n",
    "\n",
    "?????\n",
    "#===============================================================#\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# %matplotlib inline\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "# Matplotlib default color palette\n",
    "lMatPltLibclr = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "# sns.set_theme() #>! Apply SeaBorn theme\n",
    "\n",
    "runInGoogleColab = 'google.colab' in str(get_ipython())\n",
    "\n",
    "# Improve performance by benchmarking\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Reproducibility\n",
    "# torch.manual_seed(seedNum)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark     = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "FIG_SIZE_DEF    = (8, 8)\n",
    "ELM_SIZE_DEF    = 50\n",
    "CLASS_COLOR     = ('b', 'r')\n",
    "EDGE_COLOR      = 'k'\n",
    "MARKER_SIZE_DEF = 10\n",
    "LINE_WIDTH_DEF  = 2\n",
    "\n",
    "DATA_FOLDER_NAME           = 'DataSets'\n",
    "TENSOR_BOARD_FOLDER_NAME   = 'TB'\n",
    "\n",
    "BASE_FOLDER_NAME = 'FixelCourses'\n",
    "BASE_FOLDER_PATH = os.getcwd()[:(len(os.getcwd()) - (os.getcwd()[::-1].lower().find(BASE_FOLDER_NAME.lower()[::-1])))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courses Packages\n",
    "\n",
    "from DataManipulation import DownloadUrl\n",
    "from DataVisualization import PlotConfusionMatrix\n",
    "from DeepLearningPyTorch import GenDataLoaders, TrainModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Auxiliary Functions\n",
    "\n",
    "# PyTorch Data Loader\n",
    "class AudioMNISTDataset(Dataset):\n",
    "    def __init__( self, dfData: pd.DataFrame, targetCol: Literal['Digit', 'Speaker', 'Accent', 'Gender', 'NativeSpeaker'], hMap: Optional[Callable[[Union[int, str]], int]], *, hTransform: Optional[Callable] = None, padSize: int = 0, seedNum: Optional[int] = None ) -> None:\n",
    "        \"\"\"\n",
    "        PyTorch Dataset class for the AudioMNIST dataset.\n",
    "        Input:\n",
    "            - filePath (str): Path to the Parquet file containing the AudioMNIST dataset.\n",
    "            - targetCol (str): The target column to be used for classification. \n",
    "                               Options are 'Digit', 'Speaker', 'Accent', 'Gender', 'NativeSpeaker'.\n",
    "            - hMap (Callable[[Union[int, str]], int]): A mapping function to convert target values to integer labels.\n",
    "        \"\"\"\n",
    "\n",
    "        lCol = dfData.columns.tolist()\n",
    "        # Find the index of '0' in `lCol`\n",
    "        signalStartIdx = lCol.index('0')\n",
    "        \n",
    "        self._dfData         = dfData.copy()\n",
    "        self._targetCol      = targetCol\n",
    "        self._numSamples     = len(dfData)\n",
    "        self._hTransform     = hTransform\n",
    "        self._hMap           = hMap\n",
    "        self._signalStartIdx = signalStartIdx\n",
    "        self._winSize        = dfData.shape[1] + padSize #<! Window size to embed the signals into\n",
    "        self._oRng           = random.Random(seedNum)\n",
    "\n",
    "    def __len__( self: Self ) -> int:\n",
    "        \n",
    "        return self._numSamples\n",
    "\n",
    "    def __getitem__( self: Self, idx: int ) -> Tuple[Union[NDArray, Tensor], int]:\n",
    "        # Extract the signal from the Data Frame\n",
    "        # Embeds it into a window of size `self._winSize` with random shift\n",
    "        \n",
    "        numSamples = self._dfData.loc[idx, 'NumSamples']\n",
    "        vA = self._dfData.iloc[idx, self._signalStartIdx:(self._signalStartIdx + numSamples)].to_numpy() #<! Int16 Values\n",
    "        vA = vA.astype(np.int16)\n",
    "        vX = np.zeros(self._winSize, dtype = np.float32)\n",
    "        maxShift = self._winSize - len(vA)\n",
    "        \n",
    "        # Randomly shift the signal within the window\n",
    "        rndShift = self._oRng.randint(0, maxShift)\n",
    "        vX[rndShift:(rndShift + len(vA))] = vA.astype(np.float32)\n",
    "        vX = np.expand_dims(vX, axis = 0) #<! Add channel dimension\n",
    "\n",
    "        valY = self._hMap(self._dfData.loc[idx, self._targetCol])\n",
    "\n",
    "        if self._hTransform is not None:\n",
    "            vX = self._hTransform(vX)\n",
    "\n",
    "        return vX, valY\n",
    "    \n",
    "    def GenTrainValSplit( self: Self, valFraction: float ) -> Tuple[Sequence[int], Sequence[int]]:\n",
    "        \"\"\"\n",
    "        Generates training and validation datasets from the current dataset.\n",
    "        It uses the 'Speaker' column to ensure that samples from the same speaker are not split between training and validation sets.\n",
    "        \n",
    "        Input:\n",
    "            - valFraction (float): Fraction of the dataset to be used for validation.\n",
    "            - shuffle (bool): Whether to shuffle the dataset before splitting.\n",
    "            - randomSeed (Optional[int]): Random seed for shuffling.\n",
    "        Output:\n",
    "            Tuple[NDArray, NDArray]: Training and validation indices.\n",
    "        \"\"\"\n",
    "        \n",
    "        lSpeakerIdx = self._dfData['Speaker'].unique().tolist()\n",
    "        numSpeakers = len(lSpeakerIdx)\n",
    "        \n",
    "        numValSpeakers   = int(np.floor(valFraction * numSpeakers))\n",
    "        numTrainSpeakers = numSpeakers - numValSpeakers\n",
    "\n",
    "        lTrainSpkIdx = self._oRng.sample(lSpeakerIdx, k = numTrainSpeakers)\n",
    "        lValSpkIdx   = [spk for spk in lSpeakerIdx if spk not in lTrainSpkIdx]\n",
    "\n",
    "        # Get indices of the samples for training and validation sets\n",
    "        lTrainIdx = self._dfData[self._dfData['Speaker'].isin(lTrainSpkIdx)].index.tolist()\n",
    "        lValIdx   = self._dfData[self._dfData['Speaker'].isin(lValSpkIdx)].index.tolist()\n",
    "\n",
    "        return lTrainIdx, lValIdx\n",
    "    \n",
    "    def GetWinSize( self: Self ) -> int:\n",
    "        \"\"\"\n",
    "        Returns the window size used for embedding the signals.\n",
    "        \"\"\"\n",
    "        return self._winSize\n",
    "    \n",
    "\n",
    "def DownloadUrlFile( fileUrl: str, destFolderPath: str, fileName: str ) -> str:\n",
    "    \"\"\"\n",
    "    Downloads a file from a URL to a destination folder. \n",
    "    If the destination folder does not exist, it is created.\n",
    "    If the file already exists in the destination folder, it is not downloaded again.\n",
    "\n",
    "    Input:\n",
    "        - fileUrl (str): The URL of the file to download.\n",
    "        - fileName (str): The name to save the file as.\n",
    "        - destFolderPath (str): The destination folder path.\n",
    "\n",
    "    Output:\n",
    "        str: The path to the downloaded file.\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.isdir(destFolderPath):\n",
    "        os.makedirs(destFolderPath)\n",
    "\n",
    "    filePath = os.path.join(destFolderPath, fileName)\n",
    "    filePath = DownloadUrl(fileUrl, filePath)\n",
    "\n",
    "    return filePath\n",
    "\n",
    "def DecimateData( mD: NDArray, vN: NDArray, decFactor: int, winSize: int ) -> Tuple[NDArray, NDArray]:\n",
    "    \"\"\"\n",
    "    Decimates the input data matrix by the specified factor.  \n",
    "    Works on each signal (Row) individually based on its valid length.\n",
    "    Input:\n",
    "        - mD (NDArray): Input data matrix.\n",
    "        - vN (NDArray): Vector of valid lengths for each signal.\n",
    "        - decFactor (int): Decimation factor.\n",
    "        - winSize (int): Window size for the decimated signals.\n",
    "    Output:\n",
    "        - mDDec (NDArray): Decimated data matrix.\n",
    "        - vNDec (NDArray): Vector of valid lengths for each decimated signal.\n",
    "    \"\"\"\n",
    "    \n",
    "    numSignals = mD.shape[0]\n",
    "    mDDec = np.zeros((numSignals, winSize))\n",
    "    vNDec = np.zeros(numSignals, dtype = np.int32)\n",
    "\n",
    "    for ii in range(numSignals):\n",
    "        vA = mD[ii, :vN[ii]]\n",
    "        vADec = sp.signal.decimate(vA, decFactor)\n",
    "        numSamplesDec = len(vADec)\n",
    "        mDDec[ii, :numSamplesDec] = vADec\n",
    "        vNDec[ii]    = numSamplesDec\n",
    "\n",
    "    return mDDec, vNDec\n",
    "\n",
    "\n",
    "def DecimateDataThread( ii: int, mDDec: NDArray, vNDec: NDArray, mD: NDArray, vN: NDArray, decFactor: int, winSize: int ) -> None:\n",
    "    \n",
    "    vA    = mD[ii, :vN[ii]]\n",
    "    vADec = sp.signal.decimate(vA, decFactor)\n",
    "\n",
    "    numSamplesDec = len(vADec)\n",
    "\n",
    "    mDDec[ii, :numSamplesDec] = vADec\n",
    "    vNDec[ii]                 = numSamplesDec\n",
    "\n",
    "\n",
    "def DecimateDataParallel( mD: NDArray, vN: NDArray, decFactor: int, winSize: int ) -> Tuple[NDArray, NDArray]:\n",
    "    \"\"\"\n",
    "    Decimates the input data matrix by the specified factor.  \n",
    "    Works on each signal (Row) individually based on its valid length.\n",
    "    Input:\n",
    "        - mD (NDArray): Input data matrix.\n",
    "        - vN (NDArray): Vector of valid lengths for each signal.\n",
    "        - decFactor (int): Decimation factor.\n",
    "        - winSize (int): Window size for the decimated signals.\n",
    "    Output:\n",
    "        - mDDec (NDArray): Decimated data matrix.\n",
    "        - vNDec (NDArray): Vector of valid lengths for each decimated signal.\n",
    "    \"\"\"\n",
    "    \n",
    "    numSignals = mD.shape[0]\n",
    "    mDDec = np.zeros((numSignals, winSize))\n",
    "    vNDec = np.zeros(numSignals, dtype = np.int32)\n",
    "\n",
    "    hDecimateDataThread = lambda ii: DecimateDataThread(ii, mDDec, vNDec, mD, vN, decFactor, winSize)\n",
    "\n",
    "    with ThreadPoolExecutor() as hExe: #<! Optimized for IO bound tasks\n",
    "        hExe.map(hDecimateDataThread, range(numSignals))\n",
    "\n",
    "    return mDDec, vNDec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio (Speech) Classification with 1D Convolution Model in PyTorch\n",
    "\n",
    "This notebook **recognize the spoken digit** of a given set of audio samples from [`AudioMNIST`](https://github.com/soerenab/AudioMNIST).\n",
    "\n",
    "The notebook presents:\n",
    "\n",
    " * Use of convolution layers in PyTorch.\n",
    " * Use of pool layers in PyTorch.\n",
    " * Use of adaptive pool layer in PyTorch.  \n",
    "   The motivation is to set a constant output size regardless of input dimensions.\n",
    " * Use the model for inference on the test data.\n",
    "\n",
    "</br>\n",
    "\n",
    " * <font color='brown'>(**#**)</font> While the layers are called _Convolution Layer_ they actually implement correlation.  \n",
    "   Since the weights are learned, in practice it makes no difference as _Correlation_ is convolution with the a flipped kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> What kind of a problem is the _recognition_ of a single word audio?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# Data\n",
    "dataSetName     = 'AudioMNIST'\n",
    "datasetFileUrl  = r'https://huggingface.co/datasets/Royi/AudioMNIST/resolve/main/AudioMNIST.parquet'\n",
    "datasetFileName = 'AudioMNIST.parquet'\n",
    "targetCol       = 'Digit' #<! Target Column for Classification\n",
    "numCls          = 10\n",
    "\n",
    "decFactor = 8 #<! Decimation Factor from 48 [kHz] to 6 [kHz]\n",
    "winSize   = 6_000 #<! Window Size after Decimation\n",
    "\n",
    "# Dataset\n",
    "padSize     = 400 #<! Padding Size to embed the signals into (In addition to `winSize`)\n",
    "valSetRatio = 0.2 #<! Tune model's parameters\n",
    "\n",
    "# Model\n",
    "dropP = 0.1 #<! Dropout Layer\n",
    "\n",
    "# Training\n",
    "batchSize     = 512\n",
    "numWork       = 2 #<! Number of workers\n",
    "nEpochs       = 20\n",
    "learningRate = 5e-4\n",
    "weightDecay  = 5e-5\n",
    "tuβ          = (0.9, 0.99)\n",
    "\n",
    "# Visualization\n",
    "numSigPlot = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate / Load Data\n",
    "\n",
    "This section generates the data from the following model:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Data\n",
    "\n",
    "datasetFolderPath = os.path.join(BASE_FOLDER_PATH, DATA_FOLDER_NAME, dataSetName)\n",
    "datasetFilePath = DownloadUrlFile(datasetFileUrl, datasetFolderPath, datasetFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate / Load Data\n",
    "\n",
    "dfData = pd.read_parquet(datasetFilePath)\n",
    "dfData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decimation of Data\n",
    "# Data is sampled at 48 [kHz] which is overkill for digit classification.\n",
    "# The data is decimated by `decFactor`. This improves performance and reduces memory consumption.\n",
    "\n",
    "mData = dfData.iloc[:, dfData.columns.tolist().index('0'):].to_numpy()\n",
    "vN = dfData['NumSamples'].to_numpy()\n",
    "mDataDec, vNDec = DecimateData(mData, vN, decFactor = decFactor, winSize = winSize)\n",
    "# mDataDec, vNDec = DecimateDataParallel(mData, vN, decFactor = decFactor, winSize = winSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Data DataFrame\n",
    "\n",
    "dfAudioDec = pd.DataFrame(mDataDec, columns = [str(ii) for ii in range(mDataDec.shape[1])])\n",
    "dfData = pd.concat((dfData.iloc[:, :dfData.columns.tolist().index('0')].copy(), dfAudioDec), axis = 1)\n",
    "dfData['NumSamples'] = vNDec\n",
    "dfData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping invalid values\n",
    "\n",
    "dfData['Accent'] = dfData['Accent'].replace({'german': 'German', 'Egyptian_American?': 'Egyptian/American'})\n",
    "dfData['Age']    = dfData['Age'].replace({1234: 34})\n",
    "dfData['Continent'] = dfData['Continent'].replace({'South-America': 'America'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate / Load Data\n",
    "\n",
    "print(f'The number of samples : {dfData.shape[0]}')\n",
    "print(f'The number of speakers: {dfData['Speaker'].nunique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> What is the content of `vY` above? Explain its shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Data\n",
    "\n",
    "# Distribution of Digits\n",
    "hF, hA = plt.subplots(figsize = (6, 4))\n",
    "sns.histplot(data = dfData, x = 'Digit', bins = 10, discrete = True, kde = False, ax = hA)\n",
    "hA.set_title('Distribution of Digits in AudioMNIST Dataset')\n",
    "hA.set_xlabel('Digit')\n",
    "hA.set_ylabel('Count');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Samples per Speaker\n",
    "hF, hA = plt.subplots(figsize = (6, 4))\n",
    "sns.histplot(data = dfData, x = 'Speaker', bins = 10, discrete = True, kde = False, ax = hA)\n",
    "hA.set_title('Distribution of Speakers in AudioMNIST Dataset')\n",
    "hA.set_xlabel('Speaker')\n",
    "hA.set_ylabel('Count');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gender Distribution\n",
    "hF, hA = plt.subplots(figsize = (6, 4))\n",
    "sns.histplot(data = dfData, x = 'Gender', bins = 10, discrete = True, kde = False, ax = hA)\n",
    "hA.set_title('Distribution of Genders in AudioMNIST Dataset')\n",
    "hA.set_xlabel('Gender')\n",
    "hA.set_ylabel('Count');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age Distribution\n",
    "# See https://github.com/soerenab/AudioMNIST/issues/10\n",
    "hF, hA = plt.subplots(figsize = (6, 4))\n",
    "sns.histplot(data = dfData, x = 'Age', kde = False, ax = hA)\n",
    "hA.set_title('Distribution of Ages in AudioMNIST Dataset')\n",
    "hA.set_xlabel('Age')\n",
    "hA.set_ylabel('Count');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Samples Distribution\n",
    "hF, hA = plt.subplots(figsize = (6, 4))\n",
    "sns.histplot(data = dfData, x = 'NumSamples', kde = False, ax = hA)\n",
    "hA.set_title('Distribution of Number of Sample in AudioMNIST Dataset')\n",
    "hA.set_xlabel('Number of Samples')\n",
    "hA.set_ylabel('Count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Input Data\n",
    "\n",
    "Each signal has a different length.  \n",
    "In order to wrap them into a single batch one must apply some kind of transformation.  \n",
    "Common transformations are:\n",
    "\n",
    "1. Padding with zeros.\n",
    "2. Feature extraction.\n",
    "\n",
    "In this case padding with zeros is used.  \n",
    "In order to avoid the model to count the padding zeros as a feature, the signal is shifted within the window. \n",
    "\n",
    "![](https://i.imgur.com/6iPXza5.png)\n",
    "<!-- ![](https://i.postimg.cc/nL5XpXNz/Diagrams-Pad-Signal-Zeros-Rnd-Shift.png) -->\n",
    "\n",
    "The length of the signal in the batch is constant.  \n",
    "Yet the location of the start of the signal within the batch signal is random.  \n",
    "Hence the model must become insensitive to the padding of the signal and its length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> One may see the random shift as an augmentation. What other augmentation can fit this task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Dataset\n",
    "\n",
    "# The signals are embedded into windows of size `winSize` + `padSize`\n",
    "dsData = AudioMNISTDataset(dfData, targetCol = 'Digit', hMap = lambda x: int(x), hTransform = None, padSize = padSize, seedNum = seedNum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / Validation Split Methodology\n",
    "\n",
    "The dataset consists 60 speakers each with 50 samples per digit.  \n",
    "One may want to ensure each speaker is either on the _Train Set_ or the _Validation Set_ in order to prevent overfitting to a speaker.  \n",
    "\n",
    " * <font color='brown'>(**#**)</font> A deeper discussion should take into account what the real world scenario the model should solve.\n",
    " * <font color='brown'>(**#**)</font> One may look at [Data Leakage](https://en.wikipedia.org/wiki/Leakage_(machine_learning)).\n",
    "  * <font color='brown'>(**#**)</font> One may use a more delicate granularity by only ensuring each digit of a specified speaker is either on the _Train Set_ or the _Validation Set_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Validation Split\n",
    "\n",
    "lTrainIdx, lValIdx = dsData.GenTrainValSplit(valSetRatio)\n",
    "dsTrain = torch.utils.data.Subset(dsData, lTrainIdx)\n",
    "dsVal   = torch.utils.data.Subset(dsData, lValIdx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * <font color='brown'>(**#**)</font> There are tricks to wrap a `Subset` in order to allow different transforms for train and test sets.  \n",
    "   See [StackOverflow - Use a Different Data Augmentation for Train and Validation `Subset`](https://stackoverflow.com/questions/51782021), [PyTorch Forum - Use a Different Data Augmentation for Train and Validation `Subset`](https://discuss.pytorch.org/t/32209).  \n",
    "   An alternative is to use the course's `SubSet` class in `DeepLearningPyTorch.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Signals\n",
    "\n",
    "hF, hA = plt.subplots(figsize = FIG_SIZE_DEF)\n",
    "for ii in range(numSigPlot):\n",
    "    signalIdx = random.randint(0, len(dsData) - 1)\n",
    "    vX, valY = dsData[signalIdx]\n",
    "    hA.plot(vX.squeeze(), lw = 2, alpha = 0.75, label = f'Signal {ii:05d} - Digit {valY}')\n",
    "\n",
    "hA.set_title('Audio Signals from AudioMNIST Dataset')\n",
    "hA.set_xlabel('Sample Index')\n",
    "hA.set_ylabel('Amplitude')\n",
    "hA.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Data Loaders\n",
    "\n",
    "# dlTrain, dlVal = GenDataLoaders(dsTrain, dsVal, batchSize, numWorkers = numWork, persWork = True)\n",
    "dlTrain, dlVal = GenDataLoaders(dsTrain, dsVal, batchSize, numWorkers = 0) #<! Prevents issues with multiple workers in some environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate on the Loader\n",
    "# The first batch.\n",
    "tX, vY = next(iter(dlTrain)) #<! PyTorch Tensors\n",
    "\n",
    "print(f'The batch features dimensions: {tX.shape}')\n",
    "print(f'The batch features data type : {tX.dtype}')\n",
    "print(f'The batch labels dimensions  : {vY.shape}')\n",
    "print(f'The batch labels data type   : {vY.dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model\n",
    "\n",
    "The model is defined as a sequential model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "# Defining a sequential model.\n",
    "\n",
    "oModel = nn.Sequential(\n",
    "    nn.Identity(),\n",
    "        \n",
    "    nn.Conv1d(in_channels = 1,   out_channels = 16,  kernel_size = 21), nn.MaxPool1d(kernel_size = 2), nn.ReLU(),\n",
    "    nn.Conv1d(in_channels = 16,  out_channels = 32,  kernel_size = 19), nn.MaxPool1d(kernel_size = 2), nn.ReLU(),\n",
    "    nn.Conv1d(in_channels = 32,  out_channels = 48,  kernel_size = 17), nn.MaxPool1d(kernel_size = 2), nn.ReLU(),\n",
    "    nn.Conv1d(in_channels = 48,  out_channels = 64,  kernel_size = 15), nn.MaxPool1d(kernel_size = 2), nn.ReLU(),\n",
    "    nn.Conv1d(in_channels = 64,  out_channels = 96,  kernel_size = 13), nn.MaxPool1d(kernel_size = 2), nn.ReLU(),\n",
    "    nn.Conv1d(in_channels = 96,  out_channels = 128, kernel_size = 11), nn.MaxPool1d(kernel_size = 2), nn.ReLU(),\n",
    "    nn.Conv1d(in_channels = 128, out_channels = 160, kernel_size = 9) , nn.MaxPool1d(kernel_size = 2), nn.ReLU(),\n",
    "    nn.Conv1d(in_channels = 160, out_channels = 192, kernel_size = 7) , nn.MaxPool1d(kernel_size = 2), nn.ReLU(),\n",
    "                \n",
    "    nn.AdaptiveAvgPool1d(output_size = 1), nn.ReLU(),\n",
    "    nn.Flatten          (),\n",
    "    nn.Linear           (in_features = 192, out_features = 128), nn.ReLU(),\n",
    "    nn.Dropout          (p = dropP),\n",
    "    nn.Linear           (in_features = 128, out_features = 64),  nn.ReLU(),\n",
    "    nn.Dropout          (p = dropP),\n",
    "    nn.Linear           (in_features = 64, out_features = numCls),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> The [`torch.nn.AdaptiveAvgPool1d`](https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool1d.html) allows the same output shape regard less of the  input.\n",
    "* <font color='red'>(**?**)</font> What is the role of the [`torch.nn.Flatten`](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html) layers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Summary\n",
    "\n",
    "torchinfo.summary(oModel, tX.shape, col_names = ['kernel_size', 'input_size', 'output_size', 'num_params'], device = 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> Pay attention the dropout parameter of PyTorch is about the probability to zero out the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Model Graph  \n",
    "\n",
    "torchvista.trace_model(oModel, tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Model\n",
    "# Apply a test run.\n",
    "\n",
    "tXX = torch.randn(batchSize, 1, tX.shape[2])\n",
    "with torch.inference_mode():\n",
    "    vYHat = oModel(tXX)\n",
    "\n",
    "print(f'The input dimensions: {tXX.shape}')\n",
    "print(f'The output dimensions: {vYHat.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Use the training and validation samples.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU Availability\n",
    "\n",
    "runDevice = torch.device('cuda:0' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu')) #<! The 1st CUDA device\n",
    "oModel    = oModel.to(runDevice) #<! Transfer model to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Loss & Score\n",
    "\n",
    "hL = nn.CrossEntropyLoss()\n",
    "hS = MulticlassAccuracy(num_classes = numCls, average = 'micro') #<! See documentation for `macro` vs. `micro`\n",
    "hS = hS.to(runDevice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Optimizer\n",
    "\n",
    "oOpt = torch.optim.AdamW(oModel.parameters(), lr = learningRate, betas = tuβ, weight_decay = weightDecay) #<! Define optimizer\n",
    "oSch = torch.optim.lr_scheduler.OneCycleLR(oOpt, max_lr = learningRate, total_steps = nEpochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "\n",
    "_, lTrainLoss, lTrainScore, lValLoss, lValScore, lLearnRate = TrainModel(oModel, dlTrain, dlVal, oOpt, nEpochs, hL, hS, oSch = oSch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Results\n",
    "hF, vHa = plt.subplots(nrows = 1, ncols = 3, figsize = (12, 5))\n",
    "vHa = vHa.flat\n",
    "\n",
    "hA = vHa[0]\n",
    "hA.plot(lTrainLoss, lw = 2, label = 'Train')\n",
    "hA.plot(lValLoss, lw = 2, label = 'Validation')\n",
    "hA.grid()\n",
    "hA.set_title('Cross Entropy Loss')\n",
    "hA.set_xlabel('Epoch Index')\n",
    "hA.set_ylabel('Loss')\n",
    "hA.legend();\n",
    "\n",
    "\n",
    "hA = vHa[1]\n",
    "hA.plot(lTrainScore, lw = 2, label = 'Train')\n",
    "hA.plot(lValScore, lw = 2, label = 'Validation')\n",
    "hA.grid()\n",
    "hA.set_title('Accuracy Score')\n",
    "hA.set_xlabel('Epoch Index')\n",
    "hA.set_ylabel('Score')\n",
    "hA.legend();\n",
    "\n",
    "hA = vHa[2]\n",
    "hA.plot(lLearnRate, lw = 2)\n",
    "hA.grid()\n",
    "hA.set_title('Learn Rate Scheduler')\n",
    "hA.set_xlabel('Epoch')\n",
    "hA.set_ylabel('Learn Rate');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> Results are of the last iteration of the model as the best weights ar not reloaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis \n",
    "\n",
    "This section runs the model on the data and analyze results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Data Loader \n",
    "dlTrain = torch.utils.data.DataLoader(dsTrain, batch_size = 4 * batchSize, shuffle = False, num_workers = 0, drop_last = False, persistent_workers = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on Train Data\n",
    "\n",
    "lYY     = []\n",
    "lYYHat  = []\n",
    "with torch.inference_mode():\n",
    "    for tXX, vYY in dlTrain:\n",
    "        tXX = tXX.to(runDevice)\n",
    "        lYY.append(vYY)\n",
    "        lYYHat.append(oModel(tXX))\n",
    "\n",
    "vYY    = torch.cat(lYY, dim = 0).cpu().numpy()\n",
    "tYYHat = torch.cat(lYYHat, dim = 0).cpu().numpy()\n",
    "vYYHat = np.argmax(tYYHat, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRes = dfData.iloc[lTrainIdx].copy()\n",
    "dfRes = dfRes[['Digit', 'Speaker', 'NumSamples', 'Accent', 'Age', 'Gender', 'NativeSpeaker']]\n",
    "dfRes['Prediction'] = vYYHat\n",
    "dfRes['Correct'] = (dfRes['Digit'].to_numpy() == vYYHat)\n",
    "# np.all(dfRes['Digit'].to_numpy() == vYY) #<! Verify labels match\n",
    "dfRes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Accuracy per Speaker\n",
    "\n",
    "dfResGrpSpkr = dfRes.groupby('Speaker')\n",
    "dsSpkrAcc    = dfResGrpSpkr['Correct'].mean()\n",
    "\n",
    "hF, hA = plt.subplots(figsize = (14, 4))\n",
    "sns.barplot(x = dsSpkrAcc.index, y = dsSpkrAcc, ax = hA)\n",
    "hA.set_title('Accuracy per Speaker')\n",
    "hA.set_xlabel('Speaker')\n",
    "hA.set_ylabel('Accuracy');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix per Digit\n",
    "\n",
    "hF, hA = plt.subplots(figsize = (7, 6))\n",
    "\n",
    "hA, _ = PlotConfusionMatrix(vYY, vYYHat, hA = hA)\n",
    "hA.set_title(f'Train Data, Accuracy {np.mean(vYY == vYYHat): 0.2%}');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Data Analysis \n",
    "\n",
    "This section runs the model on the data and analyze results.  \n",
    "Results should be comparable to AudioNet in the original paper ([AudioMNIST: Exploring Explainable Artificial Intelligence for Audio Analysis on a Simple Benchmark](https://www.sciencedirect.com/science/article/pii/S0016003223007536)):\n",
    "\n",
    "![](https://i.imgur.com/qmvjLlZ.png)\n",
    "<!-- ![](https://i.postimg.cc/HLvbjxYQ/image.png) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on Validation Data\n",
    "lYY     = []\n",
    "lYYHat  = []\n",
    "with torch.inference_mode():\n",
    "    for tXX, vYY in dlVal:\n",
    "        tXX = tXX.to(runDevice)\n",
    "        lYY.append(vYY)\n",
    "        lYYHat.append(oModel(tXX))\n",
    "\n",
    "vYY    = torch.cat(lYY, dim = 0).cpu().numpy()\n",
    "tYYHat = torch.cat(lYYHat, dim = 0).cpu().numpy()\n",
    "vYYHat = np.argmax(tYYHat, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRes = dfData.iloc[lValIdx].copy()\n",
    "dfRes = dfRes[['Digit', 'Speaker', 'NumSamples', 'Accent', 'Age', 'Gender', 'NativeSpeaker']]\n",
    "dfRes['Prediction'] = vYYHat\n",
    "dfRes['Correct'] = (dfRes['Digit'].to_numpy() == vYYHat)\n",
    "# np.all(dfRes['Digit'].to_numpy() == vYY) #<! Verify labels match\n",
    "dfRes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Accuracy per Speaker\n",
    "\n",
    "dfResGrpSpkr = dfRes.groupby('Speaker')\n",
    "dsSpkrAcc    = dfResGrpSpkr['Correct'].mean()\n",
    "\n",
    "hF, hA = plt.subplots(figsize = (6, 4))\n",
    "sns.barplot(x = dsSpkrAcc.index, y = dsSpkrAcc, ax = hA)\n",
    "hA.set_title('Accuracy per Speaker')\n",
    "hA.set_xlabel('Speaker')\n",
    "hA.set_ylabel('Accuracy');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix per Digit\n",
    "\n",
    "hF, hA = plt.subplots(figsize = (7, 6))\n",
    "\n",
    "hA, _ = PlotConfusionMatrix(vYY, vYYHat, hA = hA)\n",
    "hA.set_title(f'Validation Data, Accuracy {np.mean(vYY == vYYHat): 0.2%}');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Can you find where the model struggles?\n",
    "* <font color='green'>(**@**)</font> Optimize the _Hyper Parameters_ (`learningRate`, `weightDecay`) to improve the model performance.\n",
    "* <font color='red'>(**?**)</font> Augmentation is changing the features during training in order to improve the model generalization.  \n",
    "  Think of possible augmentations that can be applied to this problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
