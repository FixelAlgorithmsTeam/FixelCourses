{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Fixel Algorithms](https://fixelalgorithms.co/images/CCExt.png)](https://fixelalgorithms.gitlab.io)\n",
    "\n",
    "# Deep Learning Methods\n",
    "\n",
    "## Deep Learning for Computer Vision - Image Classification with 2D Convolution (CIFAR 10)\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 1.0.002 | 15/01/2026 | Royi Avital | Using Fixel Courses' dataset folder                                |\n",
    "| 1.0.001 | 18/07/2025 | Royi Avital | Added notes on Normalization                                       |\n",
    "| 1.0.000 | 27/04/2024 | Royi Avital | First version                                                      |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/AIProgram/2024_02/0086DeepLearningConv2DCifar10.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T09:30:06.492269Z",
     "start_time": "2022-02-02T09:30:06.220934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchinfo\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "import torchvision\n",
    "\n",
    "# Miscellaneous\n",
    "import os\n",
    "from platform import python_version\n",
    "import random\n",
    "\n",
    "# Typing\n",
    "from typing import Callable, Dict, Generator, List, Optional, Self, Set, Tuple, Union\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought.\n",
    "\n",
    "Code Notations:\n",
    "\n",
    "```python\n",
    "someVar    = 2; #<! Notation for a variable\n",
    "vVector    = np.random.rand(4) #<! Notation for 1D array\n",
    "mMatrix    = np.random.rand(4, 3) #<! Notation for 2D array\n",
    "tTensor    = np.random.rand(4, 3, 2, 3) #<! Notation for nD array (Tensor)\n",
    "tuTuple    = (1, 2, 3) #<! Notation for a tuple\n",
    "lList      = [1, 2, 3] #<! Notation for a list\n",
    "dDict      = {1: 3, 2: 2, 3: 1} #<! Notation for a dictionary\n",
    "oObj       = MyClass() #<! Notation for an object\n",
    "dfData     = pd.DataFrame() #<! Notation for a data frame\n",
    "dsData     = pd.Series() #<! Notation for a series\n",
    "hObj       = plt.Axes() #<! Notation for an object / handler / function handler\n",
    "```\n",
    "\n",
    "### Code Exercise\n",
    "\n",
    " - Single line fill\n",
    "\n",
    "```python\n",
    "valToFill = ???\n",
    "```\n",
    "\n",
    " - Multi Line to Fill (At least one)\n",
    "\n",
    "```python\n",
    "# You need to start writing\n",
    "?????\n",
    "```\n",
    "\n",
    " - Section to Fill\n",
    "\n",
    "```python\n",
    "#===========================Fill This===========================#\n",
    "# 1. Explanation about what to do.\n",
    "# !! Remarks to follow / take under consideration.\n",
    "mX = ???\n",
    "\n",
    "?????\n",
    "#===============================================================#\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# %matplotlib inline\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "# Matplotlib default color palette\n",
    "lMatPltLibclr = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "# sns.set_theme() #>! Apply SeaBorn theme\n",
    "\n",
    "runInGoogleColab = 'google.colab' in str(get_ipython())\n",
    "\n",
    "# Improve performance by benchmarking\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(seedNum)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark     = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "FIG_SIZE_DEF    = (8, 8)\n",
    "ELM_SIZE_DEF    = 50\n",
    "CLASS_COLOR     = ('b', 'r')\n",
    "EDGE_COLOR      = 'k'\n",
    "MARKER_SIZE_DEF = 10\n",
    "LINE_WIDTH_DEF  = 2\n",
    "\n",
    "DATA_FOLDER_NAME           = 'DataSets'\n",
    "TENSOR_BOARD_FOLDER_NAME   = 'TB'\n",
    "\n",
    "BASE_FOLDER_NAME = 'FixelCourses'\n",
    "BASE_FOLDER_PATH = os.getcwd()[:(len(os.getcwd()) - (os.getcwd()[::-1].lower().find(BASE_FOLDER_NAME.lower()[::-1])))]\n",
    "\n",
    "D_CLASSES_CIFAR_10  = {0: 'Airplane', 1: 'Automobile', 2: 'Bird', 3: 'Cat', 4: 'Deer', 5: 'Dog', 6: 'Frog', 7: 'Horse', 8: 'Ship', 9: 'Truck'}\n",
    "L_CLASSES_CIFAR_10  = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
    "T_IMG_SIZE_CIFAR_10 = (32, 32, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Auxiliary Modules for Google Colab\n",
    "if runInGoogleColab:\n",
    "    !wget https://raw.githubusercontent.com/FixelAlgorithmsTeam/FixelCourses/master/AIProgram/2024_02/DataManipulation.py\n",
    "    !wget https://raw.githubusercontent.com/FixelAlgorithmsTeam/FixelCourses/master/AIProgram/2024_02/DataVisualization.py\n",
    "    !wget https://raw.githubusercontent.com/FixelAlgorithmsTeam/FixelCourses/master/AIProgram/2024_02/DeepLearningPyTorch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courses Packages\n",
    "\n",
    "from DataVisualization import PlotLabelsHistogram, PlotMnistImages\n",
    "from DeepLearningPyTorch import NNMode, TrainModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Auxiliary Functions\n",
    "\n",
    "def AccuracyLogits( mScore: torch.Tensor, vY: torch.Tensor) -> float:\n",
    "    \n",
    "    vHatY = torch.argmax(mScore.detach(), dim = 1) #<! Logits -> Index (As SoftMax is monotonic)\n",
    "    valAcc = torch.mean((vHatY == vY).float()).item()\n",
    "    \n",
    "    return valAcc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Convolution Layer\n",
    "\n",
    "![](https://i.imgur.com/v10mfxG.png) Illustration inspired by CS231n\n",
    "<!-- ![](https://i.postimg.cc/FHhS77T2/Diagrams-Convolution.png) -->\n",
    "\n",
    "The Convolution Layer materializes the following _inductive bias_:\n",
    " * The features to extract are local.\n",
    " * The features to extract are shift invariant.\n",
    " * The features should be processed in hierarchical manner.\n",
    "\n",
    "</br>\n",
    "\n",
    "* <font color='brown'>(**#**)</font> Hierarchical structure means features are aggregated from low level features from the shallow layers (Textures) to high level features (Shapes) at deeper layers. \n",
    "* <font color='brown'>(**#**)</font> Having _inductive bias_ / _prior assumptions_ means being optimized for the task, yet less generality.  \n",
    "  If the inductive bias matches the actual task, it works well. Yet if it is not the whole concept, it might be limiting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR 10 Image Classification with 2D Convolution Net\n",
    "\n",
    "This notebook shows the use of [`Conv2d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) layer.  \n",
    "The 2D Convolution layer means there are 2 _degrees of freedom_ for the kernel movement.\n",
    "\n",
    "This notebook applies image classification (Single label per image) on the [CIFAR 10](https://en.wikipedia.org/wiki/CIFAR-10) dataset.  \n",
    "\n",
    "The notebook presents:\n",
    "\n",
    " * Building a 2D convolution based model which fits _Computer Vision_ tasks.\n",
    " * Use of [`torch.nn.Conv2d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html).\n",
    " * Use of [`torch.nn.BatchNorm2d`](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d).\n",
    "\n",
    "</br>\n",
    "\n",
    "* <font color='brown'>(**#**)</font> Convolution is a _Linear Shift Invariant_ (LSI) operator.  \n",
    "  Hence it fits the task of processing and extracting features from images.\n",
    "* <font color='brown'>(**#**)</font> While the convolution layer is LSI the whole net is not due to Pool Layers, \n",
    "* <font color='brown'>(**#**)</font> One technique to make CNN not sensitive to shifts is by training it on a shifted data set.\n",
    "* <font color='brown'>(**#**)</font> Modern CNN's are commonly attributed to [Yan LeCun](https://en.wikipedia.org/wiki/Yann_LeCun).  \n",
    "  Yet, The first documented CNN is called _Neocognitron_ and attributed to [Kunihiko Fukushima](https://en.wikipedia.org/wiki/Kunihiko_Fukushima).\n",
    "* <font color='brown'>(**#**)</font> For history see [Annotated History of Modern AI and Deep Learning](https://people.idsia.ch/~juergen/deep-learning-history.html) ([ArXiV Paper](https://arxiv.org/abs/2212.11279))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# Data\n",
    "dataFolderPath = os.path.join(BASE_FOLDER_PATH, DATA_FOLDER_NAME)\n",
    "\n",
    "# Model\n",
    "dropP = 0.2 #<! Dropout Layer\n",
    "\n",
    "# Training\n",
    "batchSize   = 256\n",
    "numWork     = 2 #<! Number of workers\n",
    "nEpochs     = 30\n",
    "\n",
    "# Visualization\n",
    "numImg = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate / Load Data\n",
    "\n",
    "Load the [CIFAR 10 Data Set](https://en.wikipedia.org/wiki/CIFAR-10).  \n",
    "It is composed of 60,000 RGB images of size `32x32` with 10 classes uniformly spread.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The dataset is retrieved using [Torch Vision](https://pytorch.org/vision/stable/index.html)'s built in datasets.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "\n",
    "# PyTorch \n",
    "dsTrain = torchvision.datasets.CIFAR10(root = dataFolderPath, train = True,  download = True, transform = torchvision.transforms.ToTensor())\n",
    "dsVal   = torchvision.datasets.CIFAR10(root = dataFolderPath, train = False, download = True, transform = torchvision.transforms.ToTensor())\n",
    "lClasses  = dsTrain.classes\n",
    "\n",
    "\n",
    "print(f'The training data set data shape: {dsTrain.data.shape}')\n",
    "print(f'The validation data set data shape: {dsVal.data.shape}')\n",
    "print(f'The unique values of the labels: {np.unique(lClasses)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> The dataset is indexible (Subscriptable). It returns a tuple of the features and the label.\n",
    "* <font color='brown'>(**#**)</font> While data is arranged as `H x W x C` the transformer, when accessing the data, will convert it into `C x H x W`. \n",
    "* <font color='brown'>(**#**)</font> Data arrangement:\n",
    "    * [`NHWC` vs. `NCHW` : A Memory Access Perspective on GPUs](https://scribe.rip/4e79bd3b1b54).\n",
    "    * [Why Does PyTorch Prefer Using NCHW](https://discuss.pytorch.org/t/83637).\n",
    "    * [Efficient PyTorch: Tensor Memory Format Matters](https://pytorch.org/blog/tensor-memory-format-matters).\n",
    "    * [Accelerating PyTorch Vision Models with Channels Last on CPU](https://pytorch.org/blog/accelerating-pytorch-vision-models-with-channels-last-on-cpu).\n",
    "* <font color='brown'>(**#**)</font> PyTorch worked on supporting `NxHxWxC`: [(Beta) Channels Last Memory Format in PyTorch](https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Element of the Data Set\n",
    "\n",
    "mX, valY = dsTrain[0]\n",
    "\n",
    "print(f'The features shape: {mX.shape}')\n",
    "print(f'The label value: {valY}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Data\n",
    "\n",
    "tX = dsTrain.data #<! NumPy Tensor (NDarray)\n",
    "mX = np.reshape(tX, (tX.shape[0], -1))\n",
    "vY = dsTrain.targets #<! NumPy Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Data\n",
    "\n",
    "hF = PlotMnistImages(mX, vY, numImg, tuImgSize = T_IMG_SIZE_CIFAR_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of Labels\n",
    "\n",
    "hA = PlotLabelsHistogram(vY, lClass = L_CLASSES_CIFAR_10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Process Data\n",
    "\n",
    "This section normalizes the data to have zero mean and unit variance per **channel**.  \n",
    "It is required to calculate:\n",
    "\n",
    " * The average pixel value per channel.\n",
    " * The standard deviation per channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Standardization Parameters\n",
    "vMean = np.mean(dsTrain.data / 255.0, axis = (0, 1, 2))\n",
    "vStd  = np.std(dsVal.data / 255.0, axis = (0, 1, 2))\n",
    "\n",
    "print('µ =', vMean)\n",
    "print('σ =', vStd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Transformer\n",
    "\n",
    "oDataTrns = torchvision.transforms.Compose([       #<! Chaining transformations\n",
    "    torchvision.transforms.ToTensor(),             #<! Convert to Tensor (C x H x W), Normalizes into [0, 1] (https://pytorch.org/vision/main/generated/torchvision.transforms.ToTensor.html)\n",
    "    torchvision.transforms.Normalize(vMean, vStd), #<! Normalizes the Data (https://pytorch.org/vision/main/generated/torchvision.transforms.Normalize.html)\n",
    "    ])\n",
    "\n",
    "# Update the DS transformer\n",
    "dsTrain.transform = oDataTrns\n",
    "dsVal.transform   = oDataTrns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Normalized\" Image\n",
    "\n",
    "mX, valY = dsTrain[5]\n",
    "\n",
    "hF, hA = plt.subplots()\n",
    "hA.imshow(np.transpose(mX, (1, 2, 0)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loaders\n",
    "\n",
    "The dataloader is the functionality which loads the data into memory in batches.  \n",
    "Its challenge is to bring data fast enough so the Hard Disk is not the training bottleneck.  \n",
    "In order to achieve that, Multi Threading / Multi Process is used.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The multi process, by the `num_workers` parameter is not working well _out of the box_ on Windows.  \n",
    "  See [Errors When Using `num_workers > 0` in `DataLoader`](https://discuss.pytorch.org/t/97564), [On Windows `DataLoader` with `num_workers > 0` Is Slow](https://github.com/pytorch/pytorch/issues/12831).  \n",
    "  A way to overcome it is to define the training loop as a function in a different module (File) and import it (https://discuss.pytorch.org/t/97564/4, https://discuss.pytorch.org/t/121588/21). \n",
    "* <font color='brown'>(**#**)</font> The `num_workers` should be set to the lowest number which feeds the GPU fast enough.  \n",
    "  The idea is preserve as much as CPU resources to other tasks.\n",
    "* <font color='brown'>(**#**)</font> On Windows keep the `persistent_workers` parameter to `True` (_Windows_ is slower on forking processes / threads).\n",
    "* <font color='brown'>(**#**)</font> The Dataloader is a generator which can be looped on.\n",
    "* <font color='brown'>(**#**)</font> In order to make it iterable it has to be wrapped with `iter()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader\n",
    "\n",
    "dlTrain = torch.utils.data.DataLoader(dsTrain, shuffle = True, batch_size = 1 * batchSize, num_workers = numWork, drop_last = True, persistent_workers = True)\n",
    "dlVal   = torch.utils.data.DataLoader(dsVal, shuffle = False, batch_size = 2 * batchSize, num_workers = numWork, persistent_workers = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Why is the size of the batch twice as big for the test dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate on the Loader\n",
    "# The first batch.\n",
    "tX, vY = next(iter(dlTrain)) #<! PyTorch Tensors\n",
    "\n",
    "print(f'The batch features dimensions: {tX.shape}')\n",
    "print(f'The batch labels dimensions: {vY.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping\n",
    "for ii, (tX, vY) in zip(range(1), dlVal): #<! https://stackoverflow.com/questions/36106712\n",
    "    print(f'The batch features dimensions: {tX.shape}')\n",
    "    print(f'The batch labels dimensions: {vY.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model\n",
    "\n",
    "The model is defined as a sequential model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "# Defining a sequential model.\n",
    "\n",
    "numFeatures = np.prod(tX.shape[1:])\n",
    "\n",
    "oModel = nn.Sequential(\n",
    "        nn.Identity(),\n",
    "        \n",
    "        nn.Conv2d(in_channels = 3, out_channels = 30, kernel_size = 3, bias = False), #<! Input is RGB Image\n",
    "        nn.BatchNorm2d(num_features = 30),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout2d(p = dropP),\n",
    "        \n",
    "        nn.Conv2d(in_channels = 30, out_channels = 60, kernel_size = 3, bias = False),\n",
    "        nn.MaxPool2d(kernel_size = 2),\n",
    "        nn.BatchNorm2d(num_features = 60),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout2d(p = dropP),\n",
    "                \n",
    "        nn.Conv2d(in_channels = 60,  out_channels = 120, kernel_size = 3, bias = False),\n",
    "        nn.BatchNorm2d(num_features = 120),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout2d(p = dropP),\n",
    "        \n",
    "        nn.Conv2d(in_channels = 120, out_channels = 240, kernel_size = 3, bias = False),\n",
    "        nn.BatchNorm2d(num_features = 240),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout2d(p = dropP),\n",
    "        \n",
    "        nn.Conv2d(in_channels = 240, out_channels = 500, kernel_size = 3, bias = False),\n",
    "        nn.MaxPool2d(kernel_size = 2),\n",
    "        nn.BatchNorm2d(num_features = 500),\n",
    "        nn.ReLU(),\n",
    "        \n",
    "        nn.AdaptiveAvgPool2d(1),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(500, len(L_CLASSES_CIFAR_10)),\n",
    "    )\n",
    "\n",
    "torchinfo.summary(oModel, tX.shape, col_names = ['kernel_size', 'input_size', 'output_size', 'num_params'], device = 'cpu') #<! Added `kernel_size`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Why `in_channels = 3`?\n",
    "* <font color='red'>(**?**)</font> Why `bias = False` in the convolution layers?\n",
    "* <font color='red'>(**?**)</font> Could the _Batch Normalization_ layer be at the model's beginning as an alternative to data normalization?\n",
    "* <font color='red'>(**?**)</font> What's the largest kernel size for `Conv2d` to be used after the last `MaxPool2d` layer?\n",
    "* <font color='brown'>(**#**)</font> BN's `num_features` is required to know the number of parameters of the layer.\n",
    "* <font color='brown'>(**#**)</font> For videos or other 4D dimensions data one could employ `Conv3d`.\n",
    "* <font color='brown'>(**#**)</font> Guideline: The smaller the image gets, the deeper it is (More channels).   \n",
    "  The intuition, the beginning of the model learns low level features (Small number), deeper learns combinations of features (Larger number).\n",
    "* <font color='green'>(**@**)</font> The model has a recurrent block. Implement it as a class or a function and rebuild the model.\n",
    "\n",
    "\n",
    "<!-- \n",
    "class CnnBlock(nn.Module):\n",
    "    def __init__( self, chnlIn: int, chnlOut: int, dropP: float, maxPoolLyr: bool = False ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        if maxPoolLyr:\n",
    "            oMaxPool = nn.MaxPool2d(2)\n",
    "        else:\n",
    "            oMaxPool = nn.Identity\n",
    "\n",
    "        self.chnlIn     = chnlIn\n",
    "        self.chnlOut    = chnlOut\n",
    "        self.Block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = chnlIn, out_channels = chnlOut, kernel_size = 3, bias = False), #<! Input is RGB Image\n",
    "            oMaxPool,\n",
    "            nn.BatchNorm2d(num_features = 30),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(p = dropP),\n",
    "        )\n",
    "    \n",
    "    def forward( self, mX: torch.Tensor ):\n",
    "\n",
    "        return self.Block(mX)\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization\n",
    "\n",
    "While [Batch Normalization](https://en.wikipedia.org/wiki/Batch_normalization) was a key development in the ability to make NN deeper it also made few issues:\n",
    "\n",
    " - In large models (Relative ot the resources) which require small batch size the operation becomes less stable.  \n",
    "   Such cases motivated the creation of [Layer Normalization](https://arxiv.org/abs/1607.06450) (See PyTorch [`LayerNorm`](https://docs.pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)), [Instance Normalization](https://arxiv.org/abs/1607.08022) (See PyTorch's [`InstanceNorm2d`](https://docs.pytorch.org/docs/stable/generated/torch.nn.InstanceNorm2d.html)), [Group Normalization](https://arxiv.org/abs/1803.08494) (See PyTorch's [`GroupNorm`](https://docs.pytorch.org/docs/stable/generated/torch.nn.GroupNorm.html)).  \n",
    "   See discussion at [StackOverflow Instance Normalization vs Batch Normalization](https://stackoverflow.com/questions/45463778).  \n",
    "   See post at [Batch Normalization Alternatives: `LayerNorm` and `InstanceNorm`](https://scribe.rip/52bdf43624b9).  \n",
    "   See context for Transformers at [StackExchange Cross Validated - Why Do Transformers Use Layer Norm Instead of Batch Norm](https://stats.stackexchange.com/questions/474440).\n",
    " - Some argue that the [SELU Activation Layer](https://arxiv.org/abs/1706.02515) (See PyTorch's [`SELU`](https://docs.pytorch.org/docs/stable/generated/torch.nn.SELU.html)) layer can substitute the Batch Normalization.  \n",
    " See [Data Science StackExchange - RELU vs. SELU](https://datascience.stackexchange.com/questions/102724), [Activation Layers Explained](https://web.archive.org/web/20220817071122/https://mlfromscratch.com/activation-functions-explained).\n",
    " - [Batch Normalization is a Cause of Adversarial Vulnerability](https://arxiv.org/abs/1905.02161).\n",
    "\n",
    "\n",
    "![](https://i.imgur.com/xacPJEu.png)\n",
    "<!-- ![](https://i.postimg.cc/kXKv3bwB/1JdN6.png) -->\n",
    "\n",
    "<!-- [Reddit - BatchNorm Alternatives 2019](https://www.reddit.com/r/MachineLearning/comments/cx2msl) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Model\n",
    "# Apply a test run.\n",
    "\n",
    "tX      = torch.randn(128, 3, 32, 32)\n",
    "mLogits = oModel(tX) #<! Logit -> Prior to Sigmoid\n",
    "\n",
    "print(f'The input dimensions: {tX.shape}')\n",
    "print(f'The output (Logits) dimensions: {mLogits.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='green'>(**@**)</font> Try different dimensions for the input. How will it affect the output? Why?\n",
    "* <font color='brown'>(**#**)</font> The [Logit Function](https://en.wikipedia.org/wiki/Logit) is the inverse of the [Sigmoid Function](https://en.wikipedia.org/wiki/Sigmoid_function).  \n",
    "  It is commonly used to describe values in the range $\\left( - \\infty, \\infty \\right)$ which can be transformed into probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU Availability\n",
    "\n",
    "runDevice   = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') #<! The 1st CUDA device\n",
    "oModel      = oModel.to(runDevice) #<! Transfer model to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Loss & Score\n",
    "\n",
    "hL = nn.CrossEntropyLoss()\n",
    "hS = MulticlassAccuracy(num_classes = len(L_CLASSES_CIFAR_10), average = 'micro') #<! See documentation for `macro` vs. `micro`\n",
    "hS = hS.to(runDevice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='blue'>(**!**)</font> Go through `AccuracyLogits()` as equivalent tp `micro` mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Optimizer\n",
    "\n",
    "oOpt = torch.optim.AdamW(oModel.parameters(), lr = 1e-3, betas = (0.9, 0.99), weight_decay = 1e-3) #<! Define optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "\n",
    "oRunModel, lTrainLoss, lTrainScore, lValLoss, lValScore, _ = TrainModel(oModel, dlTrain, dlVal, oOpt, nEpochs, hL, hS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Why train results are worse than validation for 5-6 epochs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Results\n",
    "hF, vHa = plt.subplots(nrows = 1, ncols = 2, figsize = (12, 6))\n",
    "vHa = vHa.flat\n",
    "\n",
    "hA = vHa[0]\n",
    "hA.plot(lTrainLoss, lw = 2, label = 'Train Loss')\n",
    "hA.plot(lValLoss, lw = 2, label = 'Validation Loss')\n",
    "hA.grid()\n",
    "hA.set_title('Cross Entropy Loss')\n",
    "hA.set_xlabel('Epoch Index')\n",
    "hA.set_ylabel('Loss')\n",
    "hA.legend();\n",
    "\n",
    "\n",
    "hA = vHa[1]\n",
    "hA.plot(lTrainScore, lw = 2, label = 'Train Score')\n",
    "hA.plot(lValScore, lw = 2, label = 'Validation Score')\n",
    "hA.grid()\n",
    "hA.set_title('Accuracy Score')\n",
    "hA.set_xlabel('Epoch Index')\n",
    "hA.set_ylabel('Score')\n",
    "hA.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
