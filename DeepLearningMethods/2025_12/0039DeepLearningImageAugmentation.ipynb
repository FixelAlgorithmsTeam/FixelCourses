{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Fixel Algorithms](https://fixelalgorithms.co/images/CCExt.png)](https://fixelalgorithms.gitlab.io)\n",
    "\n",
    "# Deep Learning Methods\n",
    "\n",
    "## Deep Learning - Image Augmentation\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 1.0.001 | 17/01/2026 | Royi Avital | Updated image URL                                                  |\n",
    "| 1.0.000 | 01/06/2024 | Royi Avital | First version                                                      |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/AIProgram/2024_02/0094DeepLearningImageAugmentation.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T09:30:06.492269Z",
     "start_time": "2022-02-02T09:30:06.220934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn            as nn\n",
    "import torchvision\n",
    "from torchvision.transforms import v2 as TorchVisionTrns\n",
    "\n",
    "# Image Processing & Computer Vision\n",
    "import skimage as ski\n",
    "\n",
    "# Miscellaneous\n",
    "import os\n",
    "from platform import python_version\n",
    "import random\n",
    "\n",
    "# Typing\n",
    "from typing import Any, Callable, Dict, Generator, List, Optional, Self, Set, Tuple, Union\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought.\n",
    "\n",
    "Code Notations:\n",
    "\n",
    "```python\n",
    "someVar    = 2; #<! Notation for a variable\n",
    "vVector    = np.random.rand(4) #<! Notation for 1D array\n",
    "mMatrix    = np.random.rand(4, 3) #<! Notation for 2D array\n",
    "tTensor    = np.random.rand(4, 3, 2, 3) #<! Notation for nD array (Tensor)\n",
    "tuTuple    = (1, 2, 3) #<! Notation for a tuple\n",
    "lList      = [1, 2, 3] #<! Notation for a list\n",
    "dDict      = {1: 3, 2: 2, 3: 1} #<! Notation for a dictionary\n",
    "oObj       = MyClass() #<! Notation for an object\n",
    "dfData     = pd.DataFrame() #<! Notation for a data frame\n",
    "dsData     = pd.Series() #<! Notation for a series\n",
    "hObj       = plt.Axes() #<! Notation for an object / handler / function handler\n",
    "```\n",
    "\n",
    "### Code Exercise\n",
    "\n",
    " - Single line fill\n",
    "\n",
    "```python\n",
    "valToFill = ???\n",
    "```\n",
    "\n",
    " - Multi Line to Fill (At least one)\n",
    "\n",
    "```python\n",
    "# You need to start writing\n",
    "?????\n",
    "```\n",
    "\n",
    " - Section to Fill\n",
    "\n",
    "```python\n",
    "#===========================Fill This===========================#\n",
    "# 1. Explanation about what to do.\n",
    "# !! Remarks to follow / take under consideration.\n",
    "mX = ???\n",
    "\n",
    "?????\n",
    "#===============================================================#\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# %matplotlib inline\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "# Matplotlib default color palette\n",
    "lMatPltLibclr = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "# sns.set_theme() #>! Apply SeaBorn theme\n",
    "\n",
    "runInGoogleColab = 'google.colab' in str(get_ipython())\n",
    "\n",
    "# Improve performance by benchmarking\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Reproducibility (Per PyTorch Version on the same device)\n",
    "# torch.manual_seed(seedNum)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark     = False #<! Makes things slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "FIG_SIZE_DEF    = (8, 8)\n",
    "ELM_SIZE_DEF    = 50\n",
    "CLASS_COLOR     = ('b', 'r')\n",
    "EDGE_COLOR      = 'k'\n",
    "MARKER_SIZE_DEF = 10\n",
    "LINE_WIDTH_DEF  = 2\n",
    "\n",
    "DATA_FOLDER_NAME           = 'DataSets'\n",
    "TENSOR_BOARD_FOLDER_NAME   = 'TB'\n",
    "\n",
    "BASE_FOLDER_NAME = 'FixelCourses'\n",
    "BASE_FOLDER_PATH = os.getcwd()[:(len(os.getcwd()) - (os.getcwd()[::-1].lower().find(BASE_FOLDER_NAME.lower()[::-1])))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Auxiliary Modules for Google Colab\n",
    "if runInGoogleColab:\n",
    "    !wget https://raw.githubusercontent.com/FixelAlgorithmsTeam/FixelCourses/master/AIProgram/2024_02/DataManipulation.py\n",
    "    !wget https://raw.githubusercontent.com/FixelAlgorithmsTeam/FixelCourses/master/AIProgram/2024_02/DataVisualization.py\n",
    "    !wget https://raw.githubusercontent.com/FixelAlgorithmsTeam/FixelCourses/master/AIProgram/2024_02/DeepLearningPyTorch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courses Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Auxiliary Functions\n",
    "\n",
    "def PlotTransform( lImages: List[torchvision.tv_tensors._image.Image], titleStr: str, bAxis = False ) -> plt.Figure:\n",
    "    \n",
    "    numImg = len(lImages)\n",
    "    axWidh = 3\n",
    "    \n",
    "    lWidth  = [lImages[ii].shape[-1] for ii in range(numImg)]\n",
    "    hF, _ = plt.subplots(nrows = 1, ncols = numImg, figsize = (numImg * axWidh, 5), gridspec_kw = {'width_ratios': lWidth})\n",
    "    for ii, hA in enumerate(hF.axes):\n",
    "        mI = torch.permute(lImages[ii], (1, 2, 0))\n",
    "        hA.imshow(mI, cmap = 'gray')\n",
    "        hA.set_title(f'{ii}')\n",
    "        hA.axis('on') if bAxis else hA.axis('off')\n",
    "    \n",
    "    hF.suptitle(titleStr)\n",
    "    \n",
    "    return hF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Augmentation\n",
    "\n",
    "Applying _Image Augmentation_ expands the data available to the model to train on.  \n",
    "As more data, it also serves as a **regularization**.\n",
    "\n",
    "This notebooks presents:\n",
    "\n",
    " * The `torchvision.transforms` module.\n",
    " * Applying some of the available transforms on an image.\n",
    " * Chaining transforms.\n",
    " * Creating a custom transform.\n",
    "\n",
    "\n",
    "This notebook augments only the image data.\n",
    "\n",
    "</br>\n",
    "\n",
    "* <font color='brown'>(**#**)</font> Augmentation can be thought as the set of operation the model should be insensitive to.  \n",
    "  For instance, if it should be insensitive to shift, the same image should be trained on with different shifts.\n",
    "* <font color='brown'>(**#**)</font> PyTorch Vision is migrating its transforms module from `v1` to `v2`.  \n",
    "  This notebook will focus on `v2`.\n",
    "* <font color='brown'>(**#**)</font> While the notebook shows image augmentation in the context of Deep Learning for Computer Vision, the [_Data Augmentation_](https://en.wikipedia.org/wiki/Data_augmentation) concept can be utilized for other tasks as well.  \n",
    "  For instance, for _Audio Processing_ on could apply some noise addition, pitch change, filters, etc...\n",
    "* <font color='brown'>(**#**)</font> The are packages which specialize on image data augmentation: [Kornia](https://github.com/kornia/kornia), [Albumentations](https://github.com/albumentations-team/albumentations) (Considered to be the [fastest](https://github.com/albumentations-team/albumentations?tab=readme-ov-file#performance-comparison)), [ImgAug](https://github.com/aleju/imgaug) (Deprecated), [AugLy](https://github.com/facebookresearch/AugLy) (Audio, image, text and video)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# Data\n",
    "# imgFileUrl = r'https://raw.githubusercontent.com/FixelAlgorithmsTeam/FixelCourses/master/DeepLearningMethods/2023_02/09_TipsAndTricks/img1.jpg'\n",
    "# imgFileUrl = r'https://media.githubusercontent.com/media/FixelAlgorithmsTeam/FixelCourses/refs/heads/master/DeepLearningMethods/2023_02/09_TipsAndTricks/img1.jpg'\n",
    "\n",
    "# imgFileUrl = r'https://i.imgur.com/cTpJR8P.jpeg' #<! Img1\n",
    "# imgFileUrl = r'https://i.imgur.com/mFmqN76.jpeg' #<! Img2\n",
    "\n",
    "imgFileUrl = r'https://i.postimg.cc/Bv0knCkV/img1.jpg'\n",
    "# imgFileUrl = r'https://i.postimg.cc/26jt4Skk/img2.jpg'\n",
    "\n",
    "# Model\n",
    "\n",
    "# Training\n",
    "\n",
    "# Visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate / Load Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "\n",
    "mI = ski.io.imread(imgFileUrl)\n",
    "\n",
    "# Image Dimensions\n",
    "print(f'Image Shape             : {mI.shape}')\n",
    "print(f'Image Dimensions        : {mI.shape[:2]}')\n",
    "print(f'Image Number of Channels: {mI.shape[2]}')\n",
    "print(f'Image Element Type      : {mI.dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> The image is a _NumPy_ array. PyTorch default image loader is using [`PIL`](https://github.com/python-pillow/Pillow) (Pillow, as its optimized version) where the image is the PIL class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Data\n",
    "\n",
    "hF, hA = plt.subplots(figsize = (4, 6))\n",
    "\n",
    "hA.imshow(mI)\n",
    "hA.tick_params(axis = 'both', left = False, top = False, right = False, bottom = False, \n",
    "               labelleft = False, labeltop = False, labelright = False, labelbottom = False)\n",
    "hA.grid(False)\n",
    "hA.set_title('Input Image');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Transforms\n",
    "\n",
    "This section shows several transforms available in PyTorch Vision.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> See [TorchVision - Transforming and Augmenting Images](https://pytorch.org/vision/stable/transforms.html).\n",
    "* <font color='brown'>(**#**)</font> See TorchVision tutorials: [Getting Started with Transforms v2](https://pytorch.org/vision/stable/auto_examples/transforms/plot_transforms_getting_started.html), [Illustration of Transforms](https://pytorch.org/vision/stable/auto_examples/transforms/plot_transforms_illustrations.html).\n",
    "* <font color='brown'>(**#**)</font> There are 2 API's for the transforms. The class and the function (`Functional` API)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image to Tensor\n",
    "\n",
    "In `v2` the transform [`ToTensor`](https://pytorch.org/vision/main/generated/torchvision.transforms.ToTensor.html) is replaced by [`ToImage`](https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.ToImage.html) and / or [`ToDtype`](https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.ToDtype.html).\n",
    "\n",
    "* <font color='brown'>(**#**)</font> TorchVision has it own image container: [`torchvision.tv_tensors.Image`](https://pytorch.org/vision/stable/generated/torchvision.tv_tensors.Image.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using `ToImage`\n",
    "\n",
    "oToImg = TorchVisionTrns.ToImage() #<! Converts to TorchVision's Image (C, H, W)\n",
    "tI = oToImg(mI) #<! Does not scale or change type\n",
    "\n",
    "print(f'Tensor Type: {type(tI)}')\n",
    "print(f'Tensor Dimensions: {tI.shape}')\n",
    "print(f'Image Element Type: {tI.dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using `ToDtype`\n",
    "\n",
    "oToDtype = TorchVisionTrns.ToDtype(dtype = torch.float32, scale = True) #<! Converts to TorchVision's Image\n",
    "tIF = oToDtype(mI) #<! Does not scale or change type\n",
    "\n",
    "# Won't have affect unless the input is `tv_tensors`\n",
    "print(f'Tensor Type: {type(tIF)}')\n",
    "print(f'Tensor Dimensions: {tIF.shape}')\n",
    "print(f'Image Element Type: {tIF.dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using `ToDtype`\n",
    "\n",
    "oToImg = TorchVisionTrns.Compose([                                #<! Similar to `nn.Sequential`\n",
    "    TorchVisionTrns.ToImage(),                                    #<! Converts to `TVTensor` and (C, H, W)\n",
    "    TorchVisionTrns.ToDtype(dtype = torch.float32, scale = True), #<! Converts to `torch.float32` and scales to [0, 1]\n",
    "])\n",
    "\n",
    "tIF = oToImg(mI) #<! `mI` is a NumPy array\n",
    "\n",
    "print(f'Tensor Type        : {type(tIF)}')\n",
    "print(f'Tensor Dimensions  : {tIF.shape}')\n",
    "print(f'Image Element Type : {tIF.dtype}')\n",
    "print(f'Image Minimum Value: {torch.min(tIF)}')\n",
    "print(f'Image Maximum Value: {torch.max(tIF)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad Image\n",
    "\n",
    "Pads the image to enlarge its size.  \n",
    "Could be used to equate size of a set of images, though better be done with `CenterCrop`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad\n",
    "\n",
    "# Works on `uint8` types which are faster!\n",
    "lTrnImg = [tI] + [TorchVisionTrns.Pad(padding = padSize)(tI) for padSize in (5, 10, 30, 50)]\n",
    "hF = PlotTransform(lTrnImg, 'Pad', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Why is the boundary black? Look at `fill` parameter in [`Pad`](https://docs.pytorch.org/vision/stable/generated/torchvision.transforms.Pad.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad\n",
    "# Padding with boundary conditions\n",
    "\n",
    "lPadMode = ['constant', 'edge', 'reflect', 'symmetric']\n",
    "\n",
    "lTrnImg = [tI] + [TorchVisionTrns.Pad(padding = padSize, fill = 128, padding_mode = padMode)(tI) for padMode, padSize in zip(lPadMode, (10, 20, 30, 40))]\n",
    "hF = PlotTransform(lTrnImg, 'Pad', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resize\n",
    "\n",
    "Resizing allows handling a data set with different dimensions or adjust complexity.  \n",
    "It also can assist making the model multi scaled as it has to have the same result for different sizes.\n",
    "\n",
    "It can resize to a fixed size (May change the aspect ratio) or fixed minimum size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize\n",
    "\n",
    "# Works on `uint8` types which are faster!\n",
    "lTrnImg = [tI] + [TorchVisionTrns.Resize(size = imgSize)(tI) for imgSize in (128, 64, 32, 16)]\n",
    "hF = PlotTransform(lTrnImg, 'Resize', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Which dimension was resized? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Center Crop\n",
    "\n",
    "Effective way to normalize the image size.  \n",
    "It ensures the output size. So smaller images are padded.\n",
    "\n",
    "See also `RandomCrop`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Center Crop\n",
    "# Works on `Float32` types which are slower\n",
    "lTrnImg = [tIF] + [TorchVisionTrns.CenterCrop(size = imgSize)(tIF) for imgSize in (400, 225, 200, 175, 150)]\n",
    "hF = PlotTransform(lTrnImg, 'CenterCrop', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resize with Aspect Ratio\n",
    "\n",
    "Combining the `Resize` and `CenterCrop` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize with Keeping Aspect Ratio\n",
    "\n",
    "imgSize = 128 #<! Target output dimensions\n",
    "\n",
    "oResizeAspectRatio = TorchVisionTrns.Compose([ \n",
    "    TorchVisionTrns.Resize(size = imgSize),    \n",
    "    TorchVisionTrns.CenterCrop(size = imgSize),\n",
    "])\n",
    "\n",
    "lTrnImg = [tIF] + [oResizeAspectRatio(tIF)]\n",
    "hF = PlotTransform(lTrnImg, 'CenterCrop', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Five Crops\n",
    "\n",
    "Generates fixe crops of the image: 4 corners and center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Five Crop\n",
    "# Works on `Float32` types which are slower\n",
    "lTrnImg = [tIF] + list(TorchVisionTrns.FiveCrop(size = 224)(tIF))\n",
    "hF = PlotTransform(lTrnImg, 'FiveCrop', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grayscale\n",
    "\n",
    "In order to make the model insensitive to color, one could convert images into _grayscale_.  \n",
    "For compatibility, it allows setting the number of output channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grayscale\n",
    "# Works on `uint8` types which are faster!\n",
    "lTrnImg = [tI] + [TorchVisionTrns.Grayscale(num_output_channels = 3)(tI)]\n",
    "hF = PlotTransform(lTrnImg, 'Grayscale', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color Jitter\n",
    "\n",
    "Another way to make the model less sensitive to color, or at least color accuracy, is by changing its color randomly.  \n",
    "The `ColorJitter` transform randomly changes the brightness, saturation and other properties of an image to achieve that.\n",
    "\n",
    "There are options to alter the channels (`RandomChannelPermutation`) and combine them (`RandomPhotometricDistort`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ColorJitter\n",
    "# Works on `uint8` types which are faster!\n",
    "lTrnImg = [tI] + [TorchVisionTrns.ColorJitter(brightness = 0.25, saturation = 0.25, hue = 0.25)(tI) for _ in range(5)]\n",
    "hF = PlotTransform(lTrnImg, 'ColorJitter', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Blur\n",
    "\n",
    "Blurring the image removes details and also, to some degree, have scaling effect.  \n",
    "Hence it can be used to add robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GaussianBlur\n",
    "# Works on `uint8` types which are faster!\n",
    "lTrnImg = [tI] + [TorchVisionTrns.GaussianBlur(kernel_size = (31, 31), sigma = (ii))(tI) for ii in range(1, 11, 2)]\n",
    "hF = PlotTransform(lTrnImg, 'GaussianBlur', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Perspective \n",
    "\n",
    "Applies a transformation on the image coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomPerspective\n",
    "# Works on `uint8` types which are faster!\n",
    "lTrnImg = [tI] + [TorchVisionTrns.RandomPerspective(distortion_scale = 0.4, p = 1.0)(tI) for _ in range(5)]\n",
    "hF = PlotTransform(lTrnImg, 'RandomPerspective', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Rotation \n",
    "\n",
    "A specific case of perspective distortion is rotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomRotation\n",
    "# Works on `uint8` types which are faster!\n",
    "lTrnImg = [tI] + [TorchVisionTrns.RandomRotation(degrees = (-45, 45))(tI) for _ in range(5)]\n",
    "hF = PlotTransform(lTrnImg, 'RandomRotation', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Affine\n",
    "\n",
    "Applies _affine transformation_ on the image coordinates.\n",
    "\n",
    "**TODO**: Add example for Box / Mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomAffine\n",
    "# Works on `uint8` types which are faster!\n",
    "lTrnImg = [tI] + [TorchVisionTrns.RandomAffine(degrees = (-45, 45), translate = (0.1, 0.3), scale = (0.75, 0.95))(tI) for _ in range(5)]\n",
    "hF = PlotTransform(lTrnImg, 'RandomAffine', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Crop\n",
    "\n",
    "Applies a crop with a random location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomCrop\n",
    "# Works on `uint8` types which are faster!\n",
    "lTrnImg = [tI] + [TorchVisionTrns.RandomCrop(size = (200, 250))(tI) for _ in range(5)]\n",
    "hF = PlotTransform(lTrnImg, 'RandomCrop', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Crop and Resize\n",
    "\n",
    "Allows insensitive to partial view, shift (Random crop location) and scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomResizedCrop\n",
    "# Works on `uint8` types which are faster!\n",
    "lTrnImg = [tI] + [TorchVisionTrns.RandomResizedCrop(size = (250, 200), scale = (0.25, 1.25))(tI) for _ in range(5)]\n",
    "hF = PlotTransform(lTrnImg, 'RandomResizedCrop', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Invert, Posterize and Solarize\n",
    "\n",
    "Color effects: Invert the image, reduce the number of effective bits and selective inversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomInvert\n",
    "# Works on `uint8` types which are faster!\n",
    "lTrnImg = [tI] + [TorchVisionTrns.RandomInvert(p = 1)(tI) for _ in range(1)]\n",
    "hF = PlotTransform(lTrnImg, 'RandomInvert', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomPosterize\n",
    "# Works on `uint8` types which are faster!\n",
    "lTrnImg = [tI] + [TorchVisionTrns.RandomPosterize(bits = ii, p = 1.0)(tI) for ii in reversed(range(1, 6))]\n",
    "hF = PlotTransform(lTrnImg, 'RandomPosterize', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomSolarize\n",
    "# Works on `uint8` types which are faster!\n",
    "lTrnImg = [tI] + [TorchVisionTrns.RandomSolarize(threshold = ii, p = 1.0)(tI) for ii in [250, 200, 150, 100, 50]]\n",
    "hF = PlotTransform(lTrnImg, 'RandomSolarize', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Sharpness Adjustment\n",
    "\n",
    "Changes the sharpness of the image. \n",
    "Basically using Unsharp Mask like effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomAdjustSharpness\n",
    "# Works on `uint8` types which are faster!\n",
    "lTrnImg = [tI] + [TorchVisionTrns.RandomAdjustSharpness(sharpness_factor = ii, p = 1.0)(tI) for ii in range(0, 20, 4)]\n",
    "hF = PlotTransform(lTrnImg, 'RandomAdjustSharpness', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Auto Contrast\n",
    "\n",
    "Applies _auto contrast_ effect to the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomAutocontrast\n",
    "# Works on `uint8` types which are faster!\n",
    "lTrnImg = [tI] + [TorchVisionTrns.RandomAutocontrast(p = 1.0)(tI) for _ in range(1)]\n",
    "hF = PlotTransform(lTrnImg, 'RandomAutocontrast', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Equalize\n",
    "\n",
    "Applies _histogram equalization_ effect to the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomEqualize\n",
    "# Works on `uint8` types which are faster!\n",
    "lTrnImg = [tI] + [TorchVisionTrns.RandomEqualize(p = 1.0)(tI) for _ in range(1)]\n",
    "hF = PlotTransform(lTrnImg, 'RandomEqualize', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Vertical / Horizontal Flips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Flip\n",
    "\n",
    "oTran = TorchVisionTrns.Compose([\n",
    "    TorchVisionTrns.RandomHorizontalFlip(p = 0.5),\n",
    "    TorchVisionTrns.RandomVerticalFlip(p = 0.5),\n",
    "])\n",
    "\n",
    "lTrnImg = [tI] + [oTran(tI) for _ in range(6)]\n",
    "hF = PlotTransform(lTrnImg, 'RandomFlip', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Can it be used for the MNIST data set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto Augmentation\n",
    "\n",
    "Applies several combination according to a policy.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> In order to see the operations applied, have a look at the code linked at [`AutoAugmentPolicy`](https://pytorch.org/vision/main/generated/torchvision.transforms.AutoAugmentPolicy.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoAugment\n",
    "# Works on `uint8` types which are faster!\n",
    "lTrnImg = [tI] + [TorchVisionTrns.AutoAugment(policy = torchvision.transforms.AutoAugmentPolicy.IMAGENET)(tI) for _ in range(5)]\n",
    "hF = PlotTransform(lTrnImg, 'AutoAugment', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='blue'>(**!**)</font> Use [`Lambda`](https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.Lambda.html) to generate a custom transformation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
