{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Fixel Algorithms](https://fixelalgorithms.co/images/CCExt.png)](https://fixelalgorithms.gitlab.io)\n",
    "\n",
    "# Deep Learning Methods\n",
    "\n",
    "## Deep Learning for Computer Vision - Object Detection - Object Localization\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 1.1.000 | 27/01/2026 | Royi Avital | Added the use of Depth Wise Separable Convolution                  |\n",
    "| 1.1.000 | 27/01/2026 | Royi Avital | The dataset returns a tuple of the label and the box               |\n",
    "| 1.0.001 | 13/06/2024 | Royi Avital | Fixed issue with the class label of the results                    |\n",
    "| 1.0.000 | 08/06/2024 | Royi Avital | First version                                                      |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/AIProgram/2024_02/0098DeepLearningObjectLocalization.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T09:30:06.492269Z",
     "start_time": "2022-02-02T09:30:06.220934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn            as nn\n",
    "import torch.nn.functional as F\n",
    "import torchinfo\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "from torchmetrics.detection.iou import IntersectionOverUnion\n",
    "\n",
    "import torchvision\n",
    "from torchvision.transforms import v2 as TorchVisionTrns\n",
    "\n",
    "# Miscellaneous\n",
    "import os\n",
    "from platform import python_version\n",
    "import random\n",
    "\n",
    "# Typing\n",
    "from typing import Callable, Dict, Generator, List, Optional, Self, Set, Tuple, Union\n",
    "from numpy.typing import NDArray\n",
    "from torch import Tensor\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought.\n",
    "\n",
    "Code Notations:\n",
    "\n",
    "```python\n",
    "someVar    = 2; #<! Notation for a variable\n",
    "vVector    = np.random.rand(4) #<! Notation for 1D array\n",
    "mMatrix    = np.random.rand(4, 3) #<! Notation for 2D array\n",
    "tTensor    = np.random.rand(4, 3, 2, 3) #<! Notation for nD array (Tensor)\n",
    "tuTuple    = (1, 2, 3) #<! Notation for a tuple\n",
    "lList      = [1, 2, 3] #<! Notation for a list\n",
    "dDict      = {1: 3, 2: 2, 3: 1} #<! Notation for a dictionary\n",
    "oObj       = MyClass() #<! Notation for an object\n",
    "dfData     = pd.DataFrame() #<! Notation for a data frame\n",
    "dsData     = pd.Series() #<! Notation for a series\n",
    "hObj       = plt.Axes() #<! Notation for an object / handler / function handler\n",
    "```\n",
    "\n",
    "### Code Exercise\n",
    "\n",
    " - Single line fill\n",
    "\n",
    "```python\n",
    "valToFill = ???\n",
    "```\n",
    "\n",
    " - Multi Line to Fill (At least one)\n",
    "\n",
    "```python\n",
    "# You need to start writing\n",
    "?????\n",
    "```\n",
    "\n",
    " - Section to Fill\n",
    "\n",
    "```python\n",
    "#===========================Fill This===========================#\n",
    "# 1. Explanation about what to do.\n",
    "# !! Remarks to follow / take under consideration.\n",
    "mX = ???\n",
    "\n",
    "?????\n",
    "#===============================================================#\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# %matplotlib inline\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "# Matplotlib default color palette\n",
    "lMatPltLibclr = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "# sns.set_theme() #>! Apply SeaBorn theme\n",
    "\n",
    "runInGoogleColab = 'google.colab' in str(get_ipython())\n",
    "\n",
    "# Improve performance by benchmarking\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Reproducibility (Per PyTorch Version on the same device)\n",
    "# torch.manual_seed(seedNum)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark     = False #<! Makes things slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "FIG_SIZE_DEF    = (8, 8)\n",
    "ELM_SIZE_DEF    = 50\n",
    "CLASS_COLOR     = ('b', 'r')\n",
    "EDGE_COLOR      = 'k'\n",
    "MARKER_SIZE_DEF = 10\n",
    "LINE_WIDTH_DEF  = 2\n",
    "\n",
    "PROJECT_NAME     = 'FixelCourses'\n",
    "DATA_FOLDER_NAME = 'DataSets'\n",
    "BASE_FOLDER_PATH = os.getcwd()[:(len(os.getcwd()) - (os.getcwd()[::-1].lower().find(PROJECT_NAME.lower()[::-1])))]\n",
    "DATA_FOLDER_PATH = os.path.join(BASE_FOLDER_PATH, DATA_FOLDER_NAME)\n",
    "\n",
    "TENSOR_BOARD_BASE = 'TB'\n",
    "\n",
    "D_CLASSES  = {0: 'Red', 1: 'Green', 2: 'Blue'}\n",
    "L_CLASSES  = ['R', 'G', 'B']\n",
    "T_IMG_SIZE = (100, 100, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Auxiliary Modules for Google Colab\n",
    "if runInGoogleColab:\n",
    "    !wget https://raw.githubusercontent.com/FixelAlgorithmsTeam/FixelCourses/master/AIProgram/2024_02/DataManipulation.py\n",
    "    !wget https://raw.githubusercontent.com/FixelAlgorithmsTeam/FixelCourses/master/AIProgram/2024_02/DataVisualization.py\n",
    "    !wget https://raw.githubusercontent.com/FixelAlgorithmsTeam/FixelCourses/master/AIProgram/2024_02/DeepLearningPyTorch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courses Packages\n",
    "\n",
    "from DataManipulation import BBoxFormat\n",
    "from DataManipulation import GenLabeldEllipseImg\n",
    "from DataVisualization import PlotBox, PlotBBox, PlotLabelsHistogram\n",
    "from DeepLearningPyTorch import ObjectLocalizationDataset\n",
    "from DeepLearningPyTorch import GenDataLoaders, GetBatch, InitWeightsKaiNorm, TrainModel, TrainModelSch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='blue'>(**!**)</font> Go through `GenLabeldDataEllipse()`.\n",
    "* <font color='blue'>(**!**)</font> Go through `ObjectLocalizationDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Auxiliary Functions\n",
    "\n",
    "def GenData( numSamples: int, tuImgSize: Tuple[int, int, int], boxFormat: BBoxFormat = BBoxFormat.YOLO ) -> Tuple[NDArray, NDArray, NDArray]:\n",
    "    \"\"\"\n",
    "    Generate synthetic data for object localization..\n",
    "    Parameters\n",
    "    ----------\n",
    "    numSamples : int\n",
    "        Number of samples to generate.\n",
    "    tuImgSize : Tuple[int, int, int]\n",
    "        Image size as (Height, Width, Channels).\n",
    "    boxFormat : BBoxFormat, optional\n",
    "        Bounding box format, by default BBoxFormat.YOLO.\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[np.ndarray, np.ndarray, np.ndarray]\n",
    "        mX : np.ndarray\n",
    "            Feature matrix of shape (numSamples, Channels, Height, Width).\n",
    "        vY : np.ndarray\n",
    "            Label vector of shape (numSamples,).\n",
    "        mB : np.ndarray\n",
    "            Bounding box matrix of shape (numSamples, 4).\n",
    "    \"\"\"\n",
    "\n",
    "    numObj = 1\n",
    "    \n",
    "    mX = np.empty(shape = (numSamples, *tuImgSize[::-1]))\n",
    "    vY = np.empty(shape = numSamples, dtype = np.int_)\n",
    "    mB = np.empty(shape = (numSamples, 4))\n",
    "\n",
    "    for ii in range(numSamples):\n",
    "        mI, vLbl, mBB = GenLabeldEllipseImg(tuImgSize[:2], numObj, boxFormat = boxFormat)\n",
    "        mX[ii]  = np.transpose(mI, (2, 0, 1)) #<! (C, H, W)\n",
    "        vY[ii]  = vLbl[0]\n",
    "        mB[ii]  = mBB[0]\n",
    "    \n",
    "    return mX, vY, mB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Localization\n",
    "\n",
    "The composability of _Deep Learning_ loss allows combining 2 tasks into a _single model_.  \n",
    "_Object Localization_ is a composition of 2 tasks:\n",
    "\n",
    " - Classification: Identify the object class.\n",
    " - Regression: Localize the object by a _Bounding Box_ (BB).\n",
    "\n",
    "This notebook demonstrates:\n",
    " - Generating a synthetic data set.\n",
    " - Building a model for _object localization_\n",
    " - Training a model with a composed objective.\n",
    "\n",
    "</br>\n",
    "\n",
    "* <font color='brown'>(**#**)</font> In the notebook context _Object Localization_ assumes the existence of an object in the image and only a single object.\n",
    "* <font color='brown'>(**#**)</font> In the notebook context _Object Detection_ generalizes the task to support the case of non existence or several objects.\n",
    "* <font color='brown'>(**#**)</font> The motivation for a synthetic dataset is being able to implement the whole training process (Existing datasets are huge).  \n",
    "  Yet the ability to create synthetic dataset is a useful skill.\n",
    "* <font color='brown'>(**#**)</font> There are known datasets for object detection: [COCO Dataset](https://cocodataset.org), [PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/).   \n",
    "  They also define standards for the labeling system.  \n",
    "  Training them is on the scale of days.\n",
    "* <font color='brown'>(**#**)</font> [Object Detection Annotation Formats](https://albumentations.ai/docs/3-basic-usage/bounding-boxes-augmentations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# Data\n",
    "numSamplesTrain = 30_000\n",
    "numSamplesVal   = 10_000\n",
    "boxFormat       = BBoxFormat.YOLO\n",
    "numCls          = len(L_CLASSES) #<! Number of classes\n",
    "\n",
    "# Model\n",
    "dropP = 0.5 #<! Dropout Layer\n",
    "\n",
    "# Training\n",
    "batchSize  = 256\n",
    "numWorkers = 2 #<! Number of workers\n",
    "numEpochs  = 35\n",
    "λ          = 20.0 #<! Localization Loss\n",
    "ϵ          = 0.1  #<! Label Smoothing\n",
    "\n",
    "# Visualization\n",
    "numImg = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate / Load Data\n",
    "\n",
    "The data is synthetic data.  \n",
    "Each image includes and Ellipse where its color is the class (`R`, `G`, `B`) and the bounding rectangle.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The label is a vector of `5`: `[Class, xCenter, yCenter, boxWidth, boxHeight]`.  \n",
    "* <font color='brown'>(**#**)</font> The label is in `YOLO` format, hence it is normalized to `[0, 1]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Sample\n",
    "\n",
    "mI, vY, mBB = GenLabeldEllipseImg(T_IMG_SIZE[:2], 1, boxFormat = boxFormat)\n",
    "vBox = mBB[0] #<! Matrix to support multiple objects in a single image\n",
    "clsIdx = vY[0]\n",
    "hA = PlotBox(mI, L_CLASSES[clsIdx], vBox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> One could use negative values for the bounding box. The model will extrapolate the object dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Data\n",
    "\n",
    "tXTrain, vYTrain, mBBTrain = GenData(numSamplesTrain, T_IMG_SIZE, boxFormat = boxFormat)\n",
    "tXVal,   vYVal,   mBBVal   = GenData(numSamplesVal, T_IMG_SIZE, boxFormat = boxFormat)\n",
    "\n",
    "print(f'The training data set data shape: {tXTrain.shape}')\n",
    "print(f'The training data set labels shape: {vYTrain.shape}')\n",
    "print(f'The training data set box shape: {mBBTrain.shape}')\n",
    "print(f'The validation data set data shape: {tXVal.shape}')\n",
    "print(f'The validation data set labels shape: {vYTrain.shape}')\n",
    "print(f'The validation data set box shape: {mBBVal.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Data\n",
    "\n",
    "dsTrain = ObjectLocalizationDataset(tXTrain, vYTrain, mBBTrain)\n",
    "dsVal   = ObjectLocalizationDataset(tXVal, vYVal, mBBVal)\n",
    "lClass  = dsTrain.GetLabels(uniqueCls = False)\n",
    "\n",
    "print(f'The training data set data shape: {dsTrain._tX.shape}')\n",
    "print(f'The test data set data shape: {dsVal._tX.shape}')\n",
    "print(f'The unique values of the labels: {np.unique(lClass)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> PyTorch with the `v2` transforms deals with bounding boxes using special type: `BoundingBoxes`.\n",
    "* <font color='brown'>(**#**)</font> For _data augmentation_ see:\n",
    "    - [Transforming and Augmenting Images](https://pytorch.org/vision/stable/transforms.html).\n",
    "    - [Getting Started with Transforms v2](https://pytorch.org/vision/stable/auto_examples/transforms/plot_transforms_getting_started.html).\n",
    "    - [Transforms v2: End to End Object Detection / Segmentation Example](https://pytorch.org/vision/stable/auto_examples/transforms/plot_transforms_e2e.html).\n",
    "    - [How to Write Your Own v2 Transforms](https://pytorch.org/vision/stable/auto_examples/transforms/plot_custom_transforms.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Element of the Data Set\n",
    "\n",
    "tX, (valY, vB) = dsTrain[0]\n",
    "\n",
    "print(f'The features shape: {tX.shape}')\n",
    "print(f'The label value: {valY}')\n",
    "print(f'The bounding box value: {vB}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> Since the labels are in the same contiguous container as the bounding box parameters, their type is `Float`.\n",
    "* <font color='brown'>(**#**)</font> The bounding box is using absolute values. In practice it is commonly normalized to the image dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Data\n",
    "\n",
    "hA = PlotBox(np.transpose(tX, (1, 2, 0)), L_CLASSES[valY], vB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of Labels\n",
    "\n",
    "hA = PlotLabelsHistogram(lClass, lClass = L_CLASSES);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loaders\n",
    "\n",
    "This section defines the data loaded.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader\n",
    "\n",
    "# dlTrain = torch.utils.data.DataLoader(dsTrain, shuffle = True, batch_size = 1 * batchSize, num_workers = numWorkers, drop_last = True, persistent_workers = True)\n",
    "# dlVal   = torch.utils.data.DataLoader(dsVal, shuffle = False, batch_size = 2 * batchSize, num_workers = numWorkers, persistent_workers = True)\n",
    "\n",
    "# In case of errors with the workers, use the following code to disable them\n",
    "dlTrain = torch.utils.data.DataLoader(dsTrain, shuffle = True, batch_size = 1 * batchSize, num_workers = 0, drop_last = True, persistent_workers = False)\n",
    "dlVal   = torch.utils.data.DataLoader(dsVal, shuffle = False, batch_size = 2 * batchSize, num_workers = 0, persistent_workers = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate on the Loader\n",
    "# The first batch.\n",
    "\n",
    "tX, (vY, mB) = GetBatch(dlTrain)\n",
    "\n",
    "print(f'The batch features dimensions: {tX.shape} with dtype {tX.dtype}')\n",
    "print(f'The batch labels dimensions: {vY.shape} with dtype {vY.dtype}')\n",
    "print(f'The batch bounding box dimensions: {mB.shape} with dtype {mB.dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model\n",
    "\n",
    "This section defines the model.  \n",
    "\n",
    "* <font color='brown'>(**#**)</font> The following implementation has a model with a single output, both for the regression and the classification.\n",
    "* <font color='brown'>(**#**)</font> One could create 2 different outputs (_Heads_) for each task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depth Wise Separable Convolution\n",
    "\n",
    "The Depth Wise Separable Convolution merges 2 concept:\n",
    " - Spatial Convolutions in Groups.\n",
    " - Projection by `1x1` Convolution.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> See [Animated AI - Groups, Depthwise and Depthwise Separable Convolution (Neural Networks)](https://www.youtube.com/watch?v=vVaRhZXovbw).\n",
    "* <font color='brown'>(**#**)</font> The concept was made popular by models such as EfficientNet ([EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946)) and [MobileNet](https://en.wikipedia.org/wiki/MobileNet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depth Wise Separable Convolutional Layer\n",
    "\n",
    "class DepthWiseSeparableConv2D( nn.Module ):\n",
    "    def __init__( self, inChannels: int, outChannels: int, kernelSize: int, stride: int = 1, padding: Union[int, str] = 0, bias: bool = True ) -> None:\n",
    "        super(DepthWiseSeparableConv2D, self).__init__()\n",
    "\n",
    "        self.oDepthWiseSeparableConv2D = nn.Sequential(\n",
    "            nn.Conv2d(inChannels, inChannels, kernel_size = kernelSize, stride = stride, padding = padding, groups = inChannels, bias = bias),\n",
    "            nn.Conv2d(inChannels, outChannels, kernel_size = 1, stride = 1, padding = 0, bias = bias)\n",
    "        )\n",
    "    \n",
    "    def forward( self, tX: Tensor ) -> Tensor:\n",
    "        \n",
    "        tX = self.oDepthWiseSeparableConv2D(tX)\n",
    "        \n",
    "        return tX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "# Model generating function.\n",
    "\n",
    "def BuildModel( numCls: int, useDepthWiseSeparable: bool = False ) -> nn.Module:\n",
    "\n",
    "    if useDepthWiseSeparable:\n",
    "        oConvLayer = DepthWiseSeparableConv2D\n",
    "    else:\n",
    "        oConvLayer = nn.Conv2d\n",
    "\n",
    "    oModel = nn.Sequential(\n",
    "        nn.Identity(),\n",
    "        nn.Conv2d(3,   32,  3, stride = 2, padding = 0, bias = False), nn.BatchNorm2d(32 ), nn.ReLU(),\n",
    "        oConvLayer(32,  32,  3, stride = 1, padding = 1, bias = False), nn.BatchNorm2d(32 ), nn.ReLU(),\n",
    "        oConvLayer(32,  32,  3, stride = 2, padding = 0, bias = False), nn.BatchNorm2d(32 ), nn.ReLU(),\n",
    "        oConvLayer(32,  32,  3, stride = 1, padding = 1, bias = False), nn.BatchNorm2d(32 ), nn.ReLU(),\n",
    "        oConvLayer(32,  32,  3, stride = 1, padding = 1, bias = False), nn.BatchNorm2d(32 ), nn.ReLU(),\n",
    "        oConvLayer(32,  64,  3, stride = 2, padding = 1, bias = False), nn.BatchNorm2d(64 ), nn.ReLU(),\n",
    "        oConvLayer(64,  64,  3, stride = 1, padding = 1, bias = False), nn.BatchNorm2d(64 ), nn.ReLU(),\n",
    "        oConvLayer(64,  64,  3, stride = 1, padding = 1, bias = False), nn.BatchNorm2d(64 ), nn.ReLU(),\n",
    "        oConvLayer(64,  64,  3, stride = 1, padding = 1, bias = False), nn.BatchNorm2d(64 ), nn.ReLU(),\n",
    "        oConvLayer(64,  64,  3, stride = 1, padding = 1, bias = False), nn.BatchNorm2d(64 ), nn.ReLU(),\n",
    "        oConvLayer(64,  64,  3, stride = 2, padding = 1, bias = False), nn.BatchNorm2d(64 ), nn.ReLU(),\n",
    "        oConvLayer(64,  128, 3, stride = 1, padding = 0, bias = False), nn.BatchNorm2d(128), nn.ReLU(),\n",
    "        oConvLayer(128, 256, 3, stride = 1, padding = 0, bias = False), nn.BatchNorm2d(256), nn.ReLU(),\n",
    "        oConvLayer(256, 512, 2, stride = 1, padding = 0, bias = False), nn.BatchNorm2d(512), nn.ReLU(),\n",
    "        nn.Conv2d(512, numCls + 4, 1, stride = 1, padding = 0, bias = True),\n",
    "        nn.Flatten()\n",
    "    )\n",
    "\n",
    "    return oModel "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> What's the motivation for the depth of the model (Relatively deep)?\n",
    "* <font color='red'>(**?**)</font> Explain the actual operation of the last `Conv2D` layer. Can it be replaced with a `Linear` layer?\n",
    "* <font color='brown'>(**#**)</font> One could set the image to a power of 2. Then all convolution layers could have been with padding and stride of 2 until the size is `1x1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Model\n",
    "\n",
    "oModel = BuildModel(len(L_CLASSES), useDepthWiseSeparable = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Information\n",
    "# Pay attention to the layers name.\n",
    "torchinfo.summary(oModel, (batchSize, *(T_IMG_SIZE[::-1])), col_names = ['kernel_size', 'output_size', 'num_params'], device = 'cpu', row_settings = ['depth', 'var_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Explain the dimensions of the last layer.\n",
    "* <font color='red'>(**?**)</font> Will the model work with smaller images?\n",
    "* <font color='blue'>(**!**)</font> Compare the number of parameters when using _Depth Wise Separable Convolution_ and using regular convolution layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "This section trains the model.  \n",
    "\n",
    "* <font color='brown'>(**#**)</font> The training loop must be adapted to the new loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Localization Loss\n",
    "\n",
    "The loss is a composite of 2 loss functions:\n",
    "\n",
    "$$\\ell\\left(\\hat{\\boldsymbol{y}},\\boldsymbol{y}\\right)=\\lambda_{\\text{MSE}}\\cdot\\ell_{\\text{MSE}}\\left(\\hat{\\boldsymbol{y}}_{\\text{bbox}},\\boldsymbol{y}_{\\text{bbox}}\\right)+\\lambda_{\\text{CE}}\\cdot\\ell_{\\text{CE}}\\left(\\hat{\\boldsymbol{y}}_{\\text{label}},\\boldsymbol{y}_{\\text{label}}\\right)$$\n",
    "\n",
    "Where $\\lambda_{\\text{MSE}}$ and $\\lambda_{\\text{CE}}$ are the weights of each loss.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> In practice a single $\\lambda$ is required.\n",
    "* <font color='brown'>(**#**)</font> The MSE is not optimal loss function. It will be replaced by the _Log Euclidean_ loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object Localization Loss\n",
    "class ObjLocLoss( nn.Module ):\n",
    "    def __init__( self, numCls: int, λ: float, ϵ: float = 0.0 ) -> None:\n",
    "        super(ObjLocLoss, self).__init__()\n",
    "\n",
    "        self.numCls   = numCls\n",
    "        self.λ        = λ\n",
    "        self.ϵ        = ϵ\n",
    "        self.oRegLoss = nn.MSELoss()\n",
    "        self.oClsLoss = nn.CrossEntropyLoss(label_smoothing = ϵ)\n",
    "    \n",
    "    def forward( self: Self, mYHat: Tensor, tuY: Tuple[Tensor, Tensor] ) -> Tensor:\n",
    "\n",
    "        regLoss = self.oRegLoss(mYHat[:, self.numCls:], tuY[1])\n",
    "        clsLoss = self.oClsLoss(mYHat[:, :self.numCls], tuY[0])\n",
    "\n",
    "        lossVal = (self.λ * regLoss) + clsLoss\n",
    "\t\t\n",
    "        return lossVal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Localization Score\n",
    "\n",
    "The score is defined by the _IoU_ of a valid classification:\n",
    "\n",
    "$$\\text{Score}=\\frac{1}{N}\\sum_{i=1}^{N}\\mathbb{I}\\left\\{ \\hat{y}_{i}=y_{i}\\right\\} \\cdot\\text{IoU}\\left(\\hat{B}_{i},B_{i}\\right)$$\n",
    "\n",
    "Where:\n",
    "- $\\hat{y}_{i}$ is the predicted label\n",
    "- $y_{i}$ is the correct label\n",
    "- $\\hat{B}_{i}$ is the predicted bounding box\n",
    "- $B_{i}$ is the correct bounding box\n",
    "In other words, the average IoU, considering only correct (label) prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> What are the bounds of the values of the score function?\n",
    "* <font color='red'>(**?**)</font> Is higher or lower value bette for the score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object Localization Score\n",
    "class ObjLocScore( nn.Module ):\n",
    "    def __init__( self, numCls: int ) -> None:\n",
    "        super(ObjLocScore, self).__init__()\n",
    "\n",
    "        self.numCls = numCls\n",
    "    \n",
    "    def forward( self: Self, mYHat: Tensor, tuY: Tuple[Tensor, Tensor] ) -> Tuple[float, float, float]:\n",
    "\n",
    "        batchSize = mYHat.shape[0]\n",
    "        \n",
    "        vY, mBox = tuY[0], tuY[1]\n",
    "        vIoU = torch.diag(torchvision.ops.box_iou(torchvision.ops.box_convert(mYHat[:, self.numCls:], 'cxcywh', 'xyxy'), torchvision.ops.box_convert(mBox, 'cxcywh', 'xyxy')))\n",
    "        vCor = (vY == torch.argmax(mYHat[:, :self.numCls], dim = 1)).to(torch.float32) #<! Correct labels\n",
    "\n",
    "        # valIoU      = torch.mean(vIoU).item()\n",
    "        # valAcc      = torch.mean(vCor).item()\n",
    "        valScore    = torch.inner(vIoU, vCor) / batchSize #<! Only correct predictions contribute to the score\n",
    "\t\t\n",
    "        return valScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Device\n",
    "\n",
    "runDevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') #<! The 1st CUDA device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Score Function\n",
    "\n",
    "hL = ObjLocLoss(numCls = numCls, λ = λ, ϵ = ϵ)\n",
    "hS = ObjLocScore(numCls = numCls)\n",
    "\n",
    "hL = hL.to(runDevice)\n",
    "hS = hS.to(runDevice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "\n",
    "oModel = oModel.to(runDevice)\n",
    "oOpt = torch.optim.AdamW(oModel.parameters(), lr = 1e-5, betas = (0.9, 0.99), weight_decay = 1e-5) #<! Define optimizer\n",
    "oSch = torch.optim.lr_scheduler.OneCycleLR(oOpt, max_lr = 5e-4, total_steps = numEpochs)\n",
    "_, lTrainLoss, lTrainScore, lValLoss, lValScore, lLearnRate = TrainModel(oModel, dlTrain, dlVal, oOpt, numEpochs, hL, hS, oSch = oSch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training Phase\n",
    "\n",
    "hF, vHa = plt.subplots(nrows = 1, ncols = 3, figsize = (12, 5))\n",
    "vHa = np.ravel(vHa)\n",
    "\n",
    "hA = vHa[0]\n",
    "hA.plot(lTrainLoss, lw = 2, label = 'Train')\n",
    "hA.plot(lValLoss, lw = 2, label = 'Validation')\n",
    "hA.set_title(f'Object Localization Loss (λ = {λ:0.1f})')\n",
    "hA.set_xlabel('Epoch')\n",
    "hA.set_ylabel('Loss')\n",
    "hA.legend()\n",
    "\n",
    "hA = vHa[1]\n",
    "hA.plot(lTrainScore, lw = 2, label = 'Train')\n",
    "hA.plot(lValScore, lw = 2, label = 'Validation')\n",
    "hA.set_title('Object Localization Score')\n",
    "hA.set_xlabel('Epoch')\n",
    "hA.set_ylabel('Score')\n",
    "hA.legend()\n",
    "\n",
    "hA = vHa[2]\n",
    "hA.plot(lLearnRate, lw = 2)\n",
    "hA.set_title('Learn Rate Scheduler')\n",
    "hA.set_xlabel('Epoch')\n",
    "hA.set_ylabel('Learn Rate');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Prediction\n",
    "# TODO: Check classification\n",
    "\n",
    "rndIdx = np.random.randint(numSamplesVal)\n",
    "\n",
    "tX, (valY, vB) = dsVal[rndIdx]\n",
    "\n",
    "with torch.no_grad():\n",
    "    tX = torch.tensor(tX)\n",
    "    tX = torch.unsqueeze(tX, 0)\n",
    "    tX = tX.to(runDevice)\n",
    "    mYHat = oModel(tX).detach().cpu().numpy()\n",
    "\n",
    "vYHat   = mYHat[0]\n",
    "valYHat = np.argmax(vYHat[:numCls])\n",
    "vBHat   = vYHat[numCls:]\n",
    "\n",
    "hA = PlotBox(np.transpose(tX, (1, 2, 0)), L_CLASSES[valY], vB)\n",
    "hA = PlotBBox(hA, L_CLASSES[valYHat], vBHat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> What would be the results if the generated data had more small ellipses?\n",
    "* <font color='green'>(**@**)</font> Display the _accuracy_ and _IoU_ scores and _MSE_ and _CE_ loss over the epochs.   \n",
    "  It will require updating the Loss, Score classes and the training function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
