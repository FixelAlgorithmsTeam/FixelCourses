{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Fixel Algorithms](https://fixelalgorithms.co/images/CCExt.png)](https://fixelalgorithms.gitlab.io)\n",
    "\n",
    "# Deep Learning Methods\n",
    "\n",
    "## Deep Learning - Transfer Learning\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 1.0.002 | 19/09/2025 | Royi Avital | Updated link of Google Colab                                       |\n",
    "| 1.0.001 | 27/05/2025 | Royi Avital | Updated `GenResNetModel()`                                         |\n",
    "| 1.0.001 | 27/05/2025 | Royi Avital | Using `onedrivedownloader` to download data                        |\n",
    "| 1.0.000 | 29/05/2024 | Royi Avital | First version                                                      |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/DeepLearningMethods/2025_08/0007DeepLearningTransferLearning.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T09:30:06.492269Z",
     "start_time": "2022-02-02T09:30:06.220934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn            as nn\n",
    "import torch.nn.functional as F\n",
    "import torchinfo\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "import torchvision\n",
    "from torchvision.transforms import v2 as TorchVisionTrns\n",
    "\n",
    "# Miscellaneous\n",
    "import onedrivedownloader\n",
    "import os\n",
    "from platform import python_version\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Typing\n",
    "from typing import Callable, Dict, Generator, List, Literal, Optional, Self, Set, Tuple, Union\n",
    "\n",
    "# Visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought.\n",
    "\n",
    "Code Notations:\n",
    "\n",
    "```python\n",
    "someVar    = 2; #<! Notation for a variable\n",
    "vVector    = np.random.rand(4) #<! Notation for 1D array\n",
    "mMatrix    = np.random.rand(4, 3) #<! Notation for 2D array\n",
    "tTensor    = np.random.rand(4, 3, 2, 3) #<! Notation for nD array (Tensor)\n",
    "tuTuple    = (1, 2, 3) #<! Notation for a tuple\n",
    "lList      = [1, 2, 3] #<! Notation for a list\n",
    "dDict      = {1: 3, 2: 2, 3: 1} #<! Notation for a dictionary\n",
    "oObj       = MyClass() #<! Notation for an object\n",
    "dfData     = pd.DataFrame() #<! Notation for a data frame\n",
    "dsData     = pd.Series() #<! Notation for a series\n",
    "hObj       = plt.Axes() #<! Notation for an object / handler / function handler\n",
    "```\n",
    "\n",
    "### Code Exercise\n",
    "\n",
    " - Single line fill\n",
    "\n",
    "```python\n",
    "valToFill = ???\n",
    "```\n",
    "\n",
    " - Multi Line to Fill (At least one)\n",
    "\n",
    "```python\n",
    "# You need to start writing\n",
    "?????\n",
    "```\n",
    "\n",
    " - Section to Fill\n",
    "\n",
    "```python\n",
    "#===========================Fill This===========================#\n",
    "# 1. Explanation about what to do.\n",
    "# !! Remarks to follow / take under consideration.\n",
    "mX = ???\n",
    "\n",
    "?????\n",
    "#===============================================================#\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# %matplotlib inline\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "# Matplotlib default color palette\n",
    "lMatPltLibclr = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "# sns.set_theme() #>! Apply SeaBorn theme\n",
    "\n",
    "runInGoogleColab = 'google.colab' in str(get_ipython())\n",
    "\n",
    "# Improve performance by benchmarking\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Reproducibility (Per PyTorch Version on the same device)\n",
    "# torch.manual_seed(seedNum)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark     = False #<! Makes things slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "FIG_SIZE_DEF    = (8, 8)\n",
    "ELM_SIZE_DEF    = 50\n",
    "CLASS_COLOR     = ('b', 'r')\n",
    "EDGE_COLOR      = 'k'\n",
    "MARKER_SIZE_DEF = 10\n",
    "LINE_WIDTH_DEF  = 2\n",
    "\n",
    "DATA_SET_FILE_NAME      = 'IntelImgCls.zip'\n",
    "DATA_SET_FOLDER_NAME    = 'IntelImgCls'\n",
    "\n",
    "D_CLASSES  = {0: 'Buildings', 1: 'Forest', 2: 'Glacier', 3: 'Mountain', 4: 'Sea', 5: 'Street'}\n",
    "L_CLASSES  = ['Buildings', 'Forest', 'Glacier', 'Mountain', 'Sea', 'Street']\n",
    "T_IMG_SIZE = (150, 150, 3)\n",
    "\n",
    "DATA_FOLDER_PATH    = 'Data'\n",
    "TENSOR_BOARD_BASE   = 'TB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Auxiliary Modules for Google Colab\n",
    "if runInGoogleColab:\n",
    "    import os\n",
    "    for fileName in ('DataManipulation.py', 'DataVisualization.py', 'DeepLearningBlocks.py', 'DeepLearningPyTorch.py'):\n",
    "        if os.path.exists(fileName):\n",
    "            continue\n",
    "        os.system(f'wget https://raw.githubusercontent.com/FixelAlgorithmsTeam/FixelCourses/master/DeepLearningMethods/2025_08/{fileName}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courses Packages\n",
    "\n",
    "from DataVisualization import PlotLabelsHistogram\n",
    "from DeepLearningPyTorch import TBLogger, TestDataSet\n",
    "from DeepLearningPyTorch import TrainModel, TrainModelSch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='blue'>(**!**)</font> Go through `TestDataSet`'s code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Auxiliary Functions\n",
    "\n",
    "def GenResNetModel( trainedModel: bool, numCls: int, resNetDepth: Literal[18, 34, 50] = 18 ) -> nn.Module:\n",
    "    # Read on the API change at: How to Train State of the Art Models Using TorchVision’s Latest Primitives\n",
    "    # https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives\n",
    "\n",
    "    match resNetDepth:\n",
    "        case 18:\n",
    "            modelFun = torchvision.models.resnet18\n",
    "            modelWeights = torchvision.models.ResNet18_Weights.IMAGENET1K_V1\n",
    "        case 34:\n",
    "            modelFun = torchvision.models.resnet34\n",
    "            modelWeights = torchvision.models.ResNet34_Weights.IMAGENET1K_V1\n",
    "        case 50:\n",
    "            modelFun = torchvision.models.resnet50\n",
    "            modelWeights = torchvision.models.ResNet50_Weights.IMAGENET1K_V2\n",
    "        case 101:\n",
    "            modelFun = torchvision.models.resnet101\n",
    "            modelWeights = torchvision.models.ResNet101_Weights.IMAGENET1K_V1\n",
    "        case 152:\n",
    "            modelFun = torchvision.models.resnet152\n",
    "            modelWeights = torchvision.models.ResNet152_Weights.IMAGENET1K_V1\n",
    "        case _:\n",
    "            raise ValueError(f'The `resNetDepth`: {resNetDepth} is invalid!')\n",
    "\n",
    "    if trainedModel:\n",
    "        oModel        = modelFun(weights = modelWeights)\n",
    "        numFeaturesIn = oModel.fc.in_features\n",
    "        # Assuming numCls << 1000\n",
    "        oModel.fc     = nn.Sequential(\n",
    "            nn.Linear(numFeaturesIn, 128), nn.ReLU(),\n",
    "            nn.Linear(128, numCls),\n",
    "        )\n",
    "    else:\n",
    "        oModel = modelFun(weights = None, num_classes = numCls)\n",
    "\n",
    "    return oModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "\n",
    "The ResNet model is considered to be one of the most successful architectures.  \n",
    "Its main novelty is the _Skip Connection_ which improved the performance greatly.\n",
    "\n",
    "By _hand waiving_ the contribution of the skip connection can be explained as:\n",
    "\n",
    " * Learning the additive residual.\n",
    " * Ensemble of models.\n",
    " * Skip vanishing / exploding gradients.\n",
    "\n",
    "\n",
    "This notebook presents the basics of _Transfer Learning_ in the context of image classification:\n",
    " - Loading a pretrained model on a classification task.\n",
    " - Adjusting its structure to the new classification task.\n",
    " - Finetuning the model.\n",
    " - Evaluating the model.\n",
    "\n",
    "</br>\n",
    "\n",
    "* <font color='brown'>(**#**)</font> A great recap on _Model Fine Tuning_ is given in the book [Dive into Deep Learning](https://d2l.ai): [Computer Vision - Fine Tuning](https://d2l.ai/chapter_computer-vision/fine-tuning.html).\n",
    "* <font color='brown'>(**#**)</font> [Flaws of ImageNet, Computer Vision's Favorite Dataset](https://iclr-blogposts.github.io/2025/blog/imagenet-flaws)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# Data\n",
    "fileUrl = 'https://technionmail-my.sharepoint.com/:u:/g/personal/royia_technion_ac_il/EXnOy43-NqZJic4PXql9x8sB8lfoMKPponmp0zxeXnQAsw?e=MsYu3i' #<! OneDrive URL\n",
    "\n",
    "# Model\n",
    "resNetDepth = 18 #<! ResNet Depth\n",
    "dropP       = 0.5 #<! Dropout Layer\n",
    "\n",
    "# Training\n",
    "batchSize   = 128\n",
    "numWorkers  = 4 #<! Number of workers\n",
    "numEpochs   = 10\n",
    "\n",
    "# Visualization\n",
    "numImg = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate / Load Data\n",
    "\n",
    "This notebook use the [Intel Image Classification Data Set](https://www.kaggle.com/datasets/puneet6060/intel-image-classification).  \n",
    "The data set is composed of 6 classes: `Buildings`, `Forest`, `Glacier`, `Mountain`, `Sea`, `Street`.\n",
    "\n",
    "The following code will arrange the data in a manner compatible with PyTorch's [`ImageFolder`](https://pytorch.org/vision/main/generated/torchvision.datasets.ImageFolder.html).\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The data set originally appeared on [Analytics Vidhya - Practice Problem: Intel Scene Classification Challenge](https://datahack.analyticsvidhya.com/contest/practice-problem-intel-scene-classification-challe).\n",
    "* <font color='brown'>(**#**)</font> Some of the images are not `150x150x3` hence they should be handled.\n",
    "* <font color='brown'>(**#**)</font> Some of the images are not labeled correctly (See discussions on Kaggle).\n",
    "\n",
    "### Downloading the Data\n",
    "\n",
    "The data should be downloaded automatically.  \n",
    "**In case of an issue**, it can be downloaded manually:\n",
    "\n",
    "1. Download the Zip file `archive.zip` from [Intel Image Classification Data Set](https://www.kaggle.com/datasets/puneet6060/intel-image-classification).\n",
    "2. Copy / Move the file into `AIProgram/<YYYY_MM>/Data` folder.\n",
    "3. Rename the file to match `DATA_SET_FILE_NAME`.\n",
    "4. Comment the line `onedrivedownloader.download(fileUrl, os.path.join(DATA_FOLDER_PATH, DATA_SET_FILE_NAME), unzip = False)` and run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrange Data for Image Folder\n",
    "# Assumes `archive.zip` in `./Data`\n",
    "\n",
    "dataSetPath = os.path.join(DATA_FOLDER_PATH, DATA_SET_FOLDER_NAME)\n",
    "if not os.path.isdir(dataSetPath):\n",
    "    os.mkdir(dataSetPath)\n",
    "lFiles = os.listdir(dataSetPath)\n",
    "\n",
    "onedrivedownloader.download(fileUrl, os.path.join(DATA_FOLDER_PATH, DATA_SET_FILE_NAME), unzip = False)\n",
    "\n",
    "if '.processed' not in lFiles: #<! Run only once\n",
    "    os.makedirs(os.path.join(dataSetPath, 'TMP'), exist_ok = True)\n",
    "    os.makedirs(os.path.join(dataSetPath, 'Test'), exist_ok = True)\n",
    "    for clsName in L_CLASSES:\n",
    "        os.makedirs(os.path.join(dataSetPath, 'Train', clsName), exist_ok = True)\n",
    "        os.makedirs(os.path.join(dataSetPath, 'Validation', clsName), exist_ok = True)\n",
    "    \n",
    "    shutil.unpack_archive(os.path.join(DATA_FOLDER_PATH, DATA_SET_FILE_NAME), os.path.join(dataSetPath, 'TMP'))\n",
    "\n",
    "    for dirPath, lSubDir, lF in os.walk(os.path.join(dataSetPath, 'TMP')):\n",
    "        if len(lF) > 0:\n",
    "            if 'test' in dirPath:\n",
    "                dstPath = os.path.join(dataSetPath, 'Validation')\n",
    "            elif 'train' in dirPath:\n",
    "                dstPath = os.path.join(dataSetPath, 'Train')\n",
    "            else:\n",
    "                dstPath = os.path.join(dataSetPath, 'Test')\n",
    "            \n",
    "            if 'buildings' in dirPath:\n",
    "                for fileName in lF:\n",
    "                    shutil.move(os.path.join(dirPath, fileName), os.path.join(dstPath, 'Buildings'))\n",
    "            elif 'forest' in dirPath:\n",
    "                for fileName in lF:\n",
    "                    shutil.move(os.path.join(dirPath, fileName), os.path.join(dstPath, 'Forest'))\n",
    "            elif 'glacier' in dirPath:\n",
    "                for fileName in lF:\n",
    "                    shutil.move(os.path.join(dirPath, fileName), os.path.join(dstPath, 'Glacier'))\n",
    "            elif 'mountain' in dirPath:\n",
    "                for fileName in lF:\n",
    "                    shutil.move(os.path.join(dirPath, fileName), os.path.join(dstPath, 'Mountain'))\n",
    "            elif 'sea' in dirPath:\n",
    "                for fileName in lF:\n",
    "                    shutil.move(os.path.join(dirPath, fileName), os.path.join(dstPath, 'Sea'))\n",
    "            elif 'street' in dirPath:\n",
    "                for fileName in lF:\n",
    "                    shutil.move(os.path.join(dirPath, fileName), os.path.join(dstPath, 'Street'))\n",
    "            else:\n",
    "                for fileName in lF:\n",
    "                    shutil.move(os.path.join(dirPath, fileName), dstPath)\n",
    "    \n",
    "    shutil.rmtree(os.path.join(dataSetPath, 'TMP'))\n",
    "\n",
    "    hFile = open(os.path.join(dataSetPath, '.processed'), 'w')\n",
    "    hFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "\n",
    "dsTrain    = torchvision.datasets.ImageFolder(os.path.join(DATA_FOLDER_PATH, DATA_SET_FOLDER_NAME, 'Train'), transform = torchvision.transforms.ToTensor())\n",
    "dsVal      = torchvision.datasets.ImageFolder(os.path.join(DATA_FOLDER_PATH, DATA_SET_FOLDER_NAME, 'Validation'), transform = torchvision.transforms.ToTensor())\n",
    "dsTest     = TestDataSet(os.path.join(DATA_FOLDER_PATH, DATA_SET_FOLDER_NAME, 'Test'), transform = torchvision.transforms.ToTensor()) #<! Does not return a label\n",
    "lClass     = dsTrain.classes\n",
    "numSamples = len(dsTrain)\n",
    "\n",
    "print(f'The data set number of samples (Train): {numSamples}')\n",
    "print(f'The data set number of samples (Validation): {len(dsVal)}')\n",
    "print(f'The data set number of samples (Test): {len(dsTest)}')\n",
    "print(f'The unique values of the labels: {np.unique(lClass)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> The dataset is indexible (Subscriptable). It returns a tuple of the features and the label.\n",
    "* <font color='brown'>(**#**)</font> While data is arranged as `H x W x C` the transformer, when accessing the data, will convert it into `C x H x W`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Element of the Data Set\n",
    "\n",
    "mX, valY = dsTrain[0]\n",
    "\n",
    "print(f'The features shape: {mX.shape}')\n",
    "print(f'The label value: {valY}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Data\n",
    "\n",
    "vIdx = np.random.choice(numSamples, size = 9)\n",
    "hF, vHa = plt.subplots(nrows = 3, ncols = 3, figsize = (10, 10))\n",
    "vHa = vHa.flat\n",
    "\n",
    "for ii, hA in enumerate(vHa):\n",
    "    hA.imshow(dsTrain[vIdx[ii]][0].permute((1, 2, 0)).numpy())\n",
    "    hA.tick_params(axis = 'both', left = False, top = False, right = False, bottom = False, \n",
    "                   labelleft = False, labeltop = False, labelright = False, labelbottom = False)\n",
    "    hA.grid(False)\n",
    "    hA.set_title(f'Index = {vIdx[ii]}, Label = {L_CLASSES[dsTrain[vIdx[ii]][1]]}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> If data is converted into _grayscale_, how would it effect the performance of the classifier? Explain.  \n",
    "  You may assume the conversion is done using the mean value of the RGB pixel.\n",
    "* <font color='red'>(**?**)</font> The image at index `6737` of the training data set (`IntelImgCls\\\\Train\\\\Glacier\\\\8987.jpg`) is an example of wrongfully labeled image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Process Data\n",
    "\n",
    "This section:\n",
    "\n",
    " * Normalizes the data in a predefined manner.\n",
    " * Takes a sub set of the data.\n",
    "\n",
    "Since the model is \"borrowed\" by _Transfer Learning_ one must:\n",
    "\n",
    "1. Use the statistics from the original training set.\n",
    "1. Adapt the input dimensions to match the original training set.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The values in training are specified in documentation.  \n",
    "  As an example, see [`ResNet50` Weights](https://pytorch.org/vision/stable/models/generated/torchvision.models.resnet50.html#torchvision.models.ResNet50_Weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Standardization Parameters\n",
    "# ImageNet statistics\n",
    "vMean = np.array([0.485, 0.456, 0.406])\n",
    "vStd  = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "print('µ =', vMean)\n",
    "print('σ =', vStd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Image Dimensions (Run Only Once)\n",
    "# Verifies all images have the same size: 3 x 150 x 150.\n",
    "\n",
    "# for ii in range(len(dsTrain)):\n",
    "#     xx, yy = dsTrain[ii]\n",
    "#     imgH = xx.shape[1]\n",
    "#     imgW = xx.shape[2]\n",
    "#     if ((imgH != 150) or (imgW != 150)):\n",
    "#         print(f'The image {dsTrain.imgs[ii][0]} has incorrect size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Transforms\n",
    "# Using v2 Transforms.\n",
    "# Taking care of the different dimensions of some images.\n",
    "# Matching the input size of ImageNet.\n",
    "oDataTrnsTrain = TorchVisionTrns.Compose([\n",
    "    TorchVisionTrns.ToImage(),\n",
    "    TorchVisionTrns.ToDtype(torch.float32, scale = True),\n",
    "    TorchVisionTrns.Resize(224),\n",
    "    TorchVisionTrns.CenterCrop(224), #<! Ensures size is 150 (Pads if needed)\n",
    "    TorchVisionTrns.RandomHorizontalFlip(p = 0.5),\n",
    "    TorchVisionTrns.Normalize(mean = vMean, std = vStd),\n",
    "])\n",
    "oDataTrnsVal = TorchVisionTrns.Compose([\n",
    "    TorchVisionTrns.ToImage(),\n",
    "    TorchVisionTrns.ToDtype(torch.float32, scale = True),\n",
    "    TorchVisionTrns.Resize(224),\n",
    "    TorchVisionTrns.CenterCrop(224), #<! Ensures size is 150 (Pads if needed)\n",
    "    TorchVisionTrns.Normalize(mean = vMean, std = vStd),\n",
    "])\n",
    "\n",
    "# Using V1\n",
    "# oDataTrnsTrain = torchvision.transforms.Compose([\n",
    "#     torchvision.transforms.Resize(224),\n",
    "#     torchvision.transforms.CenterCrop(224),\n",
    "#     torchvision.transforms.RandomHorizontalFlip(0.5),\n",
    "#     torchvision.transforms.ToTensor(),\n",
    "#     torchvision.transforms.Normalize(mean = vMean, std = vStd),\n",
    "# ])\n",
    "\n",
    "# oDataTrnsVal = torchvision.transforms.Compose([\n",
    "#     torchvision.transforms.Resize(224),\n",
    "#     torchvision.transforms.CenterCrop(224),\n",
    "#     torchvision.transforms.ToTensor(),\n",
    "#     torchvision.transforms.Normalize(mean = vMean, std = vStd),\n",
    "# ])\n",
    "\n",
    "# Update the DS transformer\n",
    "dsTrain.transform   = oDataTrnsTrain\n",
    "dsVal.transform     = oDataTrnsVal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> What does `RandomHorizontalFlip` do? Why can it be used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Normalized\" Image\n",
    "\n",
    "mX, valY = dsTrain[5]\n",
    "\n",
    "hF, hA = plt.subplots()\n",
    "hImg = hA.imshow(np.transpose(mX, (1, 2, 0)))\n",
    "hF.colorbar(hImg);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> How can one get the original image from `mX`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loaders\n",
    "\n",
    "This section defines the data loaded.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader\n",
    "\n",
    "dlTrain = torch.utils.data.DataLoader(dsTrain, shuffle = True, batch_size = 1 * batchSize, num_workers = numWorkers, drop_last = True, persistent_workers = True)\n",
    "dlVal   = torch.utils.data.DataLoader(dsVal, shuffle = False, batch_size = 2 * batchSize, num_workers = numWorkers, persistent_workers = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='blue'>(**!**)</font> Plot the histogram of labels of the data. Is it balanced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate on the Loader\n",
    "# The first batch.\n",
    "tX, vY = next(iter(dlTrain)) #<! PyTorch Tensors\n",
    "\n",
    "print(f'The batch features dimensions: {tX.shape}')\n",
    "print(f'The batch labels dimensions: {vY.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Model\n",
    "\n",
    "This section loads the model.  \n",
    "The number of outputs is adjusted to match the number of classes in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading a Pre Defined Model\n",
    "oModelPreDef = GenResNetModel(trainedModel = False, numCls = len(L_CLASSES), resNetDepth = resNetDepth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='blue'>(**!**)</font> Go through `GenResNetModel()`'s code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Information - Pre Defined\n",
    "# Pay attention to the layers name.\n",
    "torchinfo.summary(oModelPreDef, tX.shape, col_names = ['kernel_size', 'output_size', 'num_params'], device = 'cpu', row_settings = ['depth', 'var_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Which layer should be adapted?\n",
    "* <font color='red'>(**?**)</font> Does the last (_Head_) dense layer includes a bias? Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Information - Pre Trained\n",
    "# Pay attention to the layers name.\n",
    "oModelPreTrn = GenResNetModel(trainedModel = True, numCls = len(L_CLASSES), resNetDepth = resNetDepth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Information\n",
    "# Pay attention to the variable name\n",
    "torchinfo.summary(oModelPreTrn, tX.shape, col_names = ['kernel_size', 'output_size', 'num_params'], device = 'cpu', row_settings = ['depth', 'var_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "This section trains the model.  \n",
    "It compares pre trained model with pre defined model using the same number of epochs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning Fine Tuning\n",
    "\n",
    "The training of the model on the new data is often called _fine tuning_ (See [Fine Tuning vs. Transfer Learning vs. Learning from Scratch](https://stats.stackexchange.com/questions/343763) for a discussion on the semantic).  \n",
    "The concept is training the new layers of the model with the new data while keeping most of the \"knowledge\" of the model from its original training.  \n",
    "The balance is done by the adaptation of the learning per layer with the extreme of zero learning rate for some layers (Freezing).  \n",
    "The most used combinations are:\n",
    "\n",
    " - Freeze Layers  \n",
    "   Freeze (Zero learning rate) the pre trained layers by disabling the gradient (`requires_grad`).\n",
    " - Smaller Learning Rate  \n",
    "   Set a smaller learning rate to the pre trained layers.\n",
    " - Fine Tuning  \n",
    "   Use small learning rate to the whole process.\n",
    "\n",
    "In some cases, the policy used is a combination of 2 (Freeze at the beginning / end, the release, etc..).\n",
    "\n",
    "* <font color='brown'>(**#**)</font> Freezing is also a regularization as its assists in preventing _over fitting_.\n",
    "* <font color='brown'>(**#**)</font> [PyTorch Optimizer - Per Parameter Learning Rate](https://pytorch.org/docs/stable/optim.html#per-parameter-options).\n",
    "* <font color='brown'>(**#**)</font> See [Dive into Deep Learning - Computer Vision - Fine Tuning](http://d2l.ai/chapter_computer-vision/fine-tuning.html).\n",
    "* <font color='brown'>(**#**)</font> Guide to Fine Tuning in PyTorch: [Part I](https://scribe.rip/8990194b71e), [Part II](https://scribe.rip/b0f8f447546b).\n",
    "* <font color='brown'>(**#**)</font> [How to Freeze Model Weights in PyTorch for Transfer Learning: Step by Step Tutorial](https://scribe.rip/a533a58051ef)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze Layers\n",
    "# Freezing the layers of the pre-trained model (The`fc` layer is not frozen).\n",
    "# Iterating over the net, see https://stackoverflow.com/questions/54203451\n",
    "\n",
    "for paramName, oPrm in oModelPreTrn.named_parameters():\n",
    "    if not ('fc' in paramName):\n",
    "        oPrm.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='blue'>(**!**)</font> Exclude _Batch Norm_ layers as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Device\n",
    "\n",
    "runDevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') #<! The 1st CUDA device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "\n",
    "lModel = [('Pre Defined Model', oModelPreDef), ('Pre Trained Model', oModelPreTrn)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Score Function\n",
    "\n",
    "hL = nn.CrossEntropyLoss()\n",
    "hS = MulticlassAccuracy(num_classes = len(lClass), average = 'micro')\n",
    "hL = hL.to(runDevice) #<! Not required!\n",
    "hS = hS.to(runDevice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> The averaging mode `macro` averages samples per class and average the result of each class.\n",
    "* <font color='brown'>(**#**)</font> The averaging mode `micro` averages all samples.\n",
    "* <font color='red'>(**?**)</font> Check results with `average = 'micro'`. Explain how `shuffle - False` in the validation data loader affects the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "\n",
    "dModelHist = {}\n",
    "\n",
    "for ii, (modelName, oModel) in enumerate(lModel):\n",
    "    print(f'Training with the {modelName} model')\n",
    "    oModel = oModel.to(runDevice) #<! Transfer model to device\n",
    "    oOpt = torch.optim.AdamW(oModel.parameters(), lr = 1e-4, betas = (0.9, 0.99), weight_decay = 2e-4) #<! Define optimizer\n",
    "    oSch = torch.optim.lr_scheduler.OneCycleLR(oOpt, max_lr = 2e-2, total_steps = numEpochs * len(dlTrain))\n",
    "    _, lTrainLoss, lTrainScore, lValLoss, lValScore, lLearnRate = TrainModelSch(oModel, dlTrain, dlVal, oOpt, oSch, numEpochs, hL, hS)\n",
    "    dModelHist[modelName] = lTrainLoss, lTrainScore, lValLoss, lValScore, lLearnRate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='green'>(**@**)</font> Add _TensorBoard_ based monitoring. You should use the `TBLogger` class.\n",
    "* <font color='red'>(**?**)</font> Compare run time and memory consumption during the training of the models. How can it be utilized?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training Phase\n",
    "\n",
    "hF, vHa = plt.subplots(nrows = 1, ncols = 3, figsize = (18, 5))\n",
    "vHa = np.ravel(vHa)\n",
    "\n",
    "for modelKey in dModelHist:\n",
    "    hA = vHa[0]\n",
    "    hA.plot(dModelHist[modelKey][0], lw = 2, label = f'Train {modelKey}')\n",
    "    hA.plot(dModelHist[modelKey][2], lw = 2, label = f'Validation {modelKey}')\n",
    "    hA.set_title('Cross Entropy Loss')\n",
    "    hA.set_xlabel('Epoch')\n",
    "    hA.set_ylabel('Loss')\n",
    "    hA.legend()\n",
    "\n",
    "    hA = vHa[1]\n",
    "    hA.plot(dModelHist[modelKey][1], lw = 2, label = f'Train {modelKey}')\n",
    "    hA.plot(dModelHist[modelKey][3], lw = 2, label = f'Validation {modelKey}')\n",
    "    hA.set_title('Accuracy Score')\n",
    "    hA.set_xlabel('Epoch')\n",
    "    hA.set_ylabel('Score')\n",
    "    hA.legend()\n",
    "\n",
    "    hA = vHa[2]\n",
    "    hA.plot(lLearnRate, lw = 2, label = f'{modelKey}')\n",
    "    hA.set_title('Learn Rate Scheduler')\n",
    "    hA.set_xlabel('Iteration')\n",
    "    hA.set_ylabel('Learn Rate')\n",
    "    hA.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='green'>(**@**)</font> Build the `Test` data loader (You may use `dsTest`) and exam the models on few samples.\n",
    "* <font color='green'>(**@**)</font> Redo the training with a different model.\n",
    "* <font color='red'>(**?**)</font> How would the results look like if only 1400 samples were available for training? Try it.\n",
    "* <font color='red'>(**?**)</font> Look at the [`Places365`](http://places.csail.mit.edu/) ([`Places365 v2`](http://places2.csail.mit.edu/)) data set.  \n",
    "  If the base model for transfer learning is trained on `Places365`, what effect will it have on the results?  \n",
    "  Think of the type of the task. You may try it with [`Release of Places365-CNNs`](https://github.com/CSAILVision/places365)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
