{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Fixel Algorithms](https://fixelalgorithms.co/images/CCExt.png)](https://fixelalgorithms.gitlab.io)\n",
    "\n",
    "# Deep Learning Methods\n",
    "\n",
    "## Deep Learning - Convolution Neural Network - Image Classification (Fashion MNIST) - Exercise\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 1.0.001 | 19/09/2025 | Royi Avital | Updated link of Google Colab                                       |\n",
    "| 1.0.000 | 27/04/2024 | Royi Avital | First version                                                      |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/DeepLearningMethods/2025_08/0004DeepLearningConv2DFashionMNISTSolution.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T09:30:06.492269Z",
     "start_time": "2022-02-02T09:30:06.220934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchinfo\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "import torchvision\n",
    "\n",
    "# Miscellaneous\n",
    "import os\n",
    "from platform import python_version\n",
    "import random\n",
    "\n",
    "# Typing\n",
    "from typing import Callable, Dict, Generator, List, Optional, Self, Set, Tuple, Union\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought.\n",
    "\n",
    "Code Notations:\n",
    "\n",
    "```python\n",
    "someVar    = 2; #<! Notation for a variable\n",
    "vVector    = np.random.rand(4) #<! Notation for 1D array\n",
    "mMatrix    = np.random.rand(4, 3) #<! Notation for 2D array\n",
    "tTensor    = np.random.rand(4, 3, 2, 3) #<! Notation for nD array (Tensor)\n",
    "tuTuple    = (1, 2, 3) #<! Notation for a tuple\n",
    "lList      = [1, 2, 3] #<! Notation for a list\n",
    "dDict      = {1: 3, 2: 2, 3: 1} #<! Notation for a dictionary\n",
    "oObj       = MyClass() #<! Notation for an object\n",
    "dfData     = pd.DataFrame() #<! Notation for a data frame\n",
    "dsData     = pd.Series() #<! Notation for a series\n",
    "hObj       = plt.Axes() #<! Notation for an object / handler / function handler\n",
    "```\n",
    "\n",
    "### Code Exercise\n",
    "\n",
    " - Single line fill\n",
    "\n",
    "```python\n",
    "valToFill = ???\n",
    "```\n",
    "\n",
    " - Multi Line to Fill (At least one)\n",
    "\n",
    "```python\n",
    "# You need to start writing\n",
    "?????\n",
    "```\n",
    "\n",
    " - Section to Fill\n",
    "\n",
    "```python\n",
    "#===========================Fill This===========================#\n",
    "# 1. Explanation about what to do.\n",
    "# !! Remarks to follow / take under consideration.\n",
    "mX = ???\n",
    "\n",
    "?????\n",
    "#===============================================================#\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# %matplotlib inline\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "# Matplotlib default color palette\n",
    "lMatPltLibclr = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "# sns.set_theme() #>! Apply SeaBorn theme\n",
    "\n",
    "runInGoogleColab = 'google.colab' in str(get_ipython())\n",
    "\n",
    "# Improve performance by benchmarking\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Reproducibility\n",
    "# torch.manual_seed(seedNum)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark     = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "FIG_SIZE_DEF    = (8, 8)\n",
    "ELM_SIZE_DEF    = 50\n",
    "CLASS_COLOR     = ('b', 'r')\n",
    "EDGE_COLOR      = 'k'\n",
    "MARKER_SIZE_DEF = 10\n",
    "LINE_WIDTH_DEF  = 2\n",
    "\n",
    "D_CLASSES_FASHION_MNIST = {0: 'T-Shirt', 1: 'Trouser', 2: 'Pullover', 3: 'Dress', 4: 'Coat', 5: 'Sandal', 6: 'Shirt', 7: 'Sneaker', 8: 'Bag', 9: 'Boots'}\n",
    "L_CLASSES_FASHION_MNIST = ['T-Shirt', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Boots']\n",
    "\n",
    "T_IMG_SIZE_MNIST = (28, 28)\n",
    "\n",
    "DATA_FOLDER_PATH    = 'Data'\n",
    "TENSOR_BOARD_BASE   = 'TB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Auxiliary Modules for Google Colab\n",
    "if runInGoogleColab:\n",
    "    import os\n",
    "    for fileName in ('DataManipulation.py', 'DataVisualization.py', 'DeepLearningBlocks.py', 'DeepLearningPyTorch.py'):\n",
    "        if os.path.exists(fileName):\n",
    "            continue\n",
    "        os.system(f'wget https://raw.githubusercontent.com/FixelAlgorithmsTeam/FixelCourses/master/DeepLearningMethods/2025_08/{fileName}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courses Packages\n",
    "\n",
    "from DataVisualization import PlotLabelsHistogram, PlotMnistImages\n",
    "from DeepLearningPyTorch import TrainModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Auxiliary Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fashion MNIST Classification with 2D Convolution Net\n",
    "\n",
    "This notebook shows the use of [`Conv2d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) layer.  \n",
    "The 2D Convolution layer means there are 2 _degrees of freedom_ for the kernel movement.\n",
    "\n",
    "This notebook applies image classification (Single label per image) on the [Fashion MNIST Data Set](https://github.com/zalandoresearch/fashion-mnist).  \n",
    "\n",
    "The notebook presents:\n",
    "\n",
    " * Building a 2D convolution based model which fits _Computer Vision_ tasks.\n",
    " * Use of [`torch.nn.Conv2d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html).\n",
    " * Use of [`torch.nn.BatchNorm2d`](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d).\n",
    " * Evaluating several models using TensorBoard.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# Data\n",
    "numSamplesTrain = 60_000\n",
    "numSamplesTest  = 10_000\n",
    "\n",
    "# Model\n",
    "dropP = 0.2 #<! Dropout Layer\n",
    "\n",
    "# Training\n",
    "batchSize   = 256\n",
    "numWork     = 2 #<! Number of workers\n",
    "nEpochs     = 30\n",
    "\n",
    "# Visualization\n",
    "numImg = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate / Load Data\n",
    "\n",
    "Load the [Fashion MNIST Data Set](https://github.com/zalandoresearch/fashion-mnist).  \n",
    "\n",
    "The _Fashion MNIST Data Set_ is considerably more challenging than the original MNIST though it is still no match to Deep Learning models.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The data set is available at [OpenML - Fashion MNIST](https://www.openml.org/search?type=data&id=40996).  \n",
    "  Yet it is not separated into the original _test_ and _train_ sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "\n",
    "mX, vY = fetch_openml('Fashion-MNIST', version = 1, return_X_y = True, as_frame = False, parser = 'auto')\n",
    "vY = vY.astype(np.int_) #<! The labels are strings, convert to integer\n",
    "\n",
    "print(f'The features data shape: {mX.shape}')\n",
    "print(f'The labels data shape: {vY.shape}')\n",
    "print(f'The unique values of the labels: {np.unique(vY)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> The images are grayscale with size `28x28`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre Process Data\n",
    "\n",
    "mX = mX / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Does the scaling affects the standardization (Zero mean, Unit variance) process?\n",
    "* <font color='red'>(**?**)</font> Would it be better to keep data as `np.uint8`? If so, what would be needed to change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Data\n",
    "\n",
    "hF = PlotMnistImages(mX, vY, numImg)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of Labels\n",
    "\n",
    "hA = PlotLabelsHistogram(vY, lClass = L_CLASSES_FASHION_MNIST)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "\n",
    "numClass = len(np.unique(vY))\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Split the data into train and test (Validation) data sets (NumPy arrays).\n",
    "# 2. Use stratified split.\n",
    "# !! The output should be: `mXTrain`, `mXTest`, `vYTrain`, `vYTest`.\n",
    "mXTrain, mXTest, vYTrain, vYTest = train_test_split(mX, vY, test_size = numSamplesTest, train_size = numSamplesTrain, shuffle = True, stratify = vY)\n",
    "#===============================================================#\n",
    "\n",
    "print(f'The training features data shape: {mXTrain.shape}')\n",
    "print(f'The training labels data shape: {vYTrain.shape}')\n",
    "print(f'The test features data shape: {mXTest.shape}')\n",
    "print(f'The test labels data shape: {vYTest.shape}')\n",
    "print(f'The unique values of the labels: {np.unique(vY)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch Datasets\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Convert the arrays to the 2D shape as needed.\n",
    "# 2. Generate Torch data sets from the NumPy arrays.\n",
    "# !! The output should be: `dsTrain`, `dsTest`.\n",
    "# !! Verify the number of channels is well defined.\n",
    "# !! The `T_IMG_SIZE_MNIST` tuple might be useful.\n",
    "# !! The `torch.utils.data.TensorDataset` class might be useful.\n",
    "# !! Pay attention to the type of the data as tensors.\n",
    "dsTrain = torch.utils.data.TensorDataset(torch.tensor(np.reshape(mXTrain, (numSamplesTrain, 1, *T_IMG_SIZE_MNIST)), dtype = torch.float32), torch.tensor(vYTrain, dtype = torch.long))\n",
    "dsTest  = torch.utils.data.TensorDataset(torch.tensor(np.reshape(mXTest, (numSamplesTest, 1, *T_IMG_SIZE_MNIST)), dtype = torch.float32), torch.tensor(vYTest, dtype = torch.long))\n",
    "#===============================================================#\n",
    "\n",
    "print(f'The training data set data shape: {(len(dsTrain), *dsTrain.tensors[0].shape[1:])}')\n",
    "print(f'The test data set data shape: {(len(dsTest), *dsTrain.tensors[0].shape[1:])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Process Data\n",
    "\n",
    "This section normalizes the data to have zero mean and unit variance per **channel**.  \n",
    "It is required to calculate:\n",
    "\n",
    " * The average pixel value per channel.\n",
    " * The standard deviation per channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Standardization Parameters\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Calculate the mean per channel.\n",
    "# 2. Calculate the standard deviation per channel.\n",
    "µ = torch.mean(dsTrain.tensors[0])\n",
    "σ = torch.std(dsTrain.tensors[0])\n",
    "#===============================================================#\n",
    "\n",
    "print('µ =', µ)\n",
    "print('σ =', σ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Transformer\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Define a transformer which normalizes the data.\n",
    "# 2. Update the `transform` object in `dsTrain` and `dsTest`.\n",
    "oDataTrns = torchvision.transforms.Compose([  #<! Chaining transformations\n",
    "    torchvision.transforms.ToTensor(),        #<! Convert to Tensor (C x H x W), Normalizes into [0, 1] (https://pytorch.org/vision/main/generated/torchvision.transforms.ToTensor.html)\n",
    "    torchvision.transforms.Normalize(µ, σ),   #<! Normalizes the Data (https://pytorch.org/vision/main/generated/torchvision.transforms.Normalize.html)\n",
    "    ])\n",
    "\n",
    "# Update the DS transformer\n",
    "dsTrain.transform = oDataTrns\n",
    "dsTest.transform  = oDataTrns\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Normalized\" Image\n",
    "\n",
    "mX, valY = dsTrain[5]\n",
    "\n",
    "hF, hA = plt.subplots()\n",
    "hA.imshow(np.transpose(mX, (1, 2, 0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loaders\n",
    "\n",
    "The dataloader is the functionality which loads the data into memory in batches.  \n",
    "Its challenge is to bring data fast enough so the Hard Disk is not the training bottleneck.  \n",
    "In order to achieve that, Multi Threading / Multi Process is used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Create the train data loader.\n",
    "# 2. Create the test data loader.\n",
    "# !! Think about the values of `shuffle` and `batch_size` for the train / test.\n",
    "dlTrain  = torch.utils.data.DataLoader(dsTrain, shuffle = True, batch_size = 1 * batchSize, num_workers = numWork, drop_last = True, persistent_workers = True)\n",
    "dlTest   = torch.utils.data.DataLoader(dsTest, shuffle = False, batch_size = 2 * batchSize, num_workers = numWork, persistent_workers = True)\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate on the Loader\n",
    "# The first batch.\n",
    "tX, vY = next(iter(dlTrain)) #<! PyTorch Tensors\n",
    "\n",
    "print(f'The batch features dimensions: {tX.shape}')\n",
    "print(f'The batch labels dimensions: {vY.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model\n",
    "\n",
    "This section build 3 different models to be evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Container\n",
    "lModels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "# Defining a sequential model.\n",
    "\n",
    "oModel = nn.Sequential(\n",
    "#===========================Fill This===========================#\n",
    "# 1. Create the 1st model.\n",
    "# 2. Use 3 layers.\n",
    "# !! You may use different kernel size, dropout probability, max pooling, etc...\n",
    "\n",
    "    nn.Identity(),\n",
    "    \n",
    "    nn.Conv2d(in_channels = 1, out_channels = 30, kernel_size = 3, bias = False),\n",
    "    nn.BatchNorm2d(num_features = 30),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout2d(p = dropP),\n",
    "    \n",
    "    nn.Conv2d(in_channels = 30, out_channels = 60, kernel_size = 3, bias = False),\n",
    "    nn.MaxPool2d(kernel_size = 2),\n",
    "    nn.BatchNorm2d(num_features = 60),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout2d(p = dropP),\n",
    "            \n",
    "    nn.Conv2d(in_channels = 60,  out_channels = 120, kernel_size = 3, bias = False),\n",
    "    nn.MaxPool2d(kernel_size = 2),\n",
    "    nn.BatchNorm2d(num_features = 120),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout2d(p = dropP),\n",
    "    \n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(120, len(L_CLASSES_FASHION_MNIST)),\n",
    "#===============================================================#\n",
    ")\n",
    "\n",
    "print(torchinfo.summary(oModel, tX.shape, col_names = ['kernel_size', 'output_size', 'num_params'], device = 'cpu')) #<! Added `kernel_size`\n",
    "\n",
    "# Append Model\n",
    "lModels.append(oModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "# Defining a sequential model.\n",
    "\n",
    "oModel = nn.Sequential(\n",
    "#===========================Fill This===========================#\n",
    "# 1. Create the 2nd model.\n",
    "# 2. Use 3 layers.\n",
    "# !! You may use different kernel size, dropout probability, max pooling, etc...\n",
    "\n",
    "    nn.Identity(),\n",
    "    \n",
    "    nn.Conv2d(in_channels = 1, out_channels = 30, kernel_size = 5, bias = False),\n",
    "    nn.BatchNorm2d(num_features = 30),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout2d(p = dropP),\n",
    "    \n",
    "    nn.Conv2d(in_channels = 30, out_channels = 60, kernel_size = 5, bias = False),\n",
    "    nn.BatchNorm2d(num_features = 60),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout2d(p = dropP),\n",
    "            \n",
    "    nn.Conv2d(in_channels = 60,  out_channels = 120, kernel_size = 5, bias = False),\n",
    "    nn.BatchNorm2d(num_features = 120),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout2d(p = dropP),\n",
    "    \n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(120, len(L_CLASSES_FASHION_MNIST)),\n",
    "#===============================================================#\n",
    ")\n",
    "\n",
    "print(torchinfo.summary(oModel, tX.shape, col_names = ['kernel_size', 'output_size', 'num_params'], device = 'cpu')) #<! Added `kernel_size`\n",
    "\n",
    "# Append Model\n",
    "lModels.append(oModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "# Defining a sequential model.\n",
    "\n",
    "oModel = nn.Sequential(\n",
    "#===========================Fill This===========================#\n",
    "# 1. Create the 3rd model.\n",
    "# 2. Use 3 layers.\n",
    "# !! You may use different kernel size, dropout probability, max pooling, etc...\n",
    "\n",
    "    nn.Identity(),\n",
    "    \n",
    "    nn.Conv2d(in_channels = 1, out_channels = 30, kernel_size = 7, bias = False),\n",
    "    nn.MaxPool2d(kernel_size = 2),\n",
    "    nn.BatchNorm2d(num_features = 30),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout2d(p = dropP),\n",
    "    \n",
    "    nn.Conv2d(in_channels = 30, out_channels = 60, kernel_size = 5, bias = False),\n",
    "    nn.MaxPool2d(kernel_size = 2),\n",
    "    nn.BatchNorm2d(num_features = 60),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout2d(p = dropP),\n",
    "            \n",
    "    nn.Conv2d(in_channels = 60,  out_channels = 120, kernel_size = 3, bias = False),\n",
    "    nn.BatchNorm2d(num_features = 120),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout2d(p = dropP),\n",
    "    \n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(120, len(L_CLASSES_FASHION_MNIST)),\n",
    "#===============================================================#\n",
    ")\n",
    "\n",
    "print(torchinfo.summary(oModel, tX.shape, col_names = ['kernel_size', 'output_size', 'num_params'], device = 'cpu')) #<! Added `kernel_size`\n",
    "\n",
    "# Append Model\n",
    "lModels.append(oModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> Guideline: The smaller the image gets, the deeper it is (More channels).   \n",
    "  The intuition, the beginning of the model learns low level features (Small number), deeper learns combinations of features (Larger number)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU Availability\n",
    "\n",
    "runDevice   = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') #<! The 1st CUDA device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Loss & Score\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Define loss function\n",
    "# 2. Define score function.\n",
    "hL = nn.CrossEntropyLoss()\n",
    "hS = MulticlassAccuracy(num_classes = len(L_CLASSES_FASHION_MNIST), average = 'micro')\n",
    "hL = hL.to(runDevice) #<! Not required!\n",
    "hS = hS.to(runDevice)\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Build a loop to evaluate all models.\n",
    "# 2. Define a TensorBoard Writer per model to keep its score.\n",
    "# !! You may use `TrainModel()`.\n",
    "\n",
    "for ii, oModel in enumerate(lModels):\n",
    "    # Hyper Parameter Loop\n",
    "    oTBWriter = SummaryWriter(log_dir = os.path.join(TENSOR_BOARD_BASE, f'Model{(ii + 1):03d}'))\n",
    "    oModel = oModel.to(runDevice) #<! Transfer model to device\n",
    "    oOpt = torch.optim.AdamW(oModel.parameters(), lr = 6e-4, betas = (0.9, 0.99), weight_decay = 1e-3) #<! Define optimizer\n",
    "    oRunModel, lTrainLoss, lTrainScore, lValLoss, lValScore, _ = TrainModel(oModel, dlTrain, dlTest, oOpt, nEpochs, hL, hS, oTBWriter = oTBWriter)\n",
    "    oTBWriter.close()\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> Run `tensorboard --logdir=TB` from the Jupyter notebook path.\n",
    "* <font color='green'>(**@**)</font> Optimize the model search to get above 92% accuracy in validation set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
