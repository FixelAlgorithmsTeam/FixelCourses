{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Fixel Algorithms](https://i.imgur.com/AqKHVZ0.png)](https://fixelalgorithms.gitlab.io)\n",
    "\n",
    "# AI Program\n",
    "\n",
    "## Machine Learning - Supervised Learning - Classification - Logistic Regression\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 1.0.002 | 16/11/2025 | Royi Avital | Added notes on Logistic Regression vs. Linear SVM                  |\n",
    "| 1.0.001 | 31/03/2024 | Royi Avital | More remarks on the derivation                                     |\n",
    "| 1.0.000 | 20/03/2024 | Royi Avital | First version                                                      |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/AIProgram/2024_02/0041LogisticRegressionSolution.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T09:30:06.492269Z",
     "start_time": "2022-02-02T09:30:06.220934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Miscellaneous\n",
    "from platform import python_version\n",
    "import random\n",
    "\n",
    "# Typing\n",
    "from typing import Callable, Dict, List, Optional, Set, Tuple, Union\n",
    "\n",
    "# Visualization\n",
    "from matplotlib.colors import LogNorm, Normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought.\n",
    "\n",
    "Code Notations:\n",
    "\n",
    "```python\n",
    "someVar    = 2; #<! Notation for a variable\n",
    "vVector    = np.random.rand(4) #<! Notation for 1D array\n",
    "mMatrix    = np.random.rand(4, 3) #<! Notation for 2D array\n",
    "tTensor    = np.random.rand(4, 3, 2, 3) #<! Notation for nD array (Tensor)\n",
    "tuTuple    = (1, 2, 3) #<! Notation for a tuple\n",
    "lList      = [1, 2, 3] #<! Notation for a list\n",
    "dDict      = {1: 3, 2: 2, 3: 1} #<! Notation for a dictionary\n",
    "oObj       = MyClass() #<! Notation for an object\n",
    "dfData     = pd.DataFrame() #<! Notation for a data frame\n",
    "dsData     = pd.Series() #<! Notation for a series\n",
    "hObj       = plt.Axes() #<! Notation for an object / handler / function handler\n",
    "```\n",
    "\n",
    "### Code Exercise\n",
    "\n",
    " - Single line fill\n",
    "\n",
    "```python\n",
    "valToFill = ???\n",
    "```\n",
    "\n",
    " - Multi Line to Fill (At least one)\n",
    "\n",
    "```python\n",
    "# You need to start writing\n",
    "?????\n",
    "```\n",
    "\n",
    " - Section to Fill\n",
    "\n",
    "```python\n",
    "#===========================Fill This===========================#\n",
    "# 1. Explanation about what to do.\n",
    "# !! Remarks to follow / take under consideration.\n",
    "mX = ???\n",
    "\n",
    "?????\n",
    "#===============================================================#\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# %matplotlib inline\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "# Matplotlib default color palette\n",
    "lMatPltLibclr = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "# sns.set_theme() #>! Apply SeaBorn theme\n",
    "\n",
    "runInGoogleColab = 'google.colab' in str(get_ipython())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "FIG_SIZE_DEF    = (8, 8)\n",
    "ELM_SIZE_DEF    = 50\n",
    "CLASS_COLOR     = ('b', 'r')\n",
    "EDGE_COLOR      = 'k'\n",
    "MARKER_SIZE_DEF = 10\n",
    "LINE_WIDTH_DEF  = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courses Packages\n",
    "\n",
    "from DataVisualization import PlotLabelsHistogram, PlotMnistImages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Auxiliary Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "In this exercise we'll use the Logistic Regression model as a classifier.  \n",
    "The SciKit Learn library implement it with the [`LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) class.\n",
    "\n",
    "I this exercise we'll do the following:\n",
    "\n",
    "1. Load the [MNIST Data set](https://en.wikipedia.org/wiki/MNIST_database) using `fetch_openml()`.\n",
    "2. Train a Logistic Regression model on the training data.\n",
    "3. Optimize the parameters: `penalty` and `C` by the `roc_auc` score.\n",
    "4. Interpret the model using its weights.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The model is a linear model, hence its weights are easy to interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy Loss vs. MSE for Probabilistic Predictions\n",
    "\n",
    "The Logistic Regression is based on the [Cross Entropy Loss](https://en.wikipedia.org/wiki/Cross-entropy) which measure similarity between distributions.  \n",
    "In the context of classification is measures the distance between 2 _discrete_ distributions.\n",
    "\n",
    "Consider the the true probabilities and 2 estimations of 6 categories data:\n",
    "\n",
    "$$ \\boldsymbol{y} = {\\left[ 0, 1, 0, 0, 0, 0 \\right]}^{T}, \\; \\hat{\\boldsymbol{y}}_{1} = {\\left[ 0.16, 0.2, 0.16, 0.16, 0.16, 0.16 \\right]}^{T}, \\; \\hat{\\boldsymbol{y}}_{2} = {\\left[ 0.5, 0.4, 0.1, 0.0, 0.0, 0.0 \\right]}^{T} $$\n",
    "\n",
    "One could use the [Mean Squared Error](https://en.wikipedia.org/wiki/Mean_squared_error) to measure the distance between the vectors (Called [Brier Score](https://en.wikipedia.org/wiki/Brier_score) in this context) as an alternative to the CE which will yield:\n",
    "\n",
    "$$ MSE \\left( \\boldsymbol{y}, \\hat{\\boldsymbol{y}}_{1} \\right) = 0.128, \\; MSE \\left( \\boldsymbol{y}, \\hat{\\boldsymbol{y}}_{2} \\right) = 0.103 $$\n",
    "\n",
    "Yet, in $\\hat{\\boldsymbol{y}}_{2}$ which has a lower error the most probable class is not the correct one while in $\\hat{\\boldsymbol{y}}_{1}$ it is.  \n",
    "The CE in contrast only \"cares\" about the error in the index of the _correct_ class and minimizes that.  \n",
    "Another advantage of the CE is being the [_Maximum Likelihood Estimator_](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation) which ensures some useful properties.\n",
    "\n",
    "Yet there are some empirical advantages to the MSE loss in this context as given by [Evaluation of Neural Architectures Trained with Square Loss vs Cross Entropy in Classification Tasks](https://arxiv.org/abs/2006.07322) which makes it a legitimate choice as well.\n",
    "\n",
    "See:\n",
    "\n",
    " * [Cross Entropy Loss vs. MSE for Multi Class Classification](https://stats.stackexchange.com/questions/573944).\n",
    " * [Disadvantages of Using a Regression Loss Function in Multi Class Classification](https://stats.stackexchange.com/questions/568238)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "numSamplesTrain = 1_500\n",
    "numSamplesVal   = 1_000 #<! Validation Samples for Hyper Parameters Tuning\n",
    "\n",
    "numImg = 3\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# Setting the Hyper Parameters Grid for Logistic Regression\n",
    "# 1. Set the options for the `penalty` parameter (Use: 'l1' and 'l2').\n",
    "# 2. Set the options for the `C` parameter (~25 values, According to computer speed).\n",
    "lPenalty    = ['l1', 'l2']\n",
    "lC          = list(np.linspace(0.001, 100, 30) / numSamplesTrain)\n",
    "#===============================================================#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate / Load Data\n",
    "\n",
    "Loading the _MNIST_ data set using SciKit Learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Data \n",
    "\n",
    "mX, vY = fetch_openml('mnist_784', version = 1, return_X_y = True, as_frame = False, parser = 'auto')\n",
    "vY = vY.astype(np.int_) #<! The labels are strings, convert to integer\n",
    "\n",
    "print(f'The features data shape: {mX.shape}')\n",
    "print(f'The labels data shape: {vY.shape}')\n",
    "print(f'The unique values of the labels: {np.unique(vY)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre Processing\n",
    "\n",
    "# The image is in the range {0, 1, ..., 255}\n",
    "# We scale it into [0, 1]\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Scale the values into the [0, 1] range.\n",
    "mX = mX / 255.0\n",
    "\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Validation Split\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Split the data such that the Train Data has `numSamplesTrain`.\n",
    "# 2. Split the data such that the Validation Data has `numSamplesValidation`.\n",
    "# 3. The distribution of the classes must match the original data.\n",
    "\n",
    "numClass = len(np.unique(vY))\n",
    "mXTrain, mXVal, vYTrain, vYVal = train_test_split(mX, vY, test_size = numSamplesVal, train_size = numSamplesTrain, shuffle = True, stratify = vY)\n",
    "\n",
    "#===============================================================#\n",
    "\n",
    "print(f'The training features data shape: {mXTrain.shape}')\n",
    "print(f'The training labels data shape: {vYTrain.shape}')\n",
    "print(f'The validation features data shape: {mXVal.shape}')\n",
    "print(f'The validation labels data shape: {vYVal.shape}')\n",
    "print(f'The unique values of the labels: {np.unique(vY)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Data\n",
    "\n",
    "hF = PlotMnistImages(mX, vY, numImg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Labels\n",
    "\n",
    "When dealing with classification, it is important to know the balance between the labels within the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Labels\n",
    "\n",
    "hA = PlotLabelsHistogram(vY);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "The _logistic regression_ can be derived in many forms.  \n",
    "We'll illustrate 2 of them.\n",
    "\n",
    "### Derivation 001\n",
    "\n",
    "One intuitive path is saying that we're after calculating the probability: $p \\left( y = 1 \\mid \\boldsymbol{x} \\right)$.  \n",
    "Since it is a probability function is must obey some rules. The first one being in the range $\\left[ 0, 1 \\right]$.  \n",
    "\n",
    "A function which maps $\\left( -\\infty, \\infty \\right) \\to \\left[0, 1 \\right]$ is the [Sigmoid Function](https://en.wikipedia.org/wiki/Sigmoid_function): $\\sigma \\left( z \\right) = \\frac{1}{1 + \\exp \\left( z \\right)}$.\n",
    "\n",
    "So now we can say that: $p \\left( y = 1 \\mid \\boldsymbol{x} \\right) = \\sigma \\left( {z}_{i} \\right)$.  \n",
    "Now the problem is modeling the parameter ${z}_{i}$. In which in a linear case will be modeled as ${z}_{i} = \\boldsymbol{w}_{i}^{T} \\boldsymbol{x} + {b}_{i}$.  \n",
    "Namely by a linear model, which in the choice of the Log of Sigmoid Function as the objective means the objective function is Convex in $\\boldsymbol{w}_{i}$ and $b$:\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/c/cb/Exam_pass_logistic_curve.svg/640px-Exam_pass_logistic_curve.svg.png)\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The function is convex due to the way it is formulated using the negative logarithm.  \n",
    "  See [Logistic Regression - Prove the Convexity of the Loss Function](https://math.stackexchange.com/questions/1582452).\n",
    "* <font color='brown'>(**#**)</font> It is guaranteed to converge only if the problem is not linear separable.  \n",
    "  In the separable scenario there is an incentive for $\\left\\| \\boldsymbol{w} \\right\\|$ to get larger to emphasize between the 2 classes.  \n",
    "  If $\\boldsymbol{w}$ is doubled the odds of elements in class 1 gets bigger log odds and elements in class 0 get smaller log odds.  \n",
    "  In this case the parameter $\\boldsymbol{w}$ won't converge, but the direction will converge. In particular, $\\frac{\\boldsymbol{w}}{\\left\\| \\boldsymbol{w} \\right\\|}$ converges.\n",
    "\n",
    "If we expand the above to multi class we'll get the [Softmax Function](https://en.wikipedia.org/wiki/Softmax_function) as in slides.\n",
    "\n",
    "### Derivation 002\n",
    "\n",
    "By _Bayes Theorem_ for the $L$ classes model:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p \\left( y = {L}_{i} \\mid \\boldsymbol{x} \\right) & = \\frac{ p \\left( \\boldsymbol{x} \\mid y = {L}_{i} \\right) p \\left( y = {L}_{i} \\right) }{ p \\left( \\boldsymbol{x} \\right) } && \\text{} \\\\\n",
    "& = \\frac{ p \\left( \\boldsymbol{x} \\mid y = {L}_{i} \\right) p \\left( y = {L}_{i} \\right) }{ \\sum_{j = 1}^{L} p \\left( \\boldsymbol{x} \\mid y = {L}_{j} \\right) p \\left( y = {L}_{j} \\right) } && \\text{Expending by law total probability} \\\\\n",
    "& = \\frac{ p \\left( \\boldsymbol{x} \\mid y = {L}_{i} \\right) p \\left( y = {L}_{i} \\right) }{ p \\left( \\boldsymbol{x} \\mid y = {L}_{i} \\right) p \\left( y = {L}_{i} \\right) + p \\left( \\boldsymbol{x} \\mid y \\neq {L}_{i} \\right) p \\left( y \\neq {L}_{i} \\right) } && \\text{Expending by law total probability} \\\\\n",
    "& = \\frac{ 1 }{ 1 + \\frac{ p \\left( \\boldsymbol{x} \\mid y \\neq {L}_{i} \\right) p \\left( y \\neq {L}_{i} \\right)}{p \\left( \\boldsymbol{x} \\mid y = {L}_{i} \\right) p \\left( y = {L}_{i} \\right)} } && \\text{Dividing by $p \\left( \\boldsymbol{x} \\mid y = {L}_{i} \\right) p \\left( y = {L}_{i} \\right)$} \\\\\n",
    "& = \\frac{ 1 }{ 1 + {e}^{\\log \\frac{ p \\left( \\boldsymbol{x} \\mid y \\neq {L}_{i} \\right) p \\left( y \\neq {L}_{i} \\right)}{p \\left( \\boldsymbol{x} \\mid y = {L}_{i} \\right) p \\left( y = {L}_{i} \\right)}} } && \\text{for $x \\in \\left[ 0, \\infty \\right) \\Rightarrow x = \\exp \\log x $} \\\\\n",
    "& = \\frac{ 1 }{ 1 + {e}^{-\\log \\frac{ p \\left( \\boldsymbol{x} \\mid y = {L}_{i} \\right) p \\left( y = {L}_{i} \\right) }{ p \\left( \\boldsymbol{x} \\mid y \\neq {L}_{i} \\right) p \\left( y \\neq {L}_{i} \\right) }} } && \\text{$\\log x = - \\log \\frac{1}{x}$} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Now, if we model the log of likelihood ratio of the ${L}_{i}$ label with a linear model:\n",
    "\n",
    "$$ \\log \\frac{ p \\left( \\boldsymbol{x} \\mid y = {L}_{i} \\right) p \\left( y = {L}_{i} \\right) }{ p \\left( \\boldsymbol{x} \\mid y \\neq {L}_{i} \\right) p \\left( y \\neq {L}_{i} \\right) } = \\boldsymbol{w}_{i}^{T} \\boldsymbol{x} + {b}_{i} $$\n",
    "\n",
    "So we get:\n",
    "\n",
    "$$ p \\left( y = {L}_{i} \\mid \\boldsymbol{x} \\right) = \\frac{1}{ 1 + {e}^{- \\left( \\boldsymbol{w}_{i}^{T} \\boldsymbol{x} + {b}_{i} \\right)} } $$\n",
    "\n",
    "Yet, since $1 = {e}^{- \\log \\frac{p \\left( \\boldsymbol{x} \\mid y = {L}_{i} \\right) p \\left( y = {L}_{i} \\right)}{p \\left( \\boldsymbol{x} \\mid y = {L}_{i} \\right) p \\left( y = {L}_{i} \\right)}}$ the above can be written as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p \\left( y = {L}_{i} \\mid \\boldsymbol{x} \\right) = \\frac{1}{ 1 + {e}^{- \\left( \\boldsymbol{w}_{i}^{T} \\boldsymbol{x} + {b}_{i} \\right)} }\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Derivation 003\n",
    "\n",
    "By _Bayes Theorem_ for the $L$ classes model:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p \\left( y = {L}_{i} \\mid \\boldsymbol{x} \\right) & = \\frac{ p \\left( \\boldsymbol{x} \\mid y = {L}_{i} \\right) p \\left( y = {L}_{i} \\right) }{ p \\left( \\boldsymbol{x} \\right) } && \\text{} \\\\\n",
    "& = \\frac{ p \\left( \\boldsymbol{x} \\mid y = {L}_{i} \\right) p \\left( y = {L}_{i} \\right) }{ \\sum_{j = 1}^{L} p \\left( \\boldsymbol{x} \\mid y = {L}_{j} \\right) p \\left( y = {L}_{j} \\right) } && \\text{Expending by law total probability} \\\\\n",
    "& = \\frac{ p \\left( \\boldsymbol{x} \\mid y = {L}_{i} \\right) p \\left( y = {L}_{i} \\right) }{ p \\left( \\boldsymbol{x} \\mid y = {L}_{k} \\right) p \\left( y = {L}_{k} \\right) + \\sum_{j \\neq k} p \\left( \\boldsymbol{x} \\mid y = {L}_{j} \\right) p \\left( y = {L}_{j} \\right) } && \\text{} \\\\\n",
    "& = \\frac{ \\frac{p \\left( \\boldsymbol{x} \\mid y = {L}_{i} \\right) p \\left( y = {L}_{i} \\right)}{p \\left( \\boldsymbol{x} \\mid y = {L}_{k} \\right) p \\left( y = {L}_{k} \\right)} }{ 1 + \\sum_{j \\neq k} \\frac{p \\left( \\boldsymbol{x} \\mid y = {L}_{j} \\right) p \\left( y = {L}_{j} \\right)}{p \\left( \\boldsymbol{x} \\mid y = {L}_{k} \\right) p \\left( y = {L}_{k} \\right)} } && \\text{Dividing by $p \\left( \\boldsymbol{x} \\mid y = {L}_{k} \\right) p \\left( y = {L}_{k} \\right)$} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "As in above, we may model the Log Likelihood Ratio by a linear function of $\\boldsymbol{x}$ then we'll get:\n",
    "\n",
    "$$ p \\left( y = {L}_{i} \\mid \\boldsymbol{x} \\right) = \\frac{ \\exp{\\left( \\boldsymbol{w}_{i}^{T} \\boldsymbol{x} \\right)} }{ 1 + \\sum_{j \\neq k} \\exp{\\left( \\boldsymbol{w}_{j}^{T} \\boldsymbol{x} \\right)}} $$\n",
    "\n",
    "Since $1 = \\exp{ \\left( \\log{ \\frac{p \\left( \\boldsymbol{x} \\mid y = {L}_{k} \\right) p \\left( y = {L}_{k} \\right)}{p \\left( \\boldsymbol{x} \\mid y = {L}_{k} \\right) p \\left( y = {L}_{k} \\right)} } \\right)}$ we can write:\n",
    "\n",
    "$$ p \\left( y = {L}_{i} \\mid \\boldsymbol{x} \\right) = \\frac{ \\exp{\\left( \\boldsymbol{w}_{i}^{T} \\boldsymbol{x} \\right)} }{ \\sum_{j} \\exp{\\left( \\boldsymbol{w}_{j}^{T} \\boldsymbol{x} \\right)}} $$\n",
    "\n",
    "### Derivation 004\n",
    "\n",
    "Given $L$ classes, we can chose a reference class: ${L}_{k}$. Then define the linear model of the log likelihood ratio compared to it:\n",
    "\n",
    "$$ \\log{ \\left( \\frac{ p \\left( y = {L}_{i} \\mid \\boldsymbol{x} \\right) }{ p \\left( {y} = {L}_{k} \\mid \\boldsymbol{x} \\right) } \\right) } = \\boldsymbol{w}_{i}^{T} \\boldsymbol{x} + {b}_{i} $$\n",
    "\n",
    "By definition $p \\left( y = {L}_{i} \\mid \\boldsymbol{x} \\right) = p \\left( y = {L}_{k} \\mid \\boldsymbol{x} \\right) \\exp{ \\left( \\boldsymbol{w}_{i}^{T} \\boldsymbol{x} + {b}_{i} \\right) }$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "1 - p \\left( y = {L}_{k} \\mid \\boldsymbol{x} \\right) & = \\sum_{j \\neq k} p \\left( y = {L}_{j} \\mid \\boldsymbol{x} \\right) && \\text{} \\\\\n",
    "& = \\sum_{j \\neq k} p \\left( y = {L}_{k} \\mid \\boldsymbol{x} \\right) \\exp{ \\left( \\boldsymbol{w}_{j}^{T} \\boldsymbol{x} + {b}_{j} \\right) } && \\text{Since $p \\left( y = {L}_{i} \\mid \\boldsymbol{x} \\right) = p \\left( y = {L}_{k} \\mid \\boldsymbol{x} \\right) \\exp{ \\left( \\boldsymbol{w}_{i}^{T} \\boldsymbol{x} + {b}_{i} \\right) }$} \\\\\n",
    "& = p \\left( y = {L}_{k} \\mid \\boldsymbol{x} \\right) \\sum_{j \\neq k} \\exp{ \\left( \\boldsymbol{w}_{j}^{T} \\boldsymbol{x} + {b}_{j} \\right) } && \\text{} \\\\\n",
    "& \\Rightarrow p \\left( y = {L}_{k} \\mid \\boldsymbol{x} \\right) = \\frac{1}{1 + \\sum_{j \\neq k} \\exp{ \\left( \\boldsymbol{w}_{j}^{T} \\boldsymbol{x} + {b}_{j} \\right) }} \\\\\n",
    "& \\Rightarrow p \\left( y = {L}_{i} \\mid \\boldsymbol{x} \\right) = \\frac{ \\exp{ \\left( \\boldsymbol{w}_{i}^{T} \\boldsymbol{x} + {b}_{i} \\right) } }{1 + \\sum_{j \\neq k} \\exp{ \\left( \\boldsymbol{w}_{j}^{T} \\boldsymbol{x} + {b}_{j} \\right) }} && \\text{}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Since $1 = \\exp{\\left( \\log{ \\frac{ p \\left( y = {L}_{k} \\mid \\boldsymbol{x} \\right) }{ p \\left( y = {L}_{k} \\mid \\boldsymbol{x} \\right) } } \\right)}$ we can write:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p \\left( y = {L}_{i} \\mid \\boldsymbol{x} \\right) & = \\frac{ \\exp{ \\left( \\boldsymbol{w}_{i}^{T} \\boldsymbol{x} + {b}_{i} \\right) } }{1 + \\sum_{j \\neq k} \\exp{ \\left( \\boldsymbol{w}_{j}^{T} \\boldsymbol{x} + {b}_{j} \\right) }} \\\\\n",
    "& = \\frac{ \\exp{ \\left( \\boldsymbol{w}_{i}^{T} \\boldsymbol{x} + {b}_{i} \\right) } }{\\exp{ \\left( \\boldsymbol{w}_{k}^{T} \\boldsymbol{x} + {b}_{k} \\right) } + \\sum_{j \\neq k} \\exp{ \\left( \\boldsymbol{w}_{j}^{T} \\boldsymbol{x} + {b}_{j} \\right) }} \\\\\n",
    "& = \\frac{ \\exp{ \\left( \\boldsymbol{w}_{i}^{T} \\boldsymbol{x} + {b}_{i} \\right) } }{ \\sum_{j} \\exp{ \\left( \\boldsymbol{w}_{j}^{T} \\boldsymbol{x} + {b}_{j} \\right) }}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Summary\n",
    "\n",
    "While there are many way to derive the logistic regression (for instance, also by assuming Binomial Distribution), the main motivation is its numerical properties.  \n",
    "Namely being convex with easy to calculate gradient.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The main motivation for _Logistic Regression_ based model is the natural expandability of the probability of the prediction.\n",
    "* <font color='brown'>(**#**)</font> The lecture slides derive the [Multinomial Logistic Regression](https://en.wikipedia.org/wiki/Multinomial_logistic_regression).\n",
    "* <font color='brown'>(**#**)</font> The first \"Deep Learning\" model were actually chaining many _Logistic Regression_ layers.\n",
    "* <font color='brown'>(**#**)</font> Most classification layers in Deep Learning models are basically _Logistic Regression_.\n",
    "* <font color='brown'>(**#**)</font> The _Logistic Regression_ model is linear, yet ca be extended by _Polynomial Transform_.  \n",
    "  See [Logistic Regression for Non Linear Separable Data](https://datascience.stackexchange.com/questions/21896).\n",
    "* <font color='brown'>(**#**)</font> The concept of Logistic Regression can also be used as pure regression for continuous data bounded in the range $\\left[ a, b \\right]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Grid Search Hyper Parameter Optimization\n",
    "\n",
    "1. Create a data frame with 3 columns:\n",
    "  - `Penalty` - The value of the `penalty` parameter.\n",
    "  - `C` - The value of `C` parameter.\n",
    "  - `ROC AUC` - The value of the `roc_auc()` of the model.\n",
    "   \n",
    "  The number of rows should match the number of combinations.  \n",
    "2. Iterate over all combinations and measure the score on the validation set.  \n",
    "3. Plot an heatmap (2D) for the combination of hyper parameters and the resulted AUC.  \n",
    "4. Extract the best model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Data Frame\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Calculate the number of combinations.\n",
    "# 2. Create a nested loop to create the combinations between the parameters.\n",
    "# 3. Store the combinations as the columns of a data frame.\n",
    "\n",
    "# For Advanced Python users: Use iteration tools for create the cartesian product\n",
    "numComb = len(lPenalty) * len(lC)\n",
    "dData   = {'Penalty': [], 'C': [], 'ROC AUC Train': [0.0] * numComb, 'ROC AUC Validation': [0.0] * numComb}\n",
    "\n",
    "for ii, paramPenalty in enumerate(lPenalty):\n",
    "    for jj, paramC in enumerate(lC):\n",
    "        dData['Penalty'].append(paramPenalty)\n",
    "        dData['C'].append(paramC)\n",
    "#===============================================================#\n",
    "\n",
    "dfModelScore = pd.DataFrame(data = dData)\n",
    "dfModelScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the Model\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Iterate over each row of the data frame `dfModelScore`.  \n",
    "#    Each row defines a set of hyper parameters to evaluate.\n",
    "# 2. Construct the model.\n",
    "# 3. Train it on the Train Data Set.\n",
    "# 4. Calculate its AUC ROC score on the train set, save it to the `ROC AUC Train`.\n",
    "# 5. Calculate its AUC ROC score on the validation set, save it to the `ROC AUC Validation`.\n",
    "# !! Make sure to chose the `saga` solver as it is the only one supporting all the regularization options.\n",
    "# !! Set the parameter `tol` to ~5e-3 to ensure convergence in a reasonable time.\n",
    "# !! Set the parameter `max_iter` to high value (10_000 or so) to make sure convergence of the model.\n",
    "\n",
    "for ii in range(numComb):\n",
    "    paramPenalty    = dfModelScore.loc[ii, 'Penalty']\n",
    "    paramC          = dfModelScore.loc[ii, 'C']\n",
    "\n",
    "    if paramPenalty == 'None':\n",
    "        paramPenalty = None\n",
    "\n",
    "    print(f'Processing model {ii + 1:03d} out of {numComb} with `penalty` = {paramPenalty} and `C` = {paramC:0.5f}.')\n",
    "\n",
    "    oLogRegCls = LogisticRegression(penalty = paramPenalty, tol = 5e-3, C = paramC, solver = 'saga', max_iter = 10_000)\n",
    "    oLogRegCls = oLogRegCls.fit(mXTrain, vYTrain)\n",
    "\n",
    "    dfModelScore.loc[ii, 'ROC AUC Train']      = roc_auc_score(vYTrain, oLogRegCls.predict_proba(mXTrain), multi_class = 'ovr')\n",
    "    dfModelScore.loc[ii, 'ROC AUC Validation'] = roc_auc_score(vYVal, oLogRegCls.predict_proba(mXVal), multi_class = 'ovr')\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> If one watches the timing of iterations above, he will see that higher regularization (Smaller `C`) will also be faster to calculate as the weights are less \"crazy\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Results\n",
    "# Display the results sorted (Validation).\n",
    "# Pandas allows sorting data by any column using the `sort_values()` method.\n",
    "# The `head()` allows us to see only the the first values.\n",
    "dfModelScore.sort_values(by = ['ROC AUC Validation'], ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Results\n",
    "# Display the results sorted (Train).\n",
    "dfModelScore.sort_values(by = ['ROC AUC Train'], ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Can you see cases of Under / Over Fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Data ROC AUC Heatmap\n",
    "# Plotting the Train Data ROC AUC as a Heat Map.\n",
    "# We can pivot the data set created to have a 2D matrix of the ROC AUC as a function of `C` and the `Penalty`.\n",
    "\n",
    "hA = sns.heatmap(data = dfModelScore.pivot(index = 'C', columns = 'Penalty', values = 'ROC AUC Train'), robust = True, linewidths = 1, annot = True, fmt = '0.2%', norm = LogNorm())\n",
    "hA.set_title('ROC AUC of the Train Data');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Data ROC AUC Heatmap\n",
    "# Plotting the Validation Data ROC AUC as a Heat Map.\n",
    "# We can pivot the data set created to have a 2D matrix of the ROC AUC as a function of `C` and the `Penalty`.\n",
    "\n",
    "hA = sns.heatmap(data = dfModelScore.pivot(index = 'C', columns = 'Penalty', values = 'ROC AUC Validation'), robust = True, linewidths = 1, annot = True, fmt = '0.2%', norm = LogNorm())\n",
    "hA.set_title('ROC AUC of the Validation Data');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Optimal Hyper Parameters\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Extract the index of row of the maximum value of `ROC AUC Validation`.\n",
    "# 2. Use the index of the row to extract the hyper parameters which were optimized.\n",
    "# !! You may find the `idxmax()` method of a Pandas data frame useful.\n",
    "\n",
    "idxArgMax = dfModelScore['ROC AUC Validation'].idxmax()\n",
    "#===============================================================#\n",
    "\n",
    "optimalPenalty  = dfModelScore.loc[idxArgMax, 'Penalty']\n",
    "optimalC        = dfModelScore.loc[idxArgMax, 'C']\n",
    "\n",
    "print(f'The optimal hyper parameters are: `penalty` = {optimalPenalty}, `C` = {optimalC}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Model\n",
    "\n",
    "In this section we'll extract the best model an retrain it on the whole data (`mX`).  \n",
    "We need to export the model which has the best score on the Validation data.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> In this case, the code does not use _Cross Validation_.  \n",
    "When time allows, it is better to use _Cross Validation_ to get more robust scoring and a good estimation of the real world performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the Optimal Model & Train on the Whole Data\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Construct the logistic regression model. Use the same `tol`, `solver` and `max_itr` as above.\n",
    "# 2. Fit the model on the whole data set (mX).\n",
    "oLogRegCls = LogisticRegression(penalty = optimalPenalty, tol = 5e-3, C = optimalC, solver = 'saga', max_iter = 10_000)\n",
    "oLogRegCls = oLogRegCls.fit(mX, vY)\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Score (Accuracy)\n",
    "\n",
    "print(f'The model score (Accuracy) is: {oLogRegCls.score(mX, vY):0.2%}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Does it match the results above? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain / Interpret the Model\n",
    "\n",
    "Linear models, which works mostly on correlation, are relatively easy to interpret / explain.  \n",
    "In this section we'll show how to interpret the weights of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Weights of the Classes\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Extract the weights of the model using the `coef_` attribute.\n",
    "mW = oLogRegCls.coef_ #<! The model weights (Without the biases)\n",
    "#===============================================================#\n",
    "\n",
    "print(f'The coefficients / weights matrix has the dimensions: {mW.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the weights basically match each pixel of the input image (As a vector) then we can display them as an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Weights as Images\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Convert the weights into the form of an image.\n",
    "# 2. Plot it using `imshow()` of Matplotlib.\n",
    "# !! You may use `PlotMnistImages()` to do this for you, look at its code.\n",
    "\n",
    "hF = PlotMnistImages(mW, range(10), numRows = 2, numCols = 5, randomChoice = False)\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Could you explain the results and how the model works?\n",
    "* <font color='brown'>(**#**)</font> Usually, for linear models, it is important to have zero mean features.\n",
    "* <font color='blue'>(**!**)</font> Run the above using the `StandardScaler()` as part of the pipeline (Don't alter the images themselves!)\n",
    "* <font color='brown'>(**#**)</font> Logistic Regression vs. Linear SVM: \n",
    "  * Compute wise the SVM has little advantage (Theoretically). In practice they are similar with efficient solvers (Stochastic for large scale).\n",
    "  * The Logistic Regression classifiers offers a native probabilistic interpretation and is mostly _calibrated_ out of teh box.\n",
    "  * The _Multinomial Logistic Regression_ classifier offers a native support for Multi Class data.\n",
    "  * The [Logistic Regression models the odds ratio which is infinite for separable case](https://stats.stackexchange.com/questions/422406). The SVM can handle such case easily.\n",
    "  * See [Similarity fo Linear SVM and Logistic Regression](https://stats.stackexchange.com/questions/375851), [Comparing SVM and Logistic Regression](https://stats.stackexchange.com/questions/95340), [SVM vs Logistic Regression](https://stats.stackexchange.com/questions/443351), [Hinge Loss vs. Logistic Loss](https://stats.stackexchange.com/questions/146277).\n",
    "* <font color='brown'>(**#**)</font> The Logistic Regression classifier can also be extended with the Kernel Trick:\n",
    "  * [Jonathan Richard Shewchuk - Introduction to Machine Learning](https://people.eecs.berkeley.edu/~jrs/189s19) - [The Kernel Trick](https://people.eecs.berkeley.edu/~jrs/189s19/lec/14.pdf).\n",
    "  * [Doina Precup - Logistic Regression: Second Order Methods and Kernels 2016](https://www.cs.mcgill.ca/~dprecup/courses/ML/Lectures/ml-lecture05.old.pdf), [Doina Precup - Logistic Regression: Second Order Methods and Kernels 2017](https://www.cs.mcgill.ca/~dprecup/courses/ML/Lectures/ml-lecture05.pdf).\n",
    "  * [Aarti Singh - Kernels: SVMs and Logistic Regression](https://www.cs.cmu.edu/~aarti/Class/10315_Fall19/lecs/Lecture11.pdf).\n",
    "  * [Reddit - Using the Kernel Trick on a Logistic Regression with Gradient Descent](https://www.reddit.com/r/MachineLearning/comments/3p9oqa).\n",
    "  * [Kernel Logistic Regression vs. Kernel SVM](https://stats.stackexchange.com/questions/43996).\n",
    "  * [Applying the Kernel Trick to Logistic Regression Classifier](https://stats.stackexchange.com/questions/529981)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
