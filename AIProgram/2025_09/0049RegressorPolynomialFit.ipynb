{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Fixel Algorithms](https://i.imgur.com/AqKHVZ0.png)](https://fixelalgorithms.gitlab.io)\n",
    "\n",
    "# AI Program\n",
    "\n",
    "## Machine Learning - Supervised Learning - Regression - Polynomial Fit\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 1.0.001 | 23/12/2025 | Royi Avital | Added a question about estimating the polynomial degree            |\n",
    "| 1.0.000 | 23/03/2024 | Royi Avital | First version                                                      |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/AIProgram/2024_02/0045RegressorPolynomialFit.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T09:30:06.492269Z",
     "start_time": "2022-02-02T09:30:06.220934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Miscellaneous\n",
    "from platform import python_version\n",
    "import random\n",
    "\n",
    "# Typing\n",
    "from typing import Callable, Dict, List, Optional, Set, Tuple, Union\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython\n",
    "from ipywidgets import FloatSlider, IntSlider, Layout\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought.\n",
    "\n",
    "Code Notations:\n",
    "\n",
    "```python\n",
    "someVar    = 2; #<! Notation for a variable\n",
    "vVector    = np.random.rand(4) #<! Notation for 1D array\n",
    "mMatrix    = np.random.rand(4, 3) #<! Notation for 2D array\n",
    "tTensor    = np.random.rand(4, 3, 2, 3) #<! Notation for nD array (Tensor)\n",
    "tuTuple    = (1, 2, 3) #<! Notation for a tuple\n",
    "lList      = [1, 2, 3] #<! Notation for a list\n",
    "dDict      = {1: 3, 2: 2, 3: 1} #<! Notation for a dictionary\n",
    "oObj       = MyClass() #<! Notation for an object\n",
    "dfData     = pd.DataFrame() #<! Notation for a data frame\n",
    "dsData     = pd.Series() #<! Notation for a series\n",
    "hObj       = plt.Axes() #<! Notation for an object / handler / function handler\n",
    "```\n",
    "\n",
    "### Code Exercise\n",
    "\n",
    " - Single line fill\n",
    "\n",
    "```python\n",
    "valToFill = ???\n",
    "```\n",
    "\n",
    " - Multi Line to Fill (At least one)\n",
    "\n",
    "```python\n",
    "# You need to start writing\n",
    "?????\n",
    "```\n",
    "\n",
    " - Section to Fill\n",
    "\n",
    "```python\n",
    "#===========================Fill This===========================#\n",
    "# 1. Explanation about what to do.\n",
    "# !! Remarks to follow / take under consideration.\n",
    "mX = ???\n",
    "\n",
    "?????\n",
    "#===============================================================#\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# %matplotlib inline\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "# Matplotlib default color palette\n",
    "lMatPltLibclr = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "# sns.set_theme() #>! Apply SeaBorn theme\n",
    "\n",
    "runInGoogleColab = 'google.colab' in str(get_ipython())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "FIG_SIZE_DEF    = (8, 8)\n",
    "ELM_SIZE_DEF    = 50\n",
    "CLASS_COLOR     = ('b', 'r')\n",
    "EDGE_COLOR      = 'k'\n",
    "MARKER_SIZE_DEF = 10\n",
    "LINE_WIDTH_DEF  = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courses Packages\n",
    "\n",
    "from DataVisualization import PlotRegressionData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Auxiliary Functions\n",
    "\n",
    "def PlotPolyFit( vX: NDArray, vY: NDArray, vP: Optional[NDArray] = None, P: int = 1, numGridPts: int = 1001, \n",
    "                hA: Optional[plt.Axes] = None, figSize: Tuple[int, int] = FIG_SIZE_DEF, markerSize: int = MARKER_SIZE_DEF, \n",
    "                lineWidth: int = LINE_WIDTH_DEF, axisTitle: str = None ) -> None:\n",
    "\n",
    "    if hA is None:\n",
    "        hF, hA = plt.subplots(1, 2, figsize = figSize)\n",
    "    else:\n",
    "        hF = hA[0].get_figure()\n",
    "\n",
    "    numSamples = len(vY)\n",
    "\n",
    "    # Polyfit\n",
    "    vW    = np.polyfit(vX, vY, P)\n",
    "    \n",
    "    # MSE\n",
    "    vHatY = np.polyval(vW, vX)\n",
    "    MSE   = (np.linalg.norm(vY - vHatY) ** 2) / numSamples\n",
    "    \n",
    "    # Plot\n",
    "    xx  = np.linspace(np.floor(np.min(vX)), np.ceil(np.max(vX)), numGridPts)\n",
    "    yy  = np.polyval(vW, xx)\n",
    "\n",
    "    hA[0].plot(vX, vY, '.r', ms = 10, label = r'$y_i$')\n",
    "    hA[0].plot(xx, yy, 'b',  lw = 2,  label = r'$\\hat{f}(x)$')\n",
    "    hA[0].set_title (f'$P = {P}$\\nMSE = {MSE}')\n",
    "    hA[0].set_xlabel('$x$')\n",
    "    # hA[0].axis(lAxis)\n",
    "    hA[0].grid()\n",
    "    hA[0].legend()\n",
    "    \n",
    "    hA[1].stem(vW[::-1], label = 'Estimated')\n",
    "    if vP is not None:\n",
    "        hA[1].stem(vP[::-1], linefmt = 'C1:', markerfmt = 'D', label = 'Ground Truth')\n",
    "    numTicks = len(vW) if vP is None else max(len(vW), len(vP))\n",
    "    hA[1].set_xticks(range(numTicks))\n",
    "    hA[1].set_title('Coefficients')\n",
    "    hA[1].set_xlabel('$w$')\n",
    "    hA[1].legend()\n",
    "\n",
    "    # return hA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Fit\n",
    "\n",
    "Polynomial Fit is about optimizing the parameters of a polynomial model to explain the given data.  \n",
    "There 2 main _polynomials_ in this context:\n",
    "\n",
    " * [Polynomial](https://en.wikipedia.org/wiki/Polynomial).\n",
    " * [Trigonometric Polynomial](https://en.wikipedia.org/wiki/Trigonometric_polynomial).\n",
    "\n",
    "The model parameters are liner with the data which means the solution is given by a _Linear System_.  \n",
    "Yet, in practice, due to noise or model incompatibility, the system can not be solved as it is.  \n",
    "Then a minimization is done using different objectives, among them the most popular are:\n",
    "\n",
    " * Least Squares (Induced by the ${L}^{2}$ Norm).\n",
    " * Least Deviation (Induced by the ${L}^{1}$ Norm).\n",
    "\n",
    "Using the the Least Squares model generates a Linear Least Squares problem.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> There are many other objectives in the field of regression in general and linear fit specifically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# Data Generation\n",
    "numSamples  = 30\n",
    "noiseStd    = 0.3\n",
    "\n",
    "vP = np.array([0.5, 2, 5])\n",
    "polynomDeg = 2\n",
    "\n",
    "# Data Visualization\n",
    "gridNoiseStd = 0.05\n",
    "numGridPts = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate / Load Data\n",
    "\n",
    "In the following we'll generate data according to the following model:\n",
    "\n",
    "$$ y_{i} = f \\left( x_{i} \\right) + \\epsilon_{i} $$\n",
    "\n",
    "Where\n",
    "\n",
    "$$ f \\left( x \\right) = \\frac{1}{2} x^{2} + 2x + 5 $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Data Generating Function\n",
    "\n",
    "def f( vX: NDArray, vP: NDArray ) -> NDArray:\n",
    "    # `vX` is a set of points in R\n",
    "\n",
    "    return np.polyval(vP, vX)\n",
    "\n",
    "hF = lambda vX: f(vX, vP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Data\n",
    "\n",
    "vX = np.linspace(-2, 2, numSamples, endpoint = True) + (gridNoiseStd * np.random.randn(numSamples))\n",
    "vN = noiseStd * np.random.randn(numSamples)\n",
    "vY = hF(vX) + vN\n",
    "\n",
    "print(f'The features data shape: {vX.shape}')\n",
    "print(f'The labels data shape: {vY.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Data\n",
    "\n",
    "PlotRegressionData(vX, vY);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> What would be the equivalent of class balance for regression? Think on the span and density of the values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Polyfit Regressor\n",
    "\n",
    "The PolyFit optimization problem is given by:\n",
    "\n",
    "$$ \\arg \\min_{\\boldsymbol{w}} {\\left\\| \\boldsymbol{\\Phi} \\boldsymbol{w} - \\boldsymbol{y} \\right\\|}_{2}^{2} $$\n",
    "\n",
    "Where\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\Phi} = \\begin{bmatrix} 1 & x_{1} & x_{1}^{2} & \\cdots & x_{1}^{p} \\\\\n",
    "1 & x_{2} & x_{2}^{2} & \\cdots & x_{2}^{p} \\\\\n",
    "\\vdots & \\vdots & \\vdots &  & \\vdots \\\\\n",
    "1 & x_{N} & x_{N}^{2} & \\cdots & x_{N}^{p}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This is a _polyfit_ with hyper parameter $p$.\n",
    "\n",
    "The optimal weights are calculated by linear system solvers.  \n",
    "Yet it is better to use solvers optimized for this task, such as:\n",
    "\n",
    " * NumPy: [`polyfit`](https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html).\n",
    " * SciKit Learn: [`LinearRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) combined with [`PolynomialFeatures`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html).\n",
    "\n",
    "In this notebook we'll use the NumPy's implementation.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> For arbitrary $\\Phi$ the above becomes a _linear regression_ problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial Fit\n",
    "\n",
    "# The order of Polynomial p(x) = w[0] * x**deg + ... + w[deg].\n",
    "# Hence we need to show it reversed:\n",
    "vW = np.polyfit(vX, vY, polynomDeg)\n",
    "\n",
    "for ii in range(polynomDeg + 1):\n",
    "    print(f'The coefficient of degree {ii}: {vW[-1 - ii]:0.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of the Polynomial Degree $p$\n",
    "\n",
    "The degree of the polynomial is basically the DoF of the model.  \n",
    "Tuning it is the way (Along with Regularization) to avoid _underfit_ and _overfit_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Polynomial Degree Parameter\n",
    "\n",
    "hPolyFit = lambda P: PlotPolyFit(vX, vY, vP = vP, P = P)\n",
    "pSlider = IntSlider(min = 0, max = 31, step = 1, value = 0, layout = Layout(width = '30%'))\n",
    "interact(hPolyFit, P = pSlider);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Beside applying cross validation, how could one infer a good approximation of the polynomial order? Think about diminishing returns.\n",
    "* <font color='red'>(**?**)</font> What happens when the degree of the polynomial is higher than the number of samples?\n",
    "* <font color='red'>(**?**)</font> What would be the optimal $\\boldsymbol{w}$ in case the model matrix is given by $\\boldsymbol{\\Phi} = \\begin{bmatrix} 5 & 2 x_{1} & \\frac{1}{2} x_{1}^{2} \\\\\n",
    "5 & 2 x_{2} & \\frac{1}{2} x_{2}^{2} & \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "5 & 2 x_{N} & \\frac{1}{2} x_{N}^{2}\n",
    "\\end{bmatrix}$?\n",
    "* <font color='brown'>(**#**)</font> The properties of the model matrix are important. As we basically after the best approximation of the data in the space its columns spans. For instance, for Polynomial Fit, we ca use better basis for polynomials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity to Support\n",
    "\n",
    "We'll show the effect of the support, given a number of sample on the estimated weights (_Coefficients_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Generation\n",
    "vN = 20 * noiseStd * np.random.randn(numSamples)\n",
    "\n",
    "def GenDataByRadius( vP, P, vN, valR: float = 1.0 ):\n",
    "\n",
    "    vX = np.linspace(-valR, valR, np.size(vN), endpoint = True)\n",
    "    vY = f(vX, vP) + vN\n",
    "    \n",
    "    PlotPolyFit(vX, vY, vP = vP, P = P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Visualization\n",
    "\n",
    "hGenDataByRadius = lambda valR: GenDataByRadius(vP, polynomDeg, vN, valR)\n",
    "\n",
    "rSlider = FloatSlider(min = 0.1, max = 50.0, step = 0.1, value = 0.1, layout = Layout(width = '30%'))\n",
    "interact(hGenDataByRadius, valR = rSlider);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samples and Coefficients MSE as a Function of Support\n",
    "# Doing the above manually.\n",
    "\n",
    "def GenDataRadius( vP, vN, valR: float = 1.0 ):\n",
    "\n",
    "    vX = np.linspace(-valR, valR, np.size(vN), endpoint = True)\n",
    "    vY = f(vX, vP) + vN\n",
    "\n",
    "    return vX, vY\n",
    "\n",
    "vR = np.linspace(0.1, 50, 20)\n",
    "\n",
    "\n",
    "for valR in vR:\n",
    "    vX, vY = GenDataRadius(vP, 0.5 * vN, valR)\n",
    "    vW = np.polyfit(vX, vY, polynomDeg)\n",
    "\n",
    "    vYPred = np.polyval(vW, vX)\n",
    "    valMSESamples = np.mean(np.square(vYPred - vY))\n",
    "    valMSECoeff = np.mean(np.square(vW - vP))\n",
    "    print(f'The Support Radius  : {valR}.')\n",
    "    print(f'The Samples MSE     : {valMSESamples}.')\n",
    "    print(f'The Coefficients MSE: {valMSECoeff}.')\n",
    "    print(f'The Estimated Coef  : {vW}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
