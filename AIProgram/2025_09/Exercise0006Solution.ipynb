{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Fixel Algorithms](https://i.imgur.com/AqKHVZ0.png)](https://fixelalgorithms.gitlab.io)\n",
    "\n",
    "# AI Program\n",
    "\n",
    "## Exercise 0006 - Classification\n",
    "\n",
    "Feature engineering for text classification.\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 1.0.000 | 23/03/2024 | Royi Avital | First version                                                      |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/AIProgram/2024_02/Exercise0006.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T09:30:06.492269Z",
     "start_time": "2022-02-02T09:30:06.220934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Miscellaneous\n",
    "import gdown\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import urllib.request\n",
    "import re\n",
    "\n",
    "# Typing\n",
    "from typing import Callable, Dict, List, Optional, Set, Tuple, Union\n",
    "\n",
    "# Visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought.\n",
    "\n",
    "Code Notations:\n",
    "\n",
    "```python\n",
    "someVar    = 2; #<! Notation for a variable\n",
    "vVector    = np.random.rand(4) #<! Notation for 1D array\n",
    "mMatrix    = np.random.rand(4, 3) #<! Notation for 2D array\n",
    "tTensor    = np.random.rand(4, 3, 2, 3) #<! Notation for nD array (Tensor)\n",
    "tuTuple    = (1, 2, 3) #<! Notation for a tuple\n",
    "lList      = [1, 2, 3] #<! Notation for a list\n",
    "dDict      = {1: 3, 2: 2, 3: 1} #<! Notation for a dictionary\n",
    "oObj       = MyClass() #<! Notation for an object\n",
    "dfData     = pd.DataFrame() #<! Notation for a data frame\n",
    "dsData     = pd.Series() #<! Notation for a series\n",
    "hObj       = plt.Axes() #<! Notation for an object / handler / function handler\n",
    "```\n",
    "\n",
    "### Code Exercise\n",
    "\n",
    " - Single line fill\n",
    "\n",
    "```python\n",
    "valToFill = ???\n",
    "```\n",
    "\n",
    " - Multi Line to Fill (At least one)\n",
    "\n",
    "```python\n",
    "# You need to start writing\n",
    "?????\n",
    "```\n",
    "\n",
    " - Section to Fill\n",
    "\n",
    "```python\n",
    "#===========================Fill This===========================#\n",
    "# 1. Explanation about what to do.\n",
    "# !! Remarks to follow / take under consideration.\n",
    "mX = ???\n",
    "\n",
    "?????\n",
    "#===============================================================#\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# %matplotlib inline\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "# sns.set_theme() #>! Apply SeaBorn theme\n",
    "\n",
    "runInGoogleColab = 'google.colab' in str(get_ipython())\n",
    "\n",
    "nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "DATA_FILE_URL  = r'https://github.com/FixelAlgorithmsTeam/FixelCourses/raw/master/DataSets/KaggleWhatsCooking.json'\n",
    "DATA_FILE_NAME = 'KaggleWhatsCooking.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Course Packages\n",
    "\n",
    "from DataVisualization import PlotConfusionMatrix, PlotLabelsHistogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Auxiliary Functions\n",
    "\n",
    "def ReadData( filePath: str ) -> tuple[list, list, list]:\n",
    "    # read data into lists\n",
    "    \n",
    "    hFile = open(filePath)\n",
    "    dJsonData = json.load(hFile)\n",
    "        \n",
    "    lId, lCuisine, lIngredients = [], [], []\n",
    "    for ii in range(len(dJsonData)):\n",
    "        lId.append(dJsonData[ii]['id'])\n",
    "        lCuisine.append(dJsonData[ii]['cuisine'])\n",
    "        lIngredients.append(dJsonData[ii]['ingredients'])  \n",
    "                \n",
    "    return lId, lCuisine, lIngredients\n",
    "\n",
    "def RemoveDigits( lIngredients: list ) -> list:\n",
    "    # Remove digits from the ingredients list\n",
    "    \n",
    "    #===========================Fill This===========================#\n",
    "    # 1. Look for the symbol of a digit in RegExp.\n",
    "    # 2. If the digit symbol is `?` put `?+` to match more than one digit in a row.\n",
    "    return [[re.sub(\"\\d+\", \"\", x) for x in y] for y in lIngredients]\n",
    "    #===============================================================#\n",
    "\n",
    "def RemoveChars( lIngredients: list ) -> list:\n",
    "    # Remove some unnecessary characters from the ingredients list\n",
    "   \n",
    "    lIngredients = [[x.replace(\"-\", \" \") for x in y] for y in lIngredients]\n",
    "    #===========================Fill This===========================# \n",
    "    # 01. Remove the following: & \n",
    "    # 02. Remove the following: '\n",
    "    # 03. Remove the following: ''\n",
    "    # 04. Remove the following: % \n",
    "    # 05. Remove the following: ! \n",
    "    # 06. Remove the following: (  \n",
    "    # 07. Remove the following: ) \n",
    "    # 08. Remove the following: / \n",
    "    # 09. Remove the following: \\ \n",
    "    # 10. Remove the following: , \n",
    "    # 11. Remove the following: . \n",
    "    # 12. Remove the following: \"\n",
    "    # !!! In some cases escaping is required.\n",
    "    # !!! Look at the above example.\n",
    "    lIngredients = [[x.replace(\"&\", \" \") for x in y] for y in lIngredients] \n",
    "    lIngredients = [[x.replace(\"'\", \" \") for x in y] for y in lIngredients] \n",
    "    lIngredients = [[x.replace(\"''\", \" \") for x in y] for y in lIngredients] \n",
    "    lIngredients = [[x.replace(\"%\", \" \") for x in y] for y in lIngredients] \n",
    "    lIngredients = [[x.replace(\"!\", \" \") for x in y] for y in lIngredients] \n",
    "    lIngredients = [[x.replace(\"(\", \" \") for x in y] for y in lIngredients] \n",
    "    lIngredients = [[x.replace(\")\", \" \") for x in y] for y in lIngredients] \n",
    "    lIngredients = [[x.replace(\"/\", \" \") for x in y] for y in lIngredients] \n",
    "    lIngredients = [[x.replace(\"\\\\\", \" \") for x in y] for y in lIngredients] \n",
    "    lIngredients = [[x.replace(\",\", \" \") for x in y] for y in lIngredients] \n",
    "    lIngredients = [[x.replace(\".\", \" \") for x in y] for y in lIngredients]\n",
    "    lIngredients = [[x.replace('\"', \" \") for x in y] for y in lIngredients]\n",
    "    #===============================================================# \n",
    "    lIngredients = [[x.replace(u\"\\u2122\", \" \") for x in y] for y in lIngredients] \n",
    "    lIngredients = [[x.replace(u\"\\u00AE\", \" \") for x in y] for y in lIngredients] \n",
    "    lIngredients = [[x.replace(u\"\\u2019\", \" \") for x in y] for y in lIngredients] \n",
    "\n",
    "    return lIngredients\n",
    "\n",
    "def LowerCase( lIngredients: list ) -> list:\n",
    "    # Make letters lowercase for the ingredients list\n",
    "    \n",
    "    #===========================Fill This===========================# \n",
    "    # 1. Make lower case of the text. \n",
    "    # !! Pay attention that the input is a list of lists!\n",
    "    return [[x.lower() for x in y] for y in lIngredients]\n",
    "    #===============================================================# \n",
    "\n",
    "def RemoveRedundantWhiteSpace( lIngredients: list ) -> list:\n",
    "    # Removes redundant whitespaces\n",
    "    \n",
    "    #===========================Fill This===========================# \n",
    "    # 1. Look for the symbol of a space in RegExp.\n",
    "    # 2. If the space symbol is `?` put `?+` to match more than one space in a row.\n",
    "    # !! Pay attention that the input is a list of lists!\n",
    "    return [[re.sub('\\s+', ' ', x).strip() for x in y] for y in lIngredients] \n",
    "    #===============================================================# \n",
    "    \n",
    "    \n",
    "def StemWords( lIngredients: list ) -> list:\n",
    "    # Word stemming for ingredients list (Per word)\n",
    "    \n",
    "    #===========================Fill This===========================# \n",
    "    # 1. Construct the `WordNetLemmatizer` object.\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    #===============================================================# \n",
    "    \n",
    "    def WordByWord( inStr: str ):\n",
    "        \n",
    "        return \" \".join([\"\".join(lmtzr.lemmatize(w)) for w in inStr.split()])\n",
    "    \n",
    "    return [[WordByWord(x) for x in y] for y in lIngredients] \n",
    "    \n",
    "    \n",
    "def RemoveUnits( lIngredients: list ) -> list:\n",
    "    # Remove units related words from ingredients\n",
    "    \n",
    "    remove_list = ['g', 'lb', 's', 'n']\n",
    "        \n",
    "    def CheckWord( inStr: str ):\n",
    "        \n",
    "        splitStr = inStr.split()\n",
    "        resStr  = [word for word in splitStr if word.lower() not in remove_list]\n",
    "        \n",
    "        return ' '.join(resStr)\n",
    "\n",
    "    return [[CheckWord(x) for x in y] for y in lIngredients]\n",
    "\n",
    "def ExtractUniqueIngredients( lIngredients: list, sortList: bool = True ) -> list:\n",
    "    # Extract all unique ingredients from the list as a single list\n",
    "\n",
    "    #===========================Fill This===========================# \n",
    "    # 1. Extract the unique values of ingredients (You use the `set()` data type of Python).\n",
    "    # 2. Sort it by name if `sortList == True`. \n",
    "    lUniqueIng = list(set([ing for lIngredient in lIngredients for ing in lIngredient]))\n",
    "    if sortList:\n",
    "        lUniqueIng = sorted(lUniqueIng)\n",
    "    #===============================================================# \n",
    "\n",
    "    return lUniqueIng\n",
    "\n",
    "def ExtractFeatureEncoding( lIngredient: list, lUniqueIng: list ) -> np.ndarray:\n",
    "    # If an ingredient is in the specific recipe\n",
    "    \n",
    "    mF = np.zeros(shape = (len(lIngredient), len(lUniqueIng)), dtype = np.uint)\n",
    "    #===========================Fill This===========================# \n",
    "    # 1. Iterate over the list of lists of the ingredients.\n",
    "    # 2. For each sample (List of ingredients), put 1 in the location of the ingredients.\n",
    "    for ii in range(len(lIngredient)):\n",
    "        for jj in lIngredient[ii]:\n",
    "            mF[ii, lUniqueIng.index(jj)] = 1\n",
    "    #===============================================================# \n",
    "            \n",
    "    return mF\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "This exercise introduces:\n",
    "\n",
    " - Working with real world data in the context of basic Natural Language Processing (NLP).\n",
    " - Working with binary features using Decision Trees.\n",
    " - Working with Ensemble Method based on trees.\n",
    " - Utilizing the `LightGBM` package with the `LGBMClassifier`.\n",
    "\n",
    "\n",
    "* <font color='brown'>(**#**)</font> One of the objectives of this exercise is working on non trivial data set in size, features and performance.\n",
    "* <font color='brown'>(**#**)</font> SciKit Learn has some text feature extractors in the [`sklearn.feature_extraction.text`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text).  \n",
    "  You're encouraged ot use them to improve results after finishing the exercise once without them.\n",
    "\n",
    "In this exercise we'll work the data set: [Yummly - What's Cooking?](https://www.kaggle.com/competitions/whats-cooking) from [Kaggle](https://www.kaggle.com).  \n",
    "The data set is basically a list of ingredients of a recipe (Features) and the type of cuisine of the recipe (Italian, French, Indian, etc...).  \n",
    "The objective is being able to classify the cuisine of a recipe by its ingredients.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The data set will be downloaded and parsed automatically.\n",
    "\n",
    "The data will be defined as the following:\n",
    "\n",
    "1. A boolean matrix of size `numSamples x numFeatures`.\n",
    "2. The features are the list of all ingredients in the recipes.\n",
    "3. For a recipe, the features vector is hot encoding of the features.  \n",
    "\n",
    "For example, if the list of features is: `basil, chicken, egg, eggplant, garlic, pasta, salt, tomato sauce`.  \n",
    "Then for Pasta with Tomato Sauce the features vector will be: `[1, 0, 0, 0, 1, 1, 1, 1]` which means: `basil, garlic, pasta, salt, tomato sauce`.  \n",
    "This will be the basic feature list while you're encourages to add more features.\n",
    "\n",
    "In this exercise:\n",
    "\n",
    "1. Download the data (Automatically by the code).\n",
    "2. Parse data into a data structure to work with (Automatically by the code).\n",
    "3. Extract features from the recipes (The basic features: Existence of an ingredient).\n",
    "4. Train an Ensemble of Decision Trees using the LightGBM models (Very fast).\n",
    "5. Optimize the model hyper parameters (See below).\n",
    "6. Plot the _confusion matrix_ of the best model on the data.\n",
    "\n",
    "Optimize features (repeat if needed) to get accuracy of at least `70%`.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> Working with text requires some knowledge in [Regular Expression](https://en.wikipedia.org/wiki/Regular_expression).  \n",
    "  This is the most useful engine to handle deterministic patterns at the character level.  \n",
    "  [RegExOne](https://regexone.com) has a great tutorial and [RegEx 101](https://regex101.com) has a great online tool to experiment with.\n",
    "* <font color='brown'>(**#**)</font> Read on: [Stemming](https://en.wikipedia.org/wiki/Stemming) and [Lemmatization](https://en.wikipedia.org/wiki/Lemmatization).\n",
    "* <font color='brown'>(**#**)</font> It might be useful to use the [NLTK](https://github.com/nltk/nltk) package for word stemming.\n",
    "* <font color='brown'>(**#**)</font> To install the package (Prior to working with the notebook):\n",
    "  - Open Anaconda command line (`Prompt`).\n",
    "  - Activate the course environment by: `conda activate <CourseEnvName>`.\n",
    "  - Install the package using `conda install nltk -c conda-forge`. \n",
    "  - You may use `micromamba` instead of `conda`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "numSamplesTrain = 35_000\n",
    "numSamplesTest  = None\n",
    "\n",
    "# Hyper Parameters of the Model\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Set the list of learning rate (4 values in range [0.05, 0.5]).\n",
    "# 2. Set the list of maximum iterations (3 integer values in range [10, 200]).\n",
    "# 3. Set the list of maximum nodes (3 integer values in range [10, 50]).\n",
    "lLearnRate  = [0.05, 0.10, 0.15, 0.20] #<! List of learn rates\n",
    "lMaxItr     = [50, 100, 200] #<! List of maximum iterations\n",
    "lMaxNodes   = [20, 30, 50] #<! List of maximum nodes (Leaves)\n",
    "#===============================================================#\n",
    "\n",
    "numFold     = 3 #<! Don't change!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='blue'>(**!**)</font> Fill the functions in `Auxiliary Functions` **after** reading the code below which use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate / Load Data\n",
    "\n",
    "Load the classification data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Data\n",
    "# This section downloads data from the given URL if needed.\n",
    "\n",
    "if not os.path.exists(DATA_FILE_NAME):\n",
    "    urllib.request.urlretrieve(DATA_FILE_URL, DATA_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "\n",
    "lId, lCuisine, lIngredients = ReadData(DATA_FILE_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre Processing the Data\n",
    "\n",
    "In this section we'll do as following:\n",
    "\n",
    "1. Make all text _lower case_.\n",
    "2. Remove digits (Weights etc...).\n",
    "3. Remove some not required chars.\n",
    "4. Remove redundant spaces.\n",
    "5. Remove units.\n",
    "6. Stem the text (See [Word Stemming](https://en.wikipedia.org/wiki/Stemming)).\n",
    "\n",
    "The objective is to reduce the sensitivity to the style used to describe the ingredients.  \n",
    "So we're after the most basic way to describe each ingredient.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The list above is the minimum to be done. You're encouraged to use more ideas. For example:\n",
    "  - The number of ingredients.\n",
    "  - Higher level aggregation of ingredients: Cheese, Flour, Sauce, etc...\n",
    "* <font color='brown'>(**#**)</font> Look at the features list after this. You'll find there are still duplications and redundancy.  \n",
    "   Removing those will improve results.\n",
    "* <font color='brown'>(**#**)</font> There are extreme number of features, in this case, being able to minimize the number by removing redundant features is useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre Process Data\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Fill the body of the functions above.\n",
    "lIng = LowerCase(lIngredients)\n",
    "lIng = RemoveDigits(lIng)\n",
    "lIng = RemoveChars(lIng)\n",
    "lIng = RemoveRedundantWhiteSpace(lIng)\n",
    "lIng = RemoveUnits(lIng)\n",
    "lIng = StemWords(lIng)\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Features\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Fill the body of the function above.\n",
    "lFeat = ExtractUniqueIngredients(lIng)\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> The function `ExtractFeatureEncoding` matches based on the whole name of the ingredient.  \n",
    "  For multi words ingredients one might use even a match of a single word.  \n",
    "  This can be useful for cases like `ketchup` vs. `tomato ketchup`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data and Feature Engineering / Extraction\n",
    "\n",
    "The idea of the feature engineering in this case is assisting the classifier to identify patterns.  \n",
    "Most of the cuisines have some patterns associated with them, for example: dough, tomato and cheese.  \n",
    "The combinations are given by one hot encoding of the ingredients.\n",
    "\n",
    "\n",
    "* <font color='brown'>(**#**)</font> You're encouraged to think on more features!\n",
    "* <font color='brown'>(**#**)</font> Pay attention to dimensionality fo the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Features Encoding\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Fill the body of the function above.\n",
    "mF = ExtractFeatureEncoding(lIng, lFeat) #<! Features matrix\n",
    "#===============================================================#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Data\n",
    "# Create a Data Frame of the data\n",
    "dfX = pd.DataFrame(columns = lFeat, data = mF)\n",
    "dfX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Labels Data\n",
    "\n",
    "dsY = pd.Series(data = lCuisine, name = 'Cuisine')\n",
    "dsY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels as Categorical Data\n",
    "\n",
    "vY          = pd.Categorical(dsY).codes\n",
    "lEncoding   = pd.Categorical(dsY).categories.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Dimensions\n",
    "\n",
    "print(f'The data shape: {dfX.shape}')\n",
    "print(f'The labels shape: {dsY.shape}')\n",
    "print(f'The number of classes: {len(dsY.unique())}')\n",
    "print(f'The unique values of the labels: {dsY.unique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Labels Distribution\n",
    "\n",
    "hF, hA = plt.subplots(figsize = (12, 8))\n",
    "hA = PlotLabelsHistogram(dsY, hA = hA, xLabelRot = 90)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Is this a balanced data set?\n",
    "* <font color='red'>(**?**)</font> If the data is imbalanced, what approach would you use in this case to handle it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data\n",
    "\n",
    "We'll split the data into training and testing.  \n",
    "Set `numSamplesTrain`. For the first tries you use small number just to verify everything works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Train & Test Data\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Split the data using `train_test_split()`.\n",
    "# 2. Make sure to use `numSamplesTrain` and `numSamplesTest`.\n",
    "# 3. Set the `random_state` so iterative runs will be reproducible.\n",
    "mXTrain, mXTest, vYTrain, vYTest = train_test_split(mF, vY, train_size = numSamplesTrain, test_size = numSamplesTest, random_state = seedNum, shuffle = True, stratify = vY)\n",
    "#===============================================================#\n",
    "\n",
    "\n",
    "# Dimensions of the Data\n",
    "print(f'The number of training data samples: {mXTrain.shape[0]}')\n",
    "print(f'The number of training features per sample: {mXTrain.shape[1]}') \n",
    "\n",
    "\n",
    "print(f'The number of test data samples: {mXTest.shape[0]}')\n",
    "print(f'The number of test features per sample: {mXTest.shape[1]}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> What's the ratio of the train samples vs. number of features? What do you think it should be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of Classes\n",
    "\n",
    "# Train\n",
    "hA = PlotLabelsHistogram(vYTrain, lClass = lEncoding, xLabelRot = 90)\n",
    "hA.set_title(hA.get_title() + ' - Train Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of Classes\n",
    "\n",
    "# Test\n",
    "hA = PlotLabelsHistogram(vYTest, lClass = lEncoding, xLabelRot = 90)\n",
    "hA.set_title(hA.get_title() + ' - Test Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Which score method would you use between _accuracy_, _recall_, _precision_ or _F1_?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize Classifiers\n",
    "\n",
    "In this section we'll train an Ensemble of Trees using the [`LGBMClassifier`](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html) class of the [LightGMB](https://github.com/microsoft/LightGBM) package.  \n",
    "We'll learn the ensemble model later in the course, but for now we'll just optimize its hyper parameters.\n",
    "This model has a lot of hyper parameters yet we'll focus on:\n",
    "\n",
    " - Number of Leaves Nodes (`num_leaves`) - Sets the maximum number of leaves in each tree.\n",
    " - Learning Rate (`learning_rate`) - The learning rate of the ensemble (The significance of each model compared to those before it).\n",
    " - Number of Trees (`n_estimators`) - The number of iterations of the algorithm. In each iteration a single tree is added.\n",
    "\n",
    "The score will be the F1 score with averaging over all classes (Use `f1_micro` string).  \n",
    "\n",
    "Those are the generic steps for hyper parameter optimization: \n",
    "\n",
    "1. Build the dictionary of parameters for the grid search.\n",
    "2. Construct the grid search object (`GridSearchCV`).\n",
    "3. Optimize the hyper parameters by the `fit()` method of the grid search object.\n",
    "\n",
    "* <font color='red'>(**?**)</font> Why is the _F1_ score a reasonable choice in this case?\n",
    "* <font color='brown'>(**#**)</font> There are several implementations of tree based ensemble methods which are considered better and more production ready than _SciKit Learn_ while being compatible with it:\n",
    "  * [XGBoost](https://github.com/dmlc/xgboost) - The pioneer of specialized boosting trees. Very efficient and widely used. Lately added the feature of _histogram based_ training.  \n",
    "    Originally developed by the _Distributed (Deep) Machine Learning Community_ (DMLC) group at _Washington University_.\n",
    "  * [LightGBM](https://github.com/microsoft/LightGBM) - Pioneered the concept of _histogram based_ training which gives a much faster training with minimal effect on the performance.  \n",
    "    Developed by _Microsoft_.\n",
    "  * [CatBoost](https://github.com/catboost/catboost) - Known for optimized treatment of _categorical_ features and extreme optimization.  \n",
    "    Developed by _Yandex_ (Russian company)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code based on SciKit Learn's Model (`HistGradientBoostingClassifier`) which is too slow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Construct the Grid Search Object\n",
    "# # Sci Kit Learn's implementation is too slow for this.\n",
    "\n",
    "# #===========================Fill This===========================#\n",
    "# # 1. Set the parameters to iterate over and their values.\n",
    "# dParams = {'learning_rate': lLearnRate, 'max_iter': lMaxItr, 'max_leaf_nodes': lMaxNodes}\n",
    "# #===============================================================#\n",
    "\n",
    "# vCatFeatFlag = np.full(shape = len(lFeat), fill_value = True)\n",
    "# oGsSvc = GridSearchCV(estimator = HistGradientBoostingClassifier(categorical_features = vCatFeatFlag), param_grid = dParams, scoring = 'f1_micro', cv = numFold, verbose = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optimize\n",
    "\n",
    "# #===========================Fill This===========================#\n",
    "# # 1. Apply the grid search phase.\n",
    "# oGsSvc = oGsSvc.fit(mXTrain, vYTrain)\n",
    "# #===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the Grid Search Object \n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Set the parameters to iterate over and their values.\n",
    "# 2. Set the estimator of `GridSearchCV` to `LGBMClassifier`.\n",
    "# 3. Set the parameters grid.\n",
    "# 4. Set the scoring to `f1_micro`.\n",
    "# 5. Set the number of folds.\n",
    "# 6. Set the verbosity level to the highest.\n",
    "dParams = {'num_leaves': lMaxNodes, 'learning_rate': lLearnRate, 'n_estimators': lMaxItr} #<! Parameters dictionary\n",
    "oGsSvc = GridSearchCV(estimator = LGBMClassifier(), param_grid = dParams, scoring = 'f1_micro', cv = numFold, verbose = 4)\n",
    "#===============================================================#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize\n",
    "# Might take few minutes!\n",
    "\n",
    "# Set the indices of the categorical features.\n",
    "# If you extend `mF` beyond the default, make sure to adjust accordingly.\n",
    "vCatFeatFlag = np.full(shape = len(lFeat), fill_value = True)\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Apply the grid search phase.\n",
    "# oGsSvc = oGsSvc.fit(mXTrain, vYTrain, **{'categorical_feature': vCatFeatFlag})\n",
    "oGsSvc = oGsSvc.fit(mXTrain, vYTrain)\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> The above might take a while (Up to 10 minutes)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix on Test Data \n",
    "\n",
    "In this section we'll test the model on the test data.\n",
    "\n",
    "1. Extract the best estimator from the grid search.\n",
    "2. If needed, fit it to the train data.\n",
    "3. Display the _confusion matrix_ for the train and test data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Best Model\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Get the best model with the optimized hyper parameters.\n",
    "oBestModel = oGsSvc.best_estimator_\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Does the best model need a refit on data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the Model\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Train the model on the whole training data.\n",
    "oBestModel = oBestModel.fit(mXTrain, vYTrain)\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Confusion Matrix (Train)\n",
    "hF, hA = plt.subplots(figsize = (12, 12))\n",
    "\n",
    "hA, mConfMat = PlotConfusionMatrix(vYTrain, oBestModel.predict(mXTrain), lLabels = lEncoding, hA = hA, xLabelRot = 90, normMethod = 'true', valFormat = '0.0%')\n",
    "hA.set_title(hA.get_title() + ' - Train Data')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Confusion Matrix (Test)\n",
    "hF, hA = plt.subplots(figsize = (12, 12))\n",
    "\n",
    "\n",
    "hA, mConfMat = PlotConfusionMatrix(vYTest, oBestModel.predict(mXTest), lLabels = lEncoding, hA = hA, xLabelRot = 90, normMethod = 'true', valFormat = '0.0%')\n",
    "hA.set_title(hA.get_title() + ' - Test Data')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "# should be above 70%\n",
    "print(f'The best model accuracy is: {oBestModel.score(mXTest, vYTest):0.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> How would you handle the case the test would have features not in the training?\n",
    "* <font color='red'>(**?**)</font> Have a look on the good performing cuisines vs. the bad ones, can you think why?\n",
    "* <font color='green'>(**@**)</font> Try to get more features and improve results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
