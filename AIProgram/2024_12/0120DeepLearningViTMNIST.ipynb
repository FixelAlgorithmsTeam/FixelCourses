{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Fixel Algorithms](https://i.imgur.com/AqKHVZ0.png)](https://fixelalgorithms.gitlab.io)\n",
    "\n",
    "# AI Program\n",
    "\n",
    "## Deep Learning - Vision Transformer (ViT) - Classification of MNIST\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 1.0.000 | 21/09/2025 | Royi Avital | First version                                                      |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/AIProgram/2024_02/0089DeepLearningPyTorchSchedulers.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T09:30:06.492269Z",
     "start_time": "2022-02-02T09:30:06.220934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn            as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchinfo\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "import torchvision\n",
    "import torchvista\n",
    "\n",
    "# Miscellaneous\n",
    "from platform import python_version\n",
    "import random\n",
    "\n",
    "# Typing\n",
    "from typing import Callable, Dict, Generator, List, Literal, Optional, Self, Set, Tuple, Union\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought.\n",
    "\n",
    "Code Notations:\n",
    "\n",
    "```python\n",
    "someVar    = 2; #<! Notation for a variable\n",
    "vVector    = np.random.rand(4) #<! Notation for 1D array\n",
    "mMatrix    = np.random.rand(4, 3) #<! Notation for 2D array\n",
    "tTensor    = np.random.rand(4, 3, 2, 3) #<! Notation for nD array (Tensor)\n",
    "tuTuple    = (1, 2, 3) #<! Notation for a tuple\n",
    "lList      = [1, 2, 3] #<! Notation for a list\n",
    "dDict      = {1: 3, 2: 2, 3: 1} #<! Notation for a dictionary\n",
    "oObj       = MyClass() #<! Notation for an object\n",
    "dfData     = pd.DataFrame() #<! Notation for a data frame\n",
    "dsData     = pd.Series() #<! Notation for a series\n",
    "hObj       = plt.Axes() #<! Notation for an object / handler / function handler\n",
    "```\n",
    "\n",
    "### Code Exercise\n",
    "\n",
    " - Single line fill\n",
    "\n",
    "```python\n",
    "valToFill = ???\n",
    "```\n",
    "\n",
    " - Multi Line to Fill (At least one)\n",
    "\n",
    "```python\n",
    "# You need to start writing\n",
    "?????\n",
    "```\n",
    "\n",
    " - Section to Fill\n",
    "\n",
    "```python\n",
    "#===========================Fill This===========================#\n",
    "# 1. Explanation about what to do.\n",
    "# !! Remarks to follow / take under consideration.\n",
    "mX = ???\n",
    "\n",
    "?????\n",
    "#===============================================================#\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# %matplotlib inline\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "# Matplotlib default color palette\n",
    "lMatPltLibclr = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "# sns.set_theme() #>! Apply SeaBorn theme\n",
    "\n",
    "runInGoogleColab = 'google.colab' in str(get_ipython())\n",
    "\n",
    "# Improve performance by benchmarking\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Reproducibility (Per PyTorch Version on the same device)\n",
    "# torch.manual_seed(seedNum)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark     = False #<! Makes things slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "FIG_SIZE_DEF    = (8, 8)\n",
    "ELM_SIZE_DEF    = 50\n",
    "CLASS_COLOR     = ('b', 'r')\n",
    "EDGE_COLOR      = 'k'\n",
    "MARKER_SIZE_DEF = 10\n",
    "LINE_WIDTH_DEF  = 2\n",
    "\n",
    "D_CLASSES_MNIST = {0: 'T-Shirt', 1: 'Trouser', 2: 'Pullover', 3: 'Dress', 4: 'Coat', 5: 'Sandal', 6: 'Shirt', 7: 'Sneaker', 8: 'Bag', 9: 'Boots'}\n",
    "L_CLASSES_MNIST = ['T-Shirt', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Boots']\n",
    "\n",
    "T_IMG_SIZE_MNIST = (28, 28)\n",
    "\n",
    "DATA_FOLDER_PATH  = 'Data'\n",
    "TENSOR_BOARD_BASE = 'TB'\n",
    "WANDB_API_KEY     = 'WANDB_API_KEY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Auxiliary Modules for Google Colab\n",
    "if runInGoogleColab:\n",
    "    !wget https://raw.githubusercontent.com/FixelAlgorithmsTeam/FixelCourses/master/AIProgram/2024_02/DataManipulation.py\n",
    "    !wget https://raw.githubusercontent.com/FixelAlgorithmsTeam/FixelCourses/master/AIProgram/2024_02/DataVisualization.py\n",
    "    !wget https://raw.githubusercontent.com/FixelAlgorithmsTeam/FixelCourses/master/AIProgram/2024_02/DeepLearningPyTorch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courses Packages\n",
    "\n",
    "from DataVisualization import PlotConfusionMatrix, PlotLabelsHistogram, PlotMnistImages\n",
    "from DeepLearningPyTorch import TrainModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Auxiliary Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Vision Transformer (ViT)\n",
    "\n",
    "fd\n",
    "\n",
    "\n",
    "</br>\n",
    "\n",
    "* <font color='brown'>(**#**)</font> Notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# Data\n",
    "numSamplesTrain = 65_000\n",
    "numSamplesVal   = 5_000\n",
    "\n",
    "# Model\n",
    "patchSize = 7\n",
    "embedDim  = 96\n",
    "hiddenDim = 160\n",
    "numHeads  = 12 #<! `embedDim` must be divisible by `numHeads`\n",
    "numLayers = 6\n",
    "dropP     = 0.025 #<! Dropout Layer\n",
    "\n",
    "# Training\n",
    "batchSize = 256\n",
    "numWork   = 2 #<! Number of workers\n",
    "numEpochs = 80\n",
    "\n",
    "# Visualization\n",
    "numImg = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate / Load Data\n",
    "\n",
    "Load the [Fashion MNIST Data Set](https://github.com/zalandoresearch/fashion-mnist).  \n",
    "\n",
    "The _Fashion MNIST Data Set_ is considerably more challenging than the original MNIST though it is still no match to Deep Learning models.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The data set is available at [OpenML - Fashion MNIST](https://www.openml.org/search?type=data&id=40996).  \n",
    "  Yet it is not separated into the original _test_ and _train_ sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "\n",
    "mX, vY = fetch_openml('Fashion-MNIST', version = 1, return_X_y = True, as_frame = False, parser = 'auto')\n",
    "# mX, vY = fetch_openml('mnist_784', version = 1, return_X_y = True, as_frame = False, parser = 'auto') #<! For debugging (Gets 99.1%)\n",
    "vY = vY.astype(np.int_) #<! The labels are strings, convert to integer\n",
    "\n",
    "print(f'The features data shape: {mX.shape}')\n",
    "print(f'The labels data shape: {vY.shape}')\n",
    "print(f'The unique values of the labels: {np.unique(vY)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> The images are grayscale with size `28x28`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre Process Data\n",
    "\n",
    "mX = mX / 255.0\n",
    "mX = mX.astype(np.float32) #<! Convert to `Float32` for PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Data\n",
    "\n",
    "hF = PlotMnistImages(mX, vY, numImg)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of Labels\n",
    "\n",
    "vHa = PlotLabelsHistogram(vY, lClass = L_CLASSES_MNIST)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train & Validation Split\n",
    "\n",
    "numClass = len(np.unique(vY))\n",
    "\n",
    "mXTrain, mXVal, vYTrain, vYVal = train_test_split(mX, vY, test_size = numSamplesVal, train_size = numSamplesTrain, shuffle = True, stratify = vY)\n",
    "\n",
    "print(f'The training features data shape  : {mXTrain.shape}')\n",
    "print(f'The training labels data shape    : {vYTrain.shape}')\n",
    "print(f'The validation features data shape: {mXVal.shape}')\n",
    "print(f'The validation labels data shape  : {vYVal.shape}')\n",
    "print(f'The unique values of the labels   : {np.unique(vY)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch DataSet\n",
    "\n",
    "dsTrain  = torch.utils.data.TensorDataset(torch.tensor(np.reshape(mXTrain, (numSamplesTrain, 1, *T_IMG_SIZE_MNIST))), torch.tensor(vYTrain)) #<! -1 -> Infer\n",
    "dsVal    = torch.utils.data.TensorDataset(torch.tensor(np.reshape(mXVal, (numSamplesVal, 1, *T_IMG_SIZE_MNIST))), torch.tensor(vYVal))\n",
    "\n",
    "print(f'The training data set data shape  : {dsTrain.tensors[0].shape}')\n",
    "print(f'The validation data set data shape: {dsVal.tensors[0].shape}')\n",
    "print(f'The unique values of the labels   : {np.unique(dsTrain.tensors[1])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> The dataset is indexable (Subscriptable). It returns a tuple of the features and the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Element of the Data Set\n",
    "\n",
    "tX, valY = dsTrain[0]\n",
    "\n",
    "print(f'The features shape: {mX.shape}')\n",
    "print(f'The label value: {valY}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader\n",
    "\n",
    "dlTrain = torch.utils.data.DataLoader(dsTrain, shuffle = True, batch_size = 1 * batchSize, num_workers = numWork, persistent_workers = True)\n",
    "dlVal   = torch.utils.data.DataLoader(dsVal, shuffle = False, batch_size = 2 * batchSize, num_workers = numWork, persistent_workers = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Why is the size of the batch twice as big for the test dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate on the Loader\n",
    "# The first batch.\n",
    "tX, vY = next(iter(dlTrain)) #<! PyTorch Tensors\n",
    "\n",
    "print(f'The batch features dimensions: {tX.shape}')\n",
    "print(f'The batch labels dimensions: {vY.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ViT Building Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image to Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgToPatches(nn.Module):\n",
    "    def __init__(self, tuImgSize: Tuple[int, int], patchSize: int, inChans: int = 1, embedDim: int = 768) -> None:\n",
    "        super(ImgToPatches, self).__init__()\n",
    "        \n",
    "        self.imgHeight, self.imgWidth = tuImgSize\n",
    "        self.patchSize = patchSize\n",
    "        self.inChans = inChans\n",
    "        self.embedDim = embedDim\n",
    "        self.numPatches = (self.imgHeight // self.patchSize) * (self.imgWidth // self.patchSize)\n",
    "\n",
    "        # Per patch, linear model\n",
    "        self.projLayer = nn.Conv2d(inChans, embedDim, kernel_size = patchSize, stride = patchSize)\n",
    "\n",
    "    def forward(self, tX: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        # Only for Debugging\n",
    "        # batchSize, numChannel, imgHeight, imgWidth = tX.shape\n",
    "        # assert (imgHeight == self.imgHeight and imgWidth == self.imgWidth), f'Input image size ({imgHeight}, {imgWidth}) does not match model ({self.imgHeight}, {self.imgWidth}).'\n",
    "        # assert (imgHeight % self.patchSize == 0 and imgWidth % self.patchSize == 0), f'Image dimensions must be divisible by the patch size ({self.patchSize}, {self.patchSize}).'\n",
    "        # assert (numChannel == self.inChans), f'Input image channels ({numChannel}) does not match the expected number of channels.'\n",
    "\n",
    "        tX = self.projLayer(tX) #<! Shape: (B, embedDim, H // patchSize, W // patchSize)\n",
    "        tX = tX.flatten(2)      #<! Shape: (B, embedDim, numPatches)\n",
    "        tX = tX.transpose(1, 2) #<! Shape: (B, numPatches, embedDim)\n",
    "\n",
    "        return tX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image to Patches Function\n",
    "def ImgToPatches(tX: torch.Tensor, patchSize: int, /, *, flattenChannels: bool = True ) -> torch.Tensor:\n",
    "\n",
    "    batchSize, numChannel, imgHeight, imgWidth = tX.shape\n",
    "\n",
    "    tX = tX.reshape(batchSize, numChannel, imgHeight // patchSize, patchSize, imgWidth // patchSize, patchSize)\n",
    "    tX = tX.permute(0, 2, 4, 1, 3, 5) #<! (B, H', W', C, p_H, p_W)\n",
    "    tX = tX.flatten(1, 2)             #<! (B, H' * W', C, p_H, p_W)\n",
    "    if flattenChannels:\n",
    "        tX = tX.flatten(2, 4)         #<! (B, H' * W', C * p_H * p_W)\n",
    "\n",
    "    return tX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Patches\n",
    "\n",
    "tP = ImgToPatches(tX, 7, flattenChannels = False) #<! Shape: (B, numPatches, C, patchSize, patchSize)\n",
    "tP = tP[:4]\n",
    "print(f'The patches dimensions: {tP.shape}')\n",
    "\n",
    "hF, vHa = plt.subplots(tP.shape[0], 1, figsize = (14, 3))\n",
    "vHa     = vHa.flat\n",
    "hF.suptitle(f'Batch Images as Sequences of Patches ({patchSize}, {patchSize})')\n",
    "for ii, hA in enumerate(vHa):\n",
    "    tImgGrid = torchvision.utils.make_grid(tP[ii], nrow = 64, normalize = True, pad_value = 0.75)\n",
    "    tImgGrid = tImgGrid.permute(1, 2, 0)\n",
    "    hA.imshow(tImgGrid)\n",
    "    hA.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Encoder\n",
    "\n",
    "Throughout the Transformer layers, the CLS token interacts with all the image patch embeddings via the self attention mechanism.  \n",
    "This allows it to gather information and learn a global representation that encapsulates the context of the entire image.  \n",
    "After processing through the _Transformer Encoder_, the final hidden state corresponding to the CLS token is typically passed through a linear layer to predict the image's class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self: Self, embedDim: int, hiddenDim: int, numHeads: int, /, *, dropoutProb: float = 0.0) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.layerNorm01 = nn.LayerNorm(embedDim)\n",
    "        self.multiAttn = nn.MultiheadAttention(embedDim, numHeads)\n",
    "        self.layerNorm02 = nn.LayerNorm(embedDim)\n",
    "        self.ffNet = nn.Sequential(\n",
    "            nn.Linear(embedDim, hiddenDim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropoutProb),\n",
    "            nn.Linear(hiddenDim, embedDim),\n",
    "            nn.Dropout(dropoutProb),\n",
    "        )\n",
    "\n",
    "    def forward(self: Self, tX: torch.Tensor) -> torch.Tensor:\n",
    "        tZ = self.layerNorm01(tX)\n",
    "        tX = tX + self.multiAttn(tZ, tZ, tZ)[0] #<! Extract the Classification Token (CLS)\n",
    "        tX = tX + self.ffNet(self.layerNorm02(tX))\n",
    "        return tX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model\n",
    "\n",
    "The model is defined as a sequential model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self: Self,\n",
    "        patchSize: int,\n",
    "        numPatches: int,\n",
    "        numChannels: int,\n",
    "        numClasses: int,\n",
    "        /, *,\n",
    "        embedDim: int = 256,\n",
    "        hiddenDim: int = 512,\n",
    "        numHeads: int = 8,\n",
    "        numLayers: int = 6,\n",
    "        dropoutProb: float = 0.0,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Simplified Vision Transformer (ViT) model.\n",
    "\n",
    "        Input:\n",
    "            patchSize   - Number of pixels that the patches have per dimension.\n",
    "            numPatches  - Maximum number of patches an image can have.\n",
    "            numChannels - Number of channels of the input image.\n",
    "            numClasses  - Number of classes to predict.\n",
    "            embedDim    - Dimensionality of the input feature vectors to the Transformer.\n",
    "            hiddenDim   - Dimensionality of the hidden layer in the feed forward networks within the Transformer.\n",
    "            numHeads    - Number of heads to use in the Multi Head Attention block.\n",
    "            numLayers   - Number of layers to use in the Transformer.\n",
    "            dropoutProb - Probability of dropout to apply in the feed forward network and on the input encoding.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.patchSize = patchSize\n",
    "\n",
    "        # Layers / Networks\n",
    "        self.vitEmbeddder = nn.Linear(numChannels * (patchSize * patchSize), embedDim)\n",
    "        self.vitEncoder = nn.Sequential(\n",
    "            *(TransformerEncoder(embedDim, hiddenDim, numHeads, dropoutProb = dropoutProb) for _ in range(numLayers))\n",
    "        )\n",
    "        self.mlpHead = nn.Sequential(nn.LayerNorm(embedDim), nn.Linear(embedDim, numClasses))\n",
    "        self.dropout = nn.Dropout(dropoutProb)\n",
    "\n",
    "        # Parameters / Embeddings\n",
    "        self.clsToken = nn.Parameter(torch.randn(1, 1, embedDim)) #<! Learnable CLS Token\n",
    "        self.posEmbedding = nn.Parameter(torch.randn(1, 1 + numPatches, embedDim)) #<! Learnable Positional Encoding\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Preprocess input\n",
    "        x = ImgToPatches(x, self.patchSize)\n",
    "        B, T, _ = x.shape\n",
    "        x = self.vitEmbeddder(x)\n",
    "\n",
    "        # Add CLS token and positional encoding\n",
    "        cls_token = self.clsToken.repeat(B, 1, 1)\n",
    "        x = torch.cat([cls_token, x], dim=1)\n",
    "        x = x + self.posEmbedding[:, : T + 1]\n",
    "\n",
    "        # Apply Transforrmer\n",
    "        x = self.dropout(x)\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.vitEncoder(x)\n",
    "\n",
    "        # Perform classification prediction\n",
    "        cls = x[0]\n",
    "        out = self.mlpHead(cls)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Model\n",
    "oModel = VisionTransformer(\n",
    "    patchSize,\n",
    "    (T_IMG_SIZE_MNIST[0] // patchSize) * (T_IMG_SIZE_MNIST[1] // patchSize),\n",
    "    1,\n",
    "    numClass,\n",
    "    embedDim    = embedDim,\n",
    "    hiddenDim   = hiddenDim,\n",
    "    numHeads    = numHeads,\n",
    "    numLayers   = numLayers,\n",
    "    dropoutProb = dropP,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Summary\n",
    "torchinfo.summary(oModel, tX.shape, col_names = ['kernel_size', 'input_size', 'output_size', 'num_params'], device = 'cpu') #<! Added `kernel_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Model\n",
    "\n",
    "torchvista.trace_model(oModel, tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> The Encoder output has shape of `(17, batchSize, embedDim)`. Explain the `17`.\n",
    "\n",
    "<!-- The input sequence if length 17. The reason is the `patchSize = 7` and teh images is `(28, 28)` hence there are `4 * 4` patches and `CLS` token each of dimension 256. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "This section trains the model using different schedulers:\n",
    "\n",
    " - Updates the training function.\n",
    " - Updates the _epoch_ function to log information at mini batch level.\n",
    " - Create a class for a logger of TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Device\n",
    "\n",
    "runDevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') #<! The 1st CUDA device\n",
    "oModel = oModel.to(runDevice) #<! Transfer model to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Score Function\n",
    "\n",
    "hL = nn.CrossEntropyLoss()\n",
    "hS = MulticlassAccuracy(num_classes = len(L_CLASSES_MNIST), average = 'micro')\n",
    "hL = hL.to(runDevice) #<! Not required!\n",
    "hS = hS.to(runDevice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Optimizer & Scheduler\n",
    "\n",
    "oOpt = torch.optim.AdamW(oModel.parameters(), lr = 1e-3, betas = (0.9, 0.99), weight_decay = 1e-3) #<! Define optimizer\n",
    "oSch = torch.optim.lr_scheduler.OneCycleLR(oOpt, max_lr = 9.5e-3, total_steps = numEpochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "\n",
    "oRunModel, lTrainLoss, lTrainScore, lValLoss, lValScore, lLearnRate = TrainModel(oModel, dlTrain, dlVal, oOpt, numEpochs, hL, hS, oSch = oSch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training Phase\n",
    "\n",
    "hF, vHa = plt.subplots(nrows = 1, ncols = 3, figsize = (12, 5))\n",
    "vHa = vHa.flat\n",
    "\n",
    "hA = vHa[0]\n",
    "hA.plot(lTrainLoss, lw = 2, label = 'Train')\n",
    "hA.plot(lValLoss, lw = 2, label = 'Validation')\n",
    "hA.set_title('Binary Cross Entropy Loss')\n",
    "hA.set_xlabel('Epoch')\n",
    "hA.set_ylabel('Loss')\n",
    "hA.legend()\n",
    "\n",
    "hA = vHa[1]\n",
    "hA.plot(lTrainScore, lw = 2, label = 'Train')\n",
    "hA.plot(lValScore, lw = 2, label = 'Validation')\n",
    "hA.set_title('Accuracy Score')\n",
    "hA.set_xlabel('Epoch')\n",
    "hA.set_ylabel('Score')\n",
    "hA.legend()\n",
    "\n",
    "hA = vHa[2]\n",
    "hA.plot(lLearnRate, lw = 2)\n",
    "hA.set_title('Learn Rate Scheduler')\n",
    "hA.set_xlabel('Epoch')\n",
    "hA.set_ylabel('Learn Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis\n",
    "\n",
    "# Aggregate results for Train Set\n",
    "\n",
    "lYPred = []\n",
    "lY     = []\n",
    "\n",
    "for ii, (tX, vY) in enumerate(dlTrain):\n",
    "    # Move Data to Model's device\n",
    "    tX = tX.to(runDevice) #<! Lazy\n",
    "    vY = vY.to(runDevice) #<! Lazy\n",
    "        \n",
    "    with torch.inference_mode():\n",
    "        mZ = oModel(tX) #<! Model output\n",
    "        vYPred = torch.argmax(mZ, dim = 1)\n",
    "    \n",
    "    lYPred.append(vYPred.detach().cpu().numpy())\n",
    "    lY.append(vY.detach().cpu().numpy())\n",
    "\n",
    "vYPredTrain  = np.concat(lYPred, axis = 0)\n",
    "vYTruthTrain = np.concat(lY, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis\n",
    "\n",
    "# Aggregate results for Validation Set\n",
    "\n",
    "lYPred = []\n",
    "lY     = []\n",
    "\n",
    "for ii, (tX, vY) in enumerate(dlVal):\n",
    "    # Move Data to Model's device\n",
    "    tX = tX.to(runDevice) #<! Lazy\n",
    "    vY = vY.to(runDevice) #<! Lazy\n",
    "        \n",
    "    with torch.inference_mode():\n",
    "        mZ = oModel(tX) #<! Model output\n",
    "        vYPred = torch.argmax(mZ, dim = 1)\n",
    "    \n",
    "    lYPred.append(vYPred.detach().cpu().numpy())\n",
    "    lY.append(vY.detach().cpu().numpy())\n",
    "\n",
    "vYPredTest  = np.concat(lYPred, axis = 0)\n",
    "vYTruthTest = np.concat(lY, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis\n",
    "# Confusion Matrix\n",
    "\n",
    "hF, vHa = plt.subplots(nrows = 1, ncols = 2, figsize = (14, 6))\n",
    "\n",
    "hA, _ = PlotConfusionMatrix(vYTruthTrain, vYPredTrain, hA = vHa[0], lLabels = L_CLASSES_MNIST, xLabelRot = 45)\n",
    "hA.set_title(f'Train Data, Accuracy {np.mean(vYTruthTrain == vYPredTrain): 0.2%}')\n",
    "\n",
    "hA, _ = PlotConfusionMatrix(vYTruthTest, vYPredTest, hA = vHa[1], lLabels = L_CLASSES_MNIST, xLabelRot = 45)\n",
    "hA.set_title(f'Test Data, Accuracy {np.mean(vYTruthTest == vYPredTest): 0.2%}');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='green'>(**@**)</font> Create a CNN model with ~400,000 parameters and compare results. Explain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
