{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Fixel Algorithms](https://i.imgur.com/AqKHVZ0.png)](https://fixelalgorithms.gitlab.io)\n",
    "\n",
    "# AI Program\n",
    "\n",
    "## Machine Learning - Supervised Learning - Regression - Polynomial Fit with LASSO Regularization\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 1.0.000 | 24/03/2024 | Royi Avital | First version                                                      |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/AIProgram/2024_02/0048RegressorPolynomialFitLasso.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T09:30:06.492269Z",
     "start_time": "2022-02-02T09:30:06.220934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import lars_path, lasso_path\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Miscellaneous\n",
    "import math\n",
    "import os\n",
    "from platform import python_version\n",
    "import random\n",
    "import timeit\n",
    "\n",
    "# Typing\n",
    "from typing import Callable, Dict, List, Optional, Set, Tuple, Union\n",
    "\n",
    "# Visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "from ipywidgets import Dropdown, FloatSlider, interact, IntSlider, Layout, SelectionSlider\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought.\n",
    "\n",
    "Code Notations:\n",
    "\n",
    "```python\n",
    "someVar    = 2; #<! Notation for a variable\n",
    "vVector    = np.random.rand(4) #<! Notation for 1D array\n",
    "mMatrix    = np.random.rand(4, 3) #<! Notation for 2D array\n",
    "tTensor    = np.random.rand(4, 3, 2, 3) #<! Notation for nD array (Tensor)\n",
    "tuTuple    = (1, 2, 3) #<! Notation for a tuple\n",
    "lList      = [1, 2, 3] #<! Notation for a list\n",
    "dDict      = {1: 3, 2: 2, 3: 1} #<! Notation for a dictionary\n",
    "oObj       = MyClass() #<! Notation for an object\n",
    "dfData     = pd.DataFrame() #<! Notation for a data frame\n",
    "dsData     = pd.Series() #<! Notation for a series\n",
    "hObj       = plt.Axes() #<! Notation for an object / handler / function handler\n",
    "```\n",
    "\n",
    "### Code Exercise\n",
    "\n",
    " - Single line fill\n",
    "\n",
    "```python\n",
    "valToFill = ???\n",
    "```\n",
    "\n",
    " - Multi Line to Fill (At least one)\n",
    "\n",
    "```python\n",
    "# You need to start writing\n",
    "?????\n",
    "```\n",
    "\n",
    " - Section to Fill\n",
    "\n",
    "```python\n",
    "#===========================Fill This===========================#\n",
    "# 1. Explanation about what to do.\n",
    "# !! Remarks to follow / take under consideration.\n",
    "mX = ???\n",
    "\n",
    "?????\n",
    "#===============================================================#\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# %matplotlib inline\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "# Matplotlib default color palette\n",
    "lMatPltLibclr = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "# sns.set_theme() #>! Apply SeaBorn theme\n",
    "\n",
    "runInGoogleColab = 'google.colab' in str(get_ipython())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "FIG_SIZE_DEF    = (8, 8)\n",
    "ELM_SIZE_DEF    = 50\n",
    "CLASS_COLOR     = ('b', 'r')\n",
    "EDGE_COLOR      = 'k'\n",
    "MARKER_SIZE_DEF = 10\n",
    "LINE_WIDTH_DEF  = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courses Packages\n",
    "\n",
    "from DataVisualization import PlotRegressionData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Auxiliary Functions\n",
    "\n",
    "def PlotPolyFitLasso( vX: np.ndarray, vY: np.ndarray, vP: Optional[np.ndarray] = None, P: int = 1, λ: float = 0.0, \n",
    "                     numGridPts: int = 1001, hA: Optional[plt.Axes] = None, figSize: Tuple[int, int] = FIG_SIZE_DEF, \n",
    "                     markerSize: int = MARKER_SIZE_DEF, lineWidth: int = LINE_WIDTH_DEF, axisTitle: str = None ) -> None:\n",
    "\n",
    "    if hA is None:\n",
    "        hF, hA = plt.subplots(1, 2, figsize = figSize)\n",
    "    else:\n",
    "        hF = hA[0].get_figure()\n",
    "\n",
    "    numSamples = len(vY)\n",
    "\n",
    "    # Polyfit\n",
    "    if λ == 0:\n",
    "        # No Lasso (Classic Polyfit)\n",
    "        vW  = np.polyfit(vX, vY, P)\n",
    "    else:\n",
    "        # Lasso\n",
    "        mX   = PolynomialFeatures(degree = P, include_bias = False).fit_transform(vX[:, None])\n",
    "        oMdl = Lasso(alpha = λ, fit_intercept = True, max_iter = 500000).fit(mX, vY)\n",
    "        # Lasso coefficients\n",
    "        vW   = np.r_[oMdl.coef_[::-1], oMdl.intercept_]\n",
    "    \n",
    "    # R2 Score\n",
    "    vHatY = np.polyval(vW, vX)\n",
    "    R2    = r2_score(vY, vHatY)\n",
    "    \n",
    "    # Plot\n",
    "    xx  = np.linspace(np.around(np.min(vX), decimals = 1) - 0.1, np.around(np.max(vX), decimals = 1) + 0.1, numGridPts)\n",
    "    yy  = np.polyval(vW, xx)\n",
    "\n",
    "    hA[0].plot(vX, vY, '.r', ms = 10, label = '$y_i$')\n",
    "    hA[0].plot(xx, yy, 'b',  lw = 2,  label = '$\\hat{f}(x)$')\n",
    "    hA[0].set_title (f'P = {P}, R2 = {R2}')\n",
    "    hA[0].set_xlabel('$x$')\n",
    "    hA[0].set_xlim(left = xx[0], right = xx[-1])\n",
    "    hA[0].set_ylim(bottom = np.floor(np.min(vY)), top = np.ceil(np.max(vY)))\n",
    "    hA[0].grid()\n",
    "    hA[0].legend()\n",
    "    \n",
    "    hA[1].stem(vW[::-1], label = 'Estimated')\n",
    "    if vP is not None:\n",
    "        hA[1].stem(vP[::-1], linefmt = 'g', markerfmt = 'gD', label = 'Ground Truth')\n",
    "    hA[1].set_title('Coefficients')\n",
    "    hA[1].set_xlabel('$w$')\n",
    "    hA[1].set_ylim(bottom = -2, top = 6)\n",
    "    hA[1].legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Fit with Regularization\n",
    "\n",
    "The _Degrees of Freedom_ (DoF) of a _Polynomial_ model depends mainly on the polynomial degree.   \n",
    "One way to _regularize_ the model is by using the degree parameter.  \n",
    "Yet parameter is discrete hence harder to tune and the control the _Bias & Variance_ tradeoff.  \n",
    "\n",
    "The model of smooth regularization is given by:\n",
    "\n",
    "$$ \\arg \\min_{\\boldsymbol{w}} \\frac{1}{2} {\\left\\| X \\boldsymbol{w} - \\boldsymbol{y} \\right\\|}_{2}^{2} + \\lambda R \\left( \\boldsymbol{w} \\right) $$\n",
    "\n",
    "Where $R \\left( \\cdot \\right)$ is the regularizer and $\\lambda \\geq 0$ is the continuous regularization parameter where higher value means stronger regularization.  \n",
    "The properties of the solution will be determined by the regularizer (Sparse, Low Values, etc...).  \n",
    "The continuous regularization parameter allows smoother abd more finely tuned regularization.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The model above can, in many cases, be interpreted as a _prior_ on the values of the parameters as in _Bayesian Estimation_ context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# Data Generation\n",
    "numSamples  = 50\n",
    "noiseStd    = 0.3\n",
    "vP = np.array([0.5, 2, 5])\n",
    "\n",
    "# Model\n",
    "polyDeg = 2\n",
    "λ       = 0.1\n",
    "\n",
    "# Data Visualization\n",
    "gridNoiseStd = 0.05\n",
    "numGridPts = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate / Load Data\n",
    "\n",
    "In the following we'll generate data according to the following model:\n",
    "\n",
    "$$ y_{i} = f \\left( x_{i} \\right) + \\epsilon_{i} $$\n",
    "\n",
    "Where\n",
    "\n",
    "$$ f \\left( x \\right) = \\frac{1}{2} x^{2} + 2x + 5 $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Data Generating Function\n",
    "\n",
    "def f( vX: np.ndarray, vP: np.ndarray ) -> np.ndarray:\n",
    "    # return 0.25 * (vX ** 2) + 2 * vX + 5\n",
    "    return np.polyval(vP, vX)\n",
    "\n",
    "\n",
    "hF = lambda vX: f(vX, vP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Data\n",
    "\n",
    "vX = np.linspace(-2, 2, numSamples, endpoint = True) + (gridNoiseStd * np.random.randn(numSamples))\n",
    "vN = noiseStd * np.random.randn(numSamples)\n",
    "vY = hF(vX) + vN\n",
    "\n",
    "print(f'The features data shape: {vX.shape}')\n",
    "print(f'The labels data shape: {vY.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Data\n",
    "\n",
    "PlotRegressionData(vX, vY)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Polyfit Regressor with LASSO Regularization\n",
    "\n",
    "The ${L}_{1}$ regularized PolyFit optimization problem is given by:\n",
    "\n",
    "$$ \\arg \\min_{\\boldsymbol{w}} \\frac{1}{2} {\\left\\| X \\boldsymbol{w} - \\boldsymbol{y} \\right\\|}_{2}^{2} + \\lambda {\\left\\| \\boldsymbol{w} \\right\\|}_{1} $$\n",
    "\n",
    "Where\n",
    "\n",
    "$$\n",
    "\\boldsymbol{X} = \\begin{bmatrix} 1 & x_{1} & x_{1}^{2} & \\cdots & x_{1}^{p} \\\\\n",
    "1 & x_{2} & x_{2}^{2} & \\cdots & x_{2}^{p} \\\\\n",
    "\\vdots & \\vdots & \\vdots &  & \\vdots \\\\\n",
    "1 & x_{N} & x_{N}^{2} & \\cdots & x_{N}^{p}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This regularization is called [_Least Absolute Shrinkage and Selection Operator_](https://en.wikipedia.org/wiki/Lasso_(statistics)) (LASSO).  \n",
    "Since the ${L}_{1}$ norm promotes sparsity we basically have a feature selector built in.\n",
    "\n",
    "<!-- ![](https://i.imgur.com/GnvWPSp.png) -->\n",
    "<!-- ![](https://i.postimg.cc/hP7yH6gN/Gnv-WPSp-1.png) -->\n",
    "![](https://i.imgur.com/KTUNSbd.png)\n",
    "<!-- ![](https://i.postimg.cc/SN7TS82G/GnvWPSp.png) -->\n",
    "\n",
    "The above compares the solutions of\n",
    " - $\\arg \\min_{\\boldsymbol{w}} {\\left\\| \\boldsymbol{w} \\right\\|}_{1} \\; \\text{ subject to } \\boldsymbol{A} \\boldsymbol{w} = \\boldsymbol{y}$ (Left, Often Sparse).\n",
    " - $\\arg \\min_{\\boldsymbol{w}} {\\left\\| \\boldsymbol{w} \\right\\|}_{2}^{2} \\; \\text{ subject to } \\boldsymbol{A} \\boldsymbol{w} = \\boldsymbol{y}$ (Right, Rarely).\n",
    "\n",
    "* <font color='brown'>(**#**)</font> [The L1 Regularization Encourage Coefficients to Shrink to Zero](https://maitbayev.github.io/posts/why-l1-loss-encourage-coefficients-to-shrink-to-zero).\n",
    "* <font color='brown'>(**#**)</font> [A visual explanation for regularization of linear models](https://explained.ai/regularization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial Fit with Lasso Regularization\n",
    "\n",
    "mX         = PolynomialFeatures(degree = polyDeg, include_bias = False).fit_transform(vX[:, None]) #<! Build the model matrix\n",
    "oLinRegL1  = Lasso(alpha = λ, fit_intercept = True, max_iter = 30000).fit(mX, vY)\n",
    "vW         = np.r_[oLinRegL1.coef_[::-1], oLinRegL1.intercept_]\n",
    "\n",
    "# Display the weights\n",
    "vW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Regressor for Various Regularization (λ) Values\n",
    "\n",
    "Let's see the effect of the strength of the regularization on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hPolyFitLasso = lambda λ: PlotPolyFitLasso(vX, vY, vP = vP, P = 15, λ = λ)\n",
    "lamSlider = FloatSlider(min = 0, max = 1, step = 0.001, value = 0, readout_format = '.4f', layout = Layout(width = '30%'))\n",
    "interact(hPolyFitLasso, λ = lamSlider)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> How do you expect the ${R}^{2}$ score to behave with increasing $\\lambda$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Path for Feature Importance\n",
    "\n",
    "The _rise_ of a feature is similar to the correlation of the feature.  \n",
    "Hence we cen use the _Lasso Path_ for feature selection / significance.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The LASSO checks the conditional correlation. Namely the specific combination of the features.  \n",
    "  While selection based on correlation is based on marginal correlation. Namely the value of specific feature (Its mean or other statistics).  \n",
    "  In practice, LASSO potentially can make a good selection when there are inter correlations between the features.\n",
    "* <font color='brown'>(**#**)</font> See [Partial / Conditional Correlation vs. Marginal Correlation](https://stats.stackexchange.com/questions/77318)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data from https://gist.github.com/seankross/a412dfbd88b3db70b74b\n",
    "# mpg - Miles per Gallon\n",
    "# cyl - # of cylinders\n",
    "# disp - displacement, in cubic inches\n",
    "# hp - horsepower\n",
    "# drat - driveshaft ratio\n",
    "# wt - weight\n",
    "# qsec - 1/4 mile time; a measure of acceleration\n",
    "# vs - 'V' or straight - engine shape\n",
    "# am - transmission; auto or manual\n",
    "# gear - # of gears\n",
    "# carb - # of carburetors\n",
    "dfMpg = pd.read_csv('https://github.com/FixelAlgorithmsTeam/FixelCourses/raw/master/DataSets/mtcars.csv')\n",
    "dfMpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for Analysis\n",
    "# The target data is the fuel consumption (`mpg`).\n",
    "dfX = dfMpg.drop(columns = ['model', 'mpg'], inplace = False)\n",
    "dfX = (dfX - dfX.mean()) / dfX.std() #<! Normalize\n",
    "dsY = dfMpg['mpg'].copy() #<! Data Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LASSO Path Analysis\n",
    "\n",
    "alphasPath, coefsPath, *_ = lasso_path(dfX, dsY)\n",
    "# alphasPath, coefsPath, *_ = lars_path(dfX, dsY, method = 'lasso')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the LASSO Path\n",
    "\n",
    "hF, hA = plt.subplots(figsize = (16, 8))\n",
    "hA.plot(alphasPath, np.abs(coefsPath.T), lw = 2, label = dfX.columns.to_list())\n",
    "hA.set_title('The Lasso Path')\n",
    "hA.set_xlabel('$\\lambda$')\n",
    "hA.set_ylabel('Coefficient Value (${w}_{i}$)')\n",
    "hA.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> Feature selection can be an objective on its own. For instance, think of a questionary of insurance company to assess the risk of the customer.  \n",
    "  Achieving the same accuracy in the risk assessment with less questions (Features) is valuable on its own.  \n",
    "  See [Why LASSO for Feature Selection](https://stats.stackexchange.com/questions/367155).\n",
    "* <font color='brown'>(**#**)</font> Usually it is better to use the _correlation_ method for _feature filtering_, dropping features which are highly correlated. While LASSO like methods for _feature selection_.\n",
    "* <font color='brown'>(**#**)</font> Correlation based _Feature Engineering_ / _Feature Selection_ are limited in the sense they measure linear function as the connection (Second moment data).  \n",
    "  One way to measure higher level moments and non linear connection is by the [_Mutual Information_](https://en.wikipedia.org/wiki/Mutual_information).  \n",
    "  Specifically by _Mutual Information Coefficient_. See [Vadim Arzamasov - An Undeservedly Forgotten Correlation Coefficient](https://scribe.rip/86245ccb774c)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
