{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Fixel Algorithms](https://i.imgur.com/AqKHVZ0.png)](https://fixelalgorithms.gitlab.io/)\n",
    "\n",
    "# AI Program\n",
    "\n",
    "## Exercise 0005 - Classification\n",
    "\n",
    "Feature engineering for color classification.\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 1.0.000 | 17/03/2024 | Royi Avital | First version                                                      |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/AIProgram/2024_02/Exercise0005.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T09:30:06.492269Z",
     "start_time": "2022-02-02T09:30:06.220934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Miscellaneous\n",
    "import gdown\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Typing\n",
    "from typing import Callable, Dict, List, Optional, Set, Tuple, Union\n",
    "\n",
    "# Visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought.\n",
    "\n",
    "Code Notations:\n",
    "\n",
    "```python\n",
    "someVar    = 2; #<! Notation for a variable\n",
    "vVector    = np.random.rand(4) #<! Notation for 1D array\n",
    "mMatrix    = np.random.rand(4, 3) #<! Notation for 2D array\n",
    "tTensor    = np.random.rand(4, 3, 2, 3) #<! Notation for nD array (Tensor)\n",
    "tuTuple    = (1, 2, 3) #<! Notation for a tuple\n",
    "lList      = [1, 2, 3] #<! Notation for a list\n",
    "dDict      = {1: 3, 2: 2, 3: 1} #<! Notation for a dictionary\n",
    "oObj       = MyClass() #<! Notation for an object\n",
    "dfData     = pd.DataFrame() #<! Notation for a data frame\n",
    "dsData     = pd.Series() #<! Notation for a series\n",
    "hObj       = plt.Axes() #<! Notation for an object / handler / function handler\n",
    "```\n",
    "\n",
    "### Code Exercise\n",
    "\n",
    " - Single line fill\n",
    "\n",
    " ```python\n",
    " vallToFill = ???\n",
    " ```\n",
    "\n",
    " - Multi Line to Fill (At least one)\n",
    "\n",
    " ```python\n",
    " # You need to start writing\n",
    " ????\n",
    " ```\n",
    "\n",
    " - Section to Fill\n",
    "\n",
    "```python\n",
    "#===========================Fill This===========================#\n",
    "# 1. Explanation about what to do.\n",
    "# !! Remarks to follow / take under consideration.\n",
    "mX = ???\n",
    "\n",
    "???\n",
    "#===============================================================#\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# %matplotlib inline\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "# sns.set_theme() #>! Apply SeaBorn theme\n",
    "\n",
    "runInGoogleColab = 'google.colab' in str(get_ipython())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "DATA_SET_FILE_URL   = r'https://drive.google.com/uc?export=download&confirm=9iBg&id=17-IWjWCPuXMSO0uUWKVDIO-NT38SoB8J'\n",
    "DATA_SET_FILE_NAME  = 'ColorClassification.zip'\n",
    "\n",
    "TEST_DATA_FILE_NAME  = 'TestData.mat'\n",
    "TRAIN_DATA_FILE_NAME = 'TrainData.mat'\n",
    "\n",
    "L_CLASSES   = ['Red', 'Green', 'Blue']\n",
    "IMG_SIZE    = [100, 100]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Course Packages\n",
    "\n",
    "from DataVisualization import PlotConfusionMatrix, PlotLabelsHistogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Auxiliary Functions\n",
    "\n",
    "def PlotImage(vX, imgClass = None, imgSize = IMG_SIZE, hA = None):\n",
    "\n",
    "    mI = np.reshape(vX, (imgSize[0], imgSize[1], 3), order = 'F') #<! Data is coming from MATLAB\n",
    "\n",
    "    if hA is None:\n",
    "        hF, hA = plt.subplots(figsize = (4, 4))\n",
    "    \n",
    "    hA.imshow(mI)\n",
    "    hA.tick_params(axis = 'both', left = False, top = False, right = False, bottom = False, labelleft = False, labeltop = False, labelright = False, labelbottom = False)\n",
    "\n",
    "    if imgClass is not None:\n",
    "        hA.set_title('Image Class: {imgClass}')\n",
    "\n",
    "    return hA\n",
    "\n",
    "\n",
    "def PlotImages(mX: np.ndarray, vY: np.ndarray, numRows: int, numCols: int, lClass = L_CLASSES, hF = None):\n",
    "\n",
    "    numSamples  = mX.shape[0]\n",
    "    numPx       = mX.shape[1]\n",
    "\n",
    "    numImg = numRows * numCols\n",
    "\n",
    "    tFigSize = (numRows * 3, numCols * 3)\n",
    "\n",
    "    if hF is None:\n",
    "        hF, hA = plt.subplots(nrows = numRows, ncols = numCols, figsize = tFigSize)\n",
    "    else:\n",
    "        hA = hF.axis\n",
    "    \n",
    "    hA = np.atleast_1d(hA) #<! To support numImg = 1\n",
    "    hA = hA.flat\n",
    "\n",
    "    vIdx = np.random.choice(numSamples, numImg, replace = False)\n",
    "    \n",
    "    for kk in range(numImg):\n",
    "        imgIdx  = vIdx[kk]\n",
    "\n",
    "        PlotImage(mX[imgIdx], hA = hA[kk])\n",
    "        hA[kk].set_title(f'Index = {imgIdx}, Label = {lClass[vY[imgIdx]]}')\n",
    "    \n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "This exercise introduces:\n",
    "\n",
    " - Concept of _Features Transform_ to reduce the amount of data.\n",
    " - Optimizing a classifier by the accuracy score.\n",
    "\n",
    "\n",
    "* <font color='brown'>(**#**)</font> While in this case the _dimensionality reduction_ of data is done manually by domain knowledge, later in the course we'll learn ML based methods.\n",
    "* <font color='brown'>(**#**)</font> There is more than one way to implement the exercise. Feel free to wander.\n",
    "\n",
    "In this exercise we'll work on images which are composed in a matrix.  \n",
    "Each image is of size `100 x 100 x 3` yet it is spread in a _column stack_ fashion as a row in the data matrix.  \n",
    "The train data has `2700` images in a matrix, so the matrix size is `2_700 x 30_000`.\n",
    "\n",
    "The objective is being able to classify the color of the image: `Red: 0`, `Green: 1`, `Blue: 2`.  \n",
    "The image colors isn't uniform but contains many colors, but the idea is to identify the dominant color.  \n",
    "The concept of _red_ / _green_ / _blue_ is not a single color but the _family_ of colors.\n",
    "\n",
    "1. Load data into `mXTrain`, `vYTrain`, `mXTest`, `vYTest`.  \n",
    "   Data is downloaded and loaded by the notebook.  \n",
    "   **Make sure internet connection is available**.\n",
    "2. Extract features from the images for the training data.  \n",
    "   Analyze the features using EDA and select the subset which you think will work best.\n",
    "3. Train a Kernel SVM (`rbf`) model on the data.  \n",
    "   Optimize the `C` and `gamma` parameters for accuracy using grid search.\n",
    "4. Extract the same features from the test images into test data.\n",
    "5. Plot the _confusion matrix_ of the best model on the test data.\n",
    "\n",
    "Optimize features (repeat if needed) to get accuracy of at least `85%` per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Think of the parameters to optimize per model (See above).\n",
    "# 2. Select the set to optimize over.\n",
    "# 3. Set the number of folds in the cross validation.\n",
    "?????\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate / Load Data\n",
    "\n",
    "Load the classification data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "\n",
    "if not (os.path.isfile(TEST_DATA_FILE_NAME) and os.path.isfile(TRAIN_DATA_FILE_NAME)):\n",
    "    # Delete files if only one exists\n",
    "    if os.path.isfile(TEST_DATA_FILE_NAME):\n",
    "        os.remove(TEST_DATA_FILE_NAME)\n",
    "    if os.path.isfile(TRAIN_DATA_FILE_NAME):\n",
    "        os.remove(TRAIN_DATA_FILE_NAME)\n",
    "    if os.path.isfile(DATA_SET_FILE_NAME):\n",
    "        os.remove(DATA_SET_FILE_NAME)\n",
    "    gdown.download(DATA_SET_FILE_URL, DATA_SET_FILE_NAME)\n",
    "    shutil.unpack_archive(DATA_SET_FILE_NAME)\n",
    "    os.remove(DATA_SET_FILE_NAME)\n",
    "\n",
    "dTestData  = sp.io.loadmat(TEST_DATA_FILE_NAME)\n",
    "dTrainData = sp.io.loadmat(TRAIN_DATA_FILE_NAME)\n",
    "\n",
    "mXTrain, vYTrain    = dTrainData['mX'], np.squeeze(dTrainData['vY'])\n",
    "mXTest, vYTest      = dTestData['mX'], np.squeeze(dTestData['vY'])\n",
    "\n",
    "print(f'The number of training data samples: {mXTrain.shape[0]}')\n",
    "print(f'The number of training features per sample: {mXTrain.shape[1]}') \n",
    "\n",
    "\n",
    "print(f'The number of test data samples: {mXTest.shape[0]}')\n",
    "print(f'The number of test features per sample: {mXTest.shape[1]}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Data\n",
    "\n",
    "A useful plot for multi features data is the _pair plot_ (See `SeaBorn`'s [`pairplot()`](https://seaborn.pydata.org/generated/seaborn.pairplot.html)).  \n",
    "The pair plot easily gives a view on the:\n",
    "\n",
    "1. Relation between each pair of the features.\n",
    "2. Distribution of each feature.\n",
    "\n",
    "It is an important tool for observation of the features and their interrelation.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> You may read on it in [Data Exploration and Visualization with SeaBorn Pair Plots](https://scribe.rip/40e6d3450f6d).\n",
    "* <font color='brown'>(**#**)</font> The plots matrix is $n \\times n$ where $n$ is the number of features. Hence it is not feasible for $n \\gg 1$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Data\n",
    "\n",
    "# Train Data\n",
    "PlotImages(mXTrain, vYTrain, 3, 3, lClass = L_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Data\n",
    "\n",
    "# Test Data\n",
    "PlotImages(mXTest, vYTest, 3, 3, lClass = L_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of Classes\n",
    "\n",
    "# Train\n",
    "hA = PlotLabelsHistogram(vYTrain, lClass = L_CLASSES)\n",
    "hA.set_title(hA.get_title() + ' - Train Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of Classes\n",
    "\n",
    "# Test\n",
    "hA = PlotLabelsHistogram(vYTest, lClass = L_CLASSES)\n",
    "hA.set_title(hA.get_title() + ' - Test Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Is the data balanced or imbalanced?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data and Feature Engineering / Extraction\n",
    "\n",
    "The vector of values doesn't fit, as is, for classification with SVM.  \n",
    "It misses a lot of the information given in the structure of the image or a color pixel.  \n",
    "In our case, the important thing is to give the classifier information about the structure of color, a vector of 3 values: `[r, g, b]`.  \n",
    "Yet, the classifier input is limited to a list of values. This is where the concept of metric comes into play.  \n",
    "\n",
    "We need to create information about distance between colors.  \n",
    "We also need to extract features to represent the colors in the image.\n",
    "\n",
    "In this section the task are:\n",
    "\n",
    "1. Implement functions to extract features from the data.\n",
    "2. Arrange the features in a _matrix_ / _data frame_ for processing.\n",
    "3. Explore the features using _SeaBorn_. Specifically if the features extracts meaningful information.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> Don't include _test data_ in the analysis for feature extraction. Other wise, a data leakage will happen.\n",
    "\n",
    "### Ideas for Features\n",
    "\n",
    "1. The distance between the the _mean_ / _median_ / _mode_ color of the image to the per _mean_ / _median_ / _mode_ color per class.\n",
    "2. The distance between the quantized histogram of `R` / `G` / `B` color channels of the image to the class.\n",
    "3. The distance of the mean color at the center of the image to the mean color of the class.\n",
    "4. The channel with the maximum value (Is this a continuous value? Does it fit the SVM model?).\n",
    "5. Use of the _HSL_ color space.\n",
    "\n",
    "\n",
    "* <font color='brown'>(**#**)</font> You're encouraged to think on more features!\n",
    "* <font color='brown'>(**#**)</font> Pay attention to dimensionality fo the data. For instance, how do you define the _median color_?\n",
    "* <font color='brown'>(**#**)</font> For simplicity we use the RGB Color Space. Yet color distance might be better calculated in other color spaces (See LAB for instance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for Feature Extraction\n",
    "#===========================Fill This===========================#\n",
    "# 1. Some function work per image, some per the whole data (Comparing stuff).\n",
    "# 2. You may want to extract statistical information from the training data and use metric between a single image and the statistical data.\n",
    "\n",
    "?????\n",
    "\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features Matrix\n",
    "# Function to Create the Features Matrix Given the RAW Data.\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Create a function that given the RAW data and other parameters calculates the feature matrix.\n",
    "# 2. It should handle both Training and Test data, yet don't pass info between.\n",
    "# 3. The output dimensions should match the number of samples of the input and the number of features: (numSamples, numFeatures).\n",
    "# 4. Make sure the order of processing keeps it aligned with the labels vector.\n",
    "\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Features\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Calculate the Features Matrix for the Training Data Set.\n",
    "# 2. Name the features matrix `mF`.\n",
    "?????\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Analysis\n",
    "\n",
    "In this section the relation between the features and the labels is analyzed.  \n",
    "You should visualize / calculate measures which imply the features makes the classes identifiable.\n",
    "\n",
    "#### Ideas for Analysis\n",
    "\n",
    "1. Display the histogram / density of each feature by the label of sample.\n",
    "2. Display the correlation between the feature to the class value (Pay attention this is a mix of continuous values and categorical values).\n",
    "\n",
    "* <font color='brown'>(**#**)</font> You may find SeaBorn's `kdeplot()` useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Visualize Features\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Visualize the distribution of the features per class.\n",
    "# 2. You're after features which separate the different classes (Least common values with other classes).\n",
    "\n",
    "?????\n",
    "\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize Classifiers\n",
    "\n",
    "In this section we'll train a Kernel SVM model with optimized hyper parameters: `C` and `gamma`.  \n",
    "The score should be the regular accuracy.\n",
    "\n",
    "1. Build the dictionary of parameters for the grid search.\n",
    "2. Construct the grid search object (`GridSearchCV`).\n",
    "3. Optimize the hyper parameters by the `fit()` method of the grid search object.\n",
    "\n",
    "* <font color='red'>(**?**)</font> Why is the accuracy a reasonable score in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search Object\n",
    "# Hyper parameter optimization by a combined grid search and cross validation.\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Construct the Grid Search object.\n",
    "# 2. Set the parameters to iterate over and their values.\n",
    "dParams = ???\n",
    "#===============================================================#\n",
    "\n",
    "oGsSvc = GridSearchCV(estimator = SVC(kernel = 'rbf'), param_grid = dParams, scoring = None, cv = numFold, verbose = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize Hyper Parameters\n",
    "# Apply the grid search.\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Apply the grid search phase.\n",
    "oGsSvc = ???\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix on Test Data \n",
    "\n",
    "In this section we'll test the model on the test data.\n",
    "\n",
    "1. Extract the best estimator from the grid search.\n",
    "2. If needed, fit it to the train data.\n",
    "3. Calculate the test set features. Make sure to avoid data leakage.\n",
    "4. Display the _confusion matrix_.\n",
    "\n",
    "The objective is to get at least `85%` accuracy per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Best Model\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Get the best model with the optimized hyper parameters.\n",
    "bestModel = ???\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Does the best model need a refit on data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Set Features\n",
    "# Calculate the test data set features.\n",
    "# Pay attention to not use of leak of data from the test set to the model / features.\n",
    "# One way to obey this is assume you got the test data one by one.\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Features of the Test Data.\n",
    "mFTest = ???\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "hF, hA = plt.subplots(figsize = (10, 10))\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Plot the Confusion Matrix.\n",
    "hA, mConfMat = ???\n",
    "#===============================================================#\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> If results are good, can you spot the dominant feature for them if there is?\n",
    "* <font color='blue'>(**!**)</font> If there are errors, analyze at least one of each class with error.\n",
    "* <font color='green'>(**@**)</font> Check results with a single feature: The channel with the highest mean value."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
