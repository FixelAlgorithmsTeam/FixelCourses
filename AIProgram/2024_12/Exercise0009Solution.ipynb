{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Fixel Algorithms](https://i.imgur.com/AqKHVZ0.png)](https://fixelalgorithms.gitlab.io)\n",
    "\n",
    "# AI Program\n",
    "\n",
    "## Exercise 0008 - Deep Learning - Convolution NN for Image Classification\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 1.0.000 | 20/05/2024 | Royi Avital | First version                                                      |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/AIProgram/2024_02/Exercise0008.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T09:30:06.492269Z",
     "start_time": "2022-02-02T09:30:06.220934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn            as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchinfo\n",
    "from torchmetrics.classification import BinaryAccuracy\n",
    "import torchvision\n",
    "from torchvision.transforms import v2 as TorchVisionTrns\n",
    "\n",
    "# Miscellaneous\n",
    "import copy\n",
    "import gdown\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import urllib.request\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "# Typing\n",
    "from typing import Any, Callable, Dict, List, Optional, Self, Set, Tuple, Union\n",
    "\n",
    "# Visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought.\n",
    "\n",
    "Code Notations:\n",
    "\n",
    "```python\n",
    "someVar    = 2; #<! Notation for a variable\n",
    "vVector    = np.random.rand(4) #<! Notation for 1D array\n",
    "mMatrix    = np.random.rand(4, 3) #<! Notation for 2D array\n",
    "tTensor    = np.random.rand(4, 3, 2, 3) #<! Notation for nD array (Tensor)\n",
    "tuTuple    = (1, 2, 3) #<! Notation for a tuple\n",
    "lList      = [1, 2, 3] #<! Notation for a list\n",
    "dDict      = {1: 3, 2: 2, 3: 1} #<! Notation for a dictionary\n",
    "oObj       = MyClass() #<! Notation for an object\n",
    "dfData     = pd.DataFrame() #<! Notation for a data frame\n",
    "dsData     = pd.Series() #<! Notation for a series\n",
    "hObj       = plt.Axes() #<! Notation for an object / handler / function handler\n",
    "```\n",
    "\n",
    "### Code Exercise\n",
    "\n",
    " - Single line fill\n",
    "\n",
    "```python\n",
    "vallToFill = ???\n",
    "```\n",
    "\n",
    " - Multi Line to Fill (At least one)\n",
    "\n",
    "```python\n",
    "# You need to start writing\n",
    "?????\n",
    "```\n",
    "\n",
    " - Section to Fill\n",
    "\n",
    "```python\n",
    "#===========================Fill This===========================#\n",
    "# 1. Explanation about what to do.\n",
    "# !! Remarks to follow / take under consideration.\n",
    "mX = ???\n",
    "\n",
    "?????\n",
    "#===============================================================#\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# %matplotlib inline\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "sns.set_theme() #>! Apply SeaBorn theme\n",
    "\n",
    "runInGoogleColab = 'google.colab' in str(get_ipython())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "DATA_FOLDER_PATH = 'Data'\n",
    "\n",
    "# Data Set Links: \n",
    "# - MicroSoft Kaggle Cats and Dogs Dataset - https://www.microsoft.com/en-us/download/details.aspx?id=54765 (Dog 11702, Cat 666 might be corrupted)\n",
    "# - Kaggle Competition - Dogs vs. Cats - https://www.kaggle.com/c/dogs-vs-cats\n",
    "DATA_SET_URL            = 'https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip'\n",
    "DATA_SET_FILE_NAME      = 'CatsDogs.zip'\n",
    "DATA_SET_FOLDER_NAME    = 'CatsDogs'\n",
    "\n",
    "D_CLASSES  = {0: 'Cat', 1: 'Dog'}\n",
    "L_CLASSES  = ['Cat', 'Dog']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Course Packages\n",
    "\n",
    "from DataManipulation import DownloadUrl\n",
    "from DeepLearningPyTorch import TrainModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Auxiliary Functions\n",
    "\n",
    "# Class to handle a folder with no labels: Test\n",
    "\n",
    "from torchvision.datasets.folder import IMG_EXTENSIONS, pil_loader\n",
    "\n",
    "class TestDataSet( torchvision.datasets.VisionDataset ):\n",
    "    def __init__(self, root: str = None, transforms: Callable[..., Any] | None = None, transform: Callable[..., Any] | None = None, target_transform: Callable[..., Any] | None = None) -> None:\n",
    "        super().__init__(root, transforms, transform, target_transform)\n",
    "\n",
    "\n",
    "        lF = os.listdir(root)\n",
    "        lFiles = [fileName for fileName in lF if (os.path.isfile(os.path.join(root, fileName)) and (os.path.splitext(os.path.join(root, fileName))[1] in IMG_EXTENSIONS))]\n",
    "\n",
    "        self.lFiles = lFiles\n",
    "        self.loader = pil_loader\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        \n",
    "        return len(self.lFiles)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Any:\n",
    "        \n",
    "        imgSample =  self.loader(os.path.join(self.root, self.lFiles[index]))\n",
    "        if self.transform is not None:\n",
    "            imgSample = self.transform(imgSample)\n",
    "        \n",
    "        return imgSample\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Cats vs. Dogs\n",
    "\n",
    "This exercises builds a model based on _Convolutional Neural Network_ for _Binary Image Classification_.  \n",
    "The data set is from the [Kaggle - Dogs vs. Cats](https://www.kaggle.com/c/dogs-vs-cats) competition.  \n",
    "The objective is to classify an image either as a _Cat_ or a _Dog_.  \n",
    "\n",
    "The challenge in this data set is working with images with different dimensions.\n",
    "\n",
    "The data contains 25,000 RGB images with different dimensions.  \n",
    "\n",
    "Tasks:\n",
    " - Download and arrange data properly.\n",
    " - Split data into 22,500 train samples and 2,500 validation samples.\n",
    " - Build a dataset and data loader.  \n",
    "   The data loader must support the case of different image dimensions.\n",
    " - Build a parameterized model (Layers, Activations, etc...).\n",
    " - Build an optimizer and a scheduler.\n",
    " - Build a training loop to optimize hyper parameters.\n",
    "\n",
    "Tips:\n",
    " - Use random transformation to enrich the data set.  \n",
    "   See [`torchvision.transforms.RandomRotation`](https://pytorch.org/vision/main/generated/torchvision.transforms.RandomRotation.html) as an examples.  \n",
    "   This is called [_Data Augmentation_](https://en.wikipedia.org/wiki/Data_augmentation).\n",
    " - Use [`torchvision.datasets.ImageFolder`](https://pytorch.org/vision/main/generated/torchvision.datasets.ImageFolder.html) to load the data easily.\n",
    " - Use [`torch.utils.data.random_split`](https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split) to split the data set.\n",
    " - You may handle the different image dimensions by:\n",
    "    - Build the model in a manner which is dimension insensitive.\n",
    "    - Transform the image into a pre defined size (Use padding to keep aspect ratio).\n",
    " - \n",
    "\n",
    "**Objective**: Above 96% accuracy on the validation set (See the [Competition Leader Board](https://www.kaggle.com/c/dogs-vs-cats/leaderboard)).\n",
    "\n",
    "* <font color='brown'>(**#**)</font> One may use single output with the Binary Cross Entropy Loss: [`BCELoss`](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html), [`BCEWithLogitsLoss`](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Different Images Size\n",
    "\n",
    "There are many options to handle different images size in the same batch / data.  \n",
    "There are 2 main approaches:\n",
    "\n",
    "1. Build a Model Insensitive to Image Input  \n",
    "   Build a model which assumes no knowledge of the image size.  \n",
    "   Usually it is built by Convolution Layers and Adaptive Pooling so the output is a function of the known number of channels and not the input.  \n",
    "   The challenge is to handle loading and processing the data.  \n",
    "   It usually done by padding all images to the size of the largest image.\n",
    "2. Adapt the Data  \n",
    "   Apply some combination of padding, resize and crop to have the same image size.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> For FCN models (1) one could also set the batch size to 1 so each image is on its own. Yet it hurts efficiency greatly.\n",
    "* <font color='brown'>(**#**)</font> See [PyTorch Forum - How to Create a `dataloader` with Variable Size Input](https://discuss.pytorch.org/t/8278).\n",
    "* <font color='brown'>(**#**)</font> See [StackOverflow - How to Train Network on Images of Different Sizes with PyTorch](https://stackoverflow.com/questions/72595995).\n",
    "* <font color='brown'>(**#**)</font> You may use the `TestDataSet` which is adapted to handle no labeled image folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# Data\n",
    "numSamplesTrain  = 22_500\n",
    "numSamplesVal    = 2_500\n",
    "numSamplesValCls = numSamplesVal // 2\n",
    "calcStat         = False\n",
    "\n",
    "# Model\n",
    "dropP = 0.5 #<! Dropout Layer\n",
    "\n",
    "# Training\n",
    "imgSize     = 128\n",
    "batchSize   = 128\n",
    "numWork     = 2 #<! Number of workers\n",
    "nEpochs     = 15\n",
    "\n",
    "# Data Visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Load the classification data set.\n",
    "\n",
    "1. Download the Data Set from [Kaggle Competition - Cat vs. Dog Dataset](https://www.kaggle.com/c/dogs-vs-cats).\n",
    "2. There are pre defined train and test (Not labeled) split.  \n",
    "3. Build a script to create a folder `Validation` and move to it part of the labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Data (Microsoft - Corrupted)\n",
    "\n",
    "# if not os.path.isfile(DATA_SET_FILE_NAME):\n",
    "#     DownloadUrl(DATA_SET_URL, DATA_SET_FILE_NAME)\n",
    "\n",
    "# if not os.path.isdir(os.path.join(DATA_FOLDER_PATH, DATA_SET_FOLDER_NAME)):\n",
    "#     oZipFile = zipfile.ZipFile(DATA_SET_FILE_NAME, 'r')\n",
    "#     lF = oZipFile.namelist()\n",
    "#     for filePath in lF:\n",
    "#         filePathExt, fileExt = os.path.splitext(filePath)\n",
    "#         if (fileExt == '.jpg') or (fileExt == '.jpeg'):\n",
    "#             if 'Cat' in filePathExt:\n",
    "#                 oZipFile.extract(filePath, path = os.path.join(DATA_FOLDER_PATH, DATA_SET_FOLDER_NAME, 'Cat'))\n",
    "#             elif 'Dog' in filePathExt:\n",
    "#                 oZipFile.extract(filePath, path = os.path.join(DATA_FOLDER_PATH, DATA_SET_FOLDER_NAME, 'Dog'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Cat / Dog Folders for Image Folder\n",
    "# Assumes data is in `.Data/CatsDogs`.\n",
    "# Data is in Train / Test folder (Extract the inner Zips).\n",
    "\n",
    "dataSetPath = os.path.join(DATA_FOLDER_PATH, DATA_SET_FOLDER_NAME)\n",
    "if not os.path.isdir(dataSetPath):\n",
    "    os.mkdir(dataSetPath)\n",
    "lFiles = os.listdir(dataSetPath)\n",
    "\n",
    "if '.processed' not in lFiles: #<! Run only once\n",
    "    os.makedirs(os.path.join(dataSetPath, 'Validation', 'Cat'), exist_ok = True)\n",
    "    os.makedirs(os.path.join(dataSetPath, 'Validation', 'Dog'), exist_ok = True)\n",
    "    for dirName in lFiles:\n",
    "        dirPath = os.path.join(DATA_FOLDER_PATH, DATA_SET_FOLDER_NAME, dirName)\n",
    "        if (os.path.isdir(dirPath) and ('train' in dirName.lower())):\n",
    "            # Process Train Folder\n",
    "            os.makedirs(os.path.join(dirPath, 'Cat'), exist_ok = True)\n",
    "            os.makedirs(os.path.join(dirPath, 'Dog'), exist_ok = True)\n",
    "            for fileName in os.listdir(dirPath):\n",
    "                fullFilePath = os.path.join(DATA_FOLDER_PATH, DATA_SET_FOLDER_NAME, dirName, fileName)\n",
    "                _, fileExt = os.path.splitext(fileName)\n",
    "                fileSize = os.path.getsize(fullFilePath)\n",
    "                if ((fileSize > 1) and ((fileExt == '.jpg') or (fileExt == '.jpeg'))):\n",
    "                    if ('cat' in fileName.lower()):\n",
    "                        shutil.move(fullFilePath, os.path.join(dirPath, 'Cat', fileName))\n",
    "                    elif ('dog' in fileName.lower()):\n",
    "                        shutil.move(fullFilePath, os.path.join(dirPath, 'Dog', fileName))\n",
    "        \n",
    "            # Should be random, yet for being able to compare\n",
    "            lF = os.listdir(os.path.join(dirPath, 'Cat'))\n",
    "            for fileName in lF[-1:-(numSamplesValCls + 1):-1]:\n",
    "                shutil.move(os.path.join(dirPath, 'Cat', fileName), os.path.join(dataSetPath, 'Validation', 'Cat', fileName))\n",
    "            lF = os.listdir(os.path.join(dirPath, 'Dog'))\n",
    "            for fileName in lF[-1:-(numSamplesValCls + 1):-1]:\n",
    "                shutil.move(os.path.join(dirPath, 'Dog', fileName), os.path.join(dataSetPath, 'Validation', 'Dog', fileName))\n",
    "\n",
    "    hFile = open(os.path.join(dataSetPath, '.processed'), 'w')\n",
    "    hFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Set \n",
    "\n",
    "dsTrain     = torchvision.datasets.ImageFolder(os.path.join(DATA_FOLDER_PATH, DATA_SET_FOLDER_NAME, 'Train'), transform = torchvision.transforms.ToTensor())\n",
    "dsVal       = torchvision.datasets.ImageFolder(os.path.join(DATA_FOLDER_PATH, DATA_SET_FOLDER_NAME, 'Validation'), transform = torchvision.transforms.ToTensor())\n",
    "dsTest      = TestDataSet(os.path.join(DATA_FOLDER_PATH, DATA_SET_FOLDER_NAME, 'Test'), transform = torchvision.transforms.ToTensor()) #<! Does not return label\n",
    "lClass      = dsTrain.classes\n",
    "numSamples  = len(dsTrain)\n",
    "\n",
    "print(f'The data set number of samples (Train): {numSamples}')\n",
    "print(f'The data set number of samples (Validation): {len(dsVal)}')\n",
    "print(f'The data set number of samples (Test): {len(dsTest)}')\n",
    "print(f'The unique values of the labels: {np.unique(lClass)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Data\n",
    "\n",
    "vIdx = np.random.choice(numSamples, size = 9)\n",
    "hF, vHa = plt.subplots(nrows = 3, ncols = 3, figsize = (8, 8))\n",
    "vHa = vHa.flat\n",
    "\n",
    "for ii, hA in enumerate(vHa):\n",
    "    hA.imshow(dsTrain[vIdx[ii]][0].permute((1, 2, 0)).numpy())\n",
    "    hA.tick_params(axis = 'both', left = False, top = False, right = False, bottom = False, \n",
    "                   labelleft = False, labeltop = False, labelright = False, labelbottom = False)\n",
    "    hA.grid(False)\n",
    "    hA.set_title(f'Index = {vIdx[ii]}, Label = {L_CLASSES[dsTrain[vIdx[ii]][1]]}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Validation Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the Mean and STD per Channel\n",
    "# Looping over all files is slow!\n",
    "# Using Running Mean / Running Squared Mean.\n",
    "# It would have been faster using Data Loader!\n",
    "\n",
    "vMean = np.zeros(3)\n",
    "vSqr = np.zeros(3)\n",
    "\n",
    "\n",
    "if calcStat:\n",
    "    for ii, (tImg, _) in enumerate(dsTrain):\n",
    "        # https://discuss.pytorch.org/t/computing-the-mean-and-std-of-dataset/34949\n",
    "        vMean += torch.mean(tImg, (1, 2)).numpy()\n",
    "        vSqr  += torch.mean(torch.square(tImg), (1, 2)).numpy()\n",
    "    \n",
    "    vMean /= len(dsTrain)\n",
    "    vSqr /= len(dsTrain)\n",
    "    \n",
    "    # σ = sqrt(E[ (x - μ)^2 ]) = sqrt(E[x^2] - μ^2)\n",
    "    vStd = np.sqrt(vSqr - np.square(vMean))\n",
    "else:\n",
    "    # Pre calculated\n",
    "    vMean   = np.array([0.48844224, 0.45524513, 0.41706942])\n",
    "    vStd    = np.array([0.26304555, 0.2565554 , 0.25900563])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Transformers for Data Augmentation\n",
    "# Read about PyTorch Transforms: https://pytorch.org/vision/stable/transforms.html.\n",
    "# Pay attention to v1 vs. v2: \n",
    "# - https://pytorch.org/vision/stable/transforms.html#v1-or-v2-which-one-should-i-use.\n",
    "# - https://pytorch.org/vision/stable/auto_examples/transforms/plot_transforms_getting_started.html#sphx-glr-auto-examples-transforms-plot-transforms-getting-started-py.\n",
    "# - https://pytorch.org/vision/stable/auto_examples/transforms/plot_transforms_e2e.html#sphx-glr-auto-examples-transforms-plot-transforms-e2e-py.\n",
    "# Augmentations adds to run time, be careful.\n",
    "\n",
    "\n",
    "oTransformTrain = TorchVisionTrns.Compose([\n",
    "    TorchVisionTrns.ToImage(),\n",
    "    TorchVisionTrns.ToDtype(torch.float32, scale = True),\n",
    "    TorchVisionTrns.Resize(imgSize),\n",
    "    TorchVisionTrns.CenterCrop(imgSize),\n",
    "    TorchVisionTrns.RandomHorizontalFlip(p = 0.5),\n",
    "    TorchVisionTrns.RandomRotation(5),\n",
    "    TorchVisionTrns.Normalize(mean = vMean, std = vStd),\n",
    "])\n",
    "oTransformVal = TorchVisionTrns.Compose([\n",
    "    TorchVisionTrns.ToImage(),\n",
    "    TorchVisionTrns.ToDtype(torch.float32, scale = True),\n",
    "    TorchVisionTrns.Resize(imgSize),\n",
    "    TorchVisionTrns.CenterCrop(imgSize),\n",
    "    TorchVisionTrns.Normalize(mean = vMean, std = vStd),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Transformers\n",
    "\n",
    "dsTrain.transform   = oTransformTrain\n",
    "dsVal.transform     = oTransformVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Data\n",
    "# Updated transformers\n",
    "\n",
    "vIdx = np.random.choice(numSamples, size = 9)\n",
    "hF, vHa = plt.subplots(nrows = 3, ncols = 3, figsize = (8, 8))\n",
    "vHa = vHa.flat\n",
    "\n",
    "for ii, hA in enumerate(vHa):\n",
    "    hA.imshow(dsTrain[vIdx[ii]][0].permute((1, 2, 0)).numpy())\n",
    "    hA.tick_params(axis = 'both', left = False, top = False, right = False, bottom = False, \n",
    "                   labelleft = False, labeltop = False, labelright = False, labelbottom = False)\n",
    "    hA.grid(False)\n",
    "    hA.set_title(f'Index = {vIdx[ii]}, Label = {L_CLASSES[dsTrain[vIdx[ii]][1]]}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> Cropping means some important information might be cropped out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels Transform\n",
    "# Using BCEWithLogitsLoss requires the labels to be probabilities (Float).\n",
    "# See using Lambda: https://discuss.pytorch.org/t/31857\n",
    "\n",
    "# oTransformTgt = TorchVisionTrns.Lambda(float) #<! Float64\n",
    "oTransformTgt = TorchVisionTrns.Lambda(np.float32) #<! Float32\n",
    "dsTrain.target_transform = oTransformTgt\n",
    "dsVal.target_transform = oTransformTgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader\n",
    "\n",
    "dlTrain = torch.utils.data.DataLoader(dsTrain, shuffle = True, batch_size = 1 * batchSize, num_workers = numWork, persistent_workers = True)\n",
    "dlVal   = torch.utils.data.DataLoader(dsVal, shuffle = False, batch_size = 2 * batchSize, num_workers = numWork, persistent_workers = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model \n",
    "\n",
    "Defining dimensions insensitive model.\n",
    "A simple approach is using Fully Convolutional Neural Network (_FCN_) as those layers support arbitrary size of input.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate on the Loader\n",
    "# The first batch.\n",
    "tX, vY = next(iter(dlTrain)) #<! PyTorch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual Class\n",
    "# Residual Block:\n",
    "# - https://scribe.rip/471810e894ed.\n",
    "# - https://stackoverflow.com/questions/57229054.\n",
    "# - https://wandb.ai/amanarora/Written-Reports/reports/Understanding-ResNets-A-Deep-Dive-into-Residual-Networks-with-PyTorch--Vmlldzo1MDAxMTk5\n",
    "# For nn. vs. F. see:\n",
    "# - https://discuss.pytorch.org/t/31857\n",
    "# - https://stackoverflow.com/questions/53419474\n",
    "\n",
    "# Simple Residual Block\n",
    "class ResidualBlock( nn.Module ):\n",
    "    def __init__( self, numChnl: int ) -> None:\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        self.oConv2D1       = nn.Conv2d(numChnl, numChnl, kernel_size = 3, padding = 1, bias = False)\n",
    "        self.oBatchNorm1    = nn.BatchNorm2d(numChnl)\n",
    "        self.oReLU1         = nn.ReLU(inplace = True)\n",
    "        self.oConv2D2       = nn.Conv2d(numChnl, numChnl, kernel_size = 3, padding = 1, bias = False)\n",
    "        self.oBatchNorm2    = nn.BatchNorm2d(numChnl)\n",
    "        self.oReLU2         = nn.ReLU(inplace = True) #<! No need for it, \n",
    "            \n",
    "    def forward( self: Self, tX: torch.Tensor ) -> torch.Tensor:\n",
    "        \n",
    "        tY = self.oReLU(self.oBatchNorm1(self.oConv2D1(tX)))\n",
    "        tY = self.oBatchNorm2(self.oConv2D2(tY))\n",
    "        tY += tX\n",
    "        tY = self.oReLU(tY)\n",
    "\t\t\n",
    "        return tY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oModel = nn.Sequential(\n",
    "    nn.Identity(),\n",
    "    \n",
    "    nn.Conv2d(3,    16, 3, bias = False), nn.BatchNorm2d(16),  nn.MaxPool2d(2), nn.ReLU(),\n",
    "    nn.Conv2d(16,   32, 3, bias = False), nn.BatchNorm2d(32),  nn.MaxPool2d(2), nn.ReLU(),\n",
    "    nn.Conv2d(32,   64, 3, bias = False), nn.BatchNorm2d(64),  nn.MaxPool2d(2), nn.ReLU(),\n",
    "    nn.Conv2d(64,  128, 3, bias = False), nn.BatchNorm2d(128), nn.MaxPool2d(2), nn.ReLU(),\n",
    "    nn.Conv2d(128, 256, 3, bias = False), nn.BatchNorm2d(256),                  nn.ReLU(),\n",
    "    \n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(256, 1),\n",
    "    nn.Flatten(0),\n",
    ")\n",
    "\n",
    "torchinfo.summary(oModel, tX.shape, col_names = ['kernel_size', 'output_size', 'num_params'], device = 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> One may use `ResidualBlock` as building blocks.\n",
    "* <font color='brown'>(**#**)</font> One may try using [PyTorch's _ResNet_](https://pytorch.org/vision/stable/models/resnet.html) model instead.  \n",
    "  Adjust the input to `224x224`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "The problem is a binary classification and the output is linear (A logit), hence using `BCEWithLogitsLoss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU Availability\n",
    "\n",
    "runDevice   = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') #<! The 1st CUDA device\n",
    "oModel      = oModel.to(runDevice) #<! Transfer model to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Loss & Score\n",
    "\n",
    "hL = nn.BCEWithLogitsLoss() #<! Includes the Sigmoid Built Int\n",
    "hS = BinaryAccuracy()\n",
    "hL = hL.to(runDevice)\n",
    "hS = hS.to(runDevice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Optimizer\n",
    "\n",
    "oOpt = torch.optim.AdamW(oModel.parameters(), lr = 1e-3, betas = (0.9, 0.99), weight_decay = 1e-3) #<! Define optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Scheduler\n",
    "\n",
    "oSch = torch.optim.lr_scheduler.OneCycleLR(oOpt, max_lr = 5e-3, total_steps = nEpochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "\n",
    "oModel, lTrainLoss, lTrainScore, lValLoss, lValScore, lLearnRate = TrainModel(oModel, dlTrain, dlVal, oOpt, nEpochs, hL, hS, oSch = oSch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training Phase\n",
    "\n",
    "hF, vHa = plt.subplots(nrows = 1, ncols = 3, figsize = (12, 5))\n",
    "vHa = np.ravel(vHa)\n",
    "\n",
    "hA = vHa[0]\n",
    "hA.plot(lTrainLoss, lw = 2, label = 'Train')\n",
    "hA.plot(lValLoss, lw = 2, label = 'Validation')\n",
    "hA.set_title('Binary Cross Entropy Loss')\n",
    "hA.set_xlabel('Epoch')\n",
    "hA.set_ylabel('Loss')\n",
    "hA.legend()\n",
    "\n",
    "hA = vHa[1]\n",
    "hA.plot(lTrainScore, lw = 2, label = 'Train')\n",
    "hA.plot(lValScore, lw = 2, label = 'Validation')\n",
    "hA.set_title('Accuracy Score')\n",
    "hA.set_xlabel('Epoch')\n",
    "hA.set_ylabel('Score')\n",
    "hA.legend()\n",
    "\n",
    "hA = vHa[2]\n",
    "hA.plot(lLearnRate, lw = 2)\n",
    "hA.set_title('Learn Rate Scheduler')\n",
    "hA.set_xlabel('Epoch')\n",
    "hA.set_ylabel('Learn Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Best Model\n",
    "oModelBest = copy.deepcopy(oModel)\n",
    "oModelBest.load_state_dict(torch.load('BestModel.pt')['Model'])\n",
    "oModelBest.to('cpu')\n",
    "oModelBest.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre PRocess Function\n",
    "# to apply the same transform on a random image.\n",
    "\n",
    "def PreProcessImg( tX: torch.Tensor, imgSize: int = imgSize, vMean: np.ndarray = vMean, vStd: np.ndarray = vStd ) -> torch.Tensor:\n",
    "    # Assumes tX is an image (C x H x W) in range [0, 1]\n",
    "    tX = TorchVisionTrns.functional.resize(tX, imgSize)\n",
    "    tX = TorchVisionTrns.functional.center_crop(tX, imgSize)\n",
    "    tX = TorchVisionTrns.functional.normalize(tX, mean = vMean, std = vStd)\n",
    "    tX = torch.unsqueeze(tX, 0)\n",
    "\n",
    "    return tX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on Test\n",
    "\n",
    "vIdx = np.random.choice(len(dsTest), size = 9)\n",
    "hF, vHa = plt.subplots(nrows = 3, ncols = 3, figsize = (12, 12))\n",
    "vHa = vHa.flat\n",
    "\n",
    "for ii, hA in enumerate(vHa):\n",
    "    tImg = dsTest[vIdx[ii]]\n",
    "    tX = PreProcessImg(tImg)\n",
    "    valY = oModelBest(tX) #<! Logit -> Label\n",
    "    # print(f'{valY.item()}')\n",
    "    lblIdx = int(valY.item() > 0.0)\n",
    "    hA.imshow(tImg.permute((1, 2, 0)).numpy())\n",
    "    hA.tick_params(axis = 'both', left = False, top = False, right = False, bottom = False, \n",
    "                   labelleft = False, labeltop = False, labelright = False, labelbottom = False)\n",
    "    hA.grid(False)\n",
    "    hA.set_title(f'Index = {vIdx[ii]}, Estimated Label = {L_CLASSES[lblIdx]}')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
