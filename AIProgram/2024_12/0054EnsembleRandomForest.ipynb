{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Fixel Algorithms](https://i.imgur.com/AqKHVZ0.png)](https://fixelalgorithms.gitlab.io)\n",
    "\n",
    "# AI Program\n",
    "\n",
    "## Machine Learning - Supervised Learning - Ensemble Methods - Random Forest\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 1.0.002 | 18/03/2025 | Royi Avital | Added the feature `passTicket`                                     |\n",
    "| 1.0.001 | 14/04/2024 | Royi Avital | Added links to data description                                    |\n",
    "| 1.0.000 | 08/04/2024 | Royi Avital | First version                                                      |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/AIProgram/2024_02/0054EnsembleRandomForest.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T09:30:06.492269Z",
     "start_time": "2022-02-02T09:30:06.220934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Miscellaneous\n",
    "import math\n",
    "import os\n",
    "from platform import python_version\n",
    "import random\n",
    "\n",
    "# Typing\n",
    "from typing import Callable, Dict, List, Optional, Self, Set, Tuple, Union\n",
    "\n",
    "# Visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought.\n",
    "\n",
    "Code Notations:\n",
    "\n",
    "```python\n",
    "someVar    = 2; #<! Notation for a variable\n",
    "vVector    = np.random.rand(4) #<! Notation for 1D array\n",
    "mMatrix    = np.random.rand(4, 3) #<! Notation for 2D array\n",
    "tTensor    = np.random.rand(4, 3, 2, 3) #<! Notation for nD array (Tensor)\n",
    "tuTuple    = (1, 2, 3) #<! Notation for a tuple\n",
    "lList      = [1, 2, 3] #<! Notation for a list\n",
    "dDict      = {1: 3, 2: 2, 3: 1} #<! Notation for a dictionary\n",
    "oObj       = MyClass() #<! Notation for an object\n",
    "dfData     = pd.DataFrame() #<! Notation for a data frame\n",
    "dsData     = pd.Series() #<! Notation for a series\n",
    "hObj       = plt.Axes() #<! Notation for an object / handler / function handler\n",
    "```\n",
    "\n",
    "### Code Exercise\n",
    "\n",
    " - Single line fill\n",
    "\n",
    "```python\n",
    "vallToFill = ???\n",
    "```\n",
    "\n",
    " - Multi Line to Fill (At least one)\n",
    "\n",
    "```python\n",
    "# You need to start writing\n",
    "?????\n",
    "```\n",
    "\n",
    " - Section to Fill\n",
    "\n",
    "```python\n",
    "#===========================Fill This===========================#\n",
    "# 1. Explanation about what to do.\n",
    "# !! Remarks to follow / take under consideration.\n",
    "mX = ???\n",
    "\n",
    "?????\n",
    "#===============================================================#\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# %matplotlib inline\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "# Matplotlib default color palette\n",
    "lMatPltLibclr = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "# sns.set_theme() #>! Apply SeaBorn theme\n",
    "\n",
    "runInGoogleColab = 'google.colab' in str(get_ipython())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "FIG_SIZE_DEF    = (8, 8)\n",
    "ELM_SIZE_DEF    = 50\n",
    "CLASS_COLOR     = ('b', 'r')\n",
    "EDGE_COLOR      = 'k'\n",
    "MARKER_SIZE_DEF = 10\n",
    "LINE_WIDTH_DEF  = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courses Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Auxiliary Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classification\n",
    "\n",
    "In this note book we'll use the _Random Forest_ based classifier in the task of estimating whether a passenger on the Titanic will or will not survive.  \n",
    "We'll focus on basic pre processing of the data and analyzing the importance of features using the classifier.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> This is a very popular data set for classification. You may have a look for notebooks on the need with a deeper analysis of the data set itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# Data\n",
    "trainRatio = 0.75\n",
    "\n",
    "# Model\n",
    "numEst = 150\n",
    "spliCrit = 'gini'\n",
    "maxLeafNodes = 20\n",
    "outBagScore = True\n",
    "\n",
    "# Feature Permutation\n",
    "numRepeats = 50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate / Load Data\n",
    "\n",
    "The data is based on the [Titanic - Machine Learning from Disaster](https://www.kaggle.com/competitions/titanic).  \n",
    "\n",
    "* <font color='brown'>(**#**)</font> The [Titanic Data Description](https://www.kaggle.com/c/titanic/data).\n",
    "* <font color='brown'>(**#**)</font> The [Extended Titanic Data Description](https://www.kaggle.com/datasets/ibrahimelsayed182/titanic-dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "\n",
    "dfX, dsY = fetch_openml('titanic', version = 1, return_X_y = True, as_frame = True, parser = 'auto')\n",
    "\n",
    "print(f'The features data shape: {dfX.shape}')\n",
    "print(f'The labels data shape: {dsY.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Data\n",
    "\n",
    "# The Data Frame of Features\n",
    "dfX.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> What kind of a feature is `name`? Should it be used?\n",
    "* <font color='red'>(**?**)</font> What kind of a feature is `ticket`? Should it be used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Feature Engineering by the Ticket\n",
    "\n",
    "The ticket will be used in a way it adds some information.  \n",
    "We'd count the number of passengers on the same ticket.  \n",
    "Assuming those passengers have some kind of relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the Number of Passenger on the Same Ticket Number\n",
    "dMap              = dfX['ticket'].value_counts().to_dict() #<! Equivalent to `dfX.groupby('ticket').size().sort_values(ascending = False).to_dict()`\n",
    "dfX['passTicket'] = dfX['ticket'].map(dMap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Processing and EDA\n",
    "\n",
    "* <font color='brown'>(**#**)</font> [Julia Silge](https://juliasilge.com) is known for her [EDA Tutorials](https://www.youtube.com/@JuliaSilge/videos) and [Blog Posts](https://juliasilge.com/blog).\n",
    "* <font color='brown'>(**#**)</font> David Robinson is known for his [EDA Tutorials](https://www.youtube.com/@safe4democracy/videos).\n",
    "* <font color='brown'>(**#**)</font> The [Machine Learning Sliced VLog](https://www.youtube.com/playlist?list=PL6PX3YIZuHhyQmXKnyZmVDzdgAYbzwgDw) shows the whole process of working with real world data with an emphasize on Exploratory Data Analysis (EDA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping Features\n",
    "# Dropping the `name` and `ticket` features (Avoid 1:1 identification)\n",
    "\n",
    "dfX = dfX.drop(columns = ['name', 'ticket'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> Pay attention that we dropped the features, but a deeper analysis could extract information from them (Type of ticket, families, etc...). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Features Data Frame\n",
    "dfX.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Labels Data Series\n",
    "dsY.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging Data\n",
    "dfData = pd.concat([dfX, dsY], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Frame Info\n",
    "dfData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing / Invalid Values\n",
    "# Null / NA / NaN Matrix\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Calculate the logical map of invalid values using the method `isna()`.\n",
    "mInvData = dfData.isna() #<! The logical matrix of invalid values\n",
    "#===============================================================#\n",
    "\n",
    "hF, hA = plt.subplots(figsize = FIG_SIZE_DEF)\n",
    "sns.heatmap(data = mInvData, square = False, ax = hA)\n",
    "hA.set_title('Invalid Data Map')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the features `cabin`, `boat`, `body` and `home.dest` have mostly non valid values the will be dropped as well.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> Some implementation of Ensemble Trees can handle missing values. They might benefit in such case asl well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features Filtering\n",
    "# Removing Features with Invalid Values.\n",
    "\n",
    "dfData = dfData.drop(columns = ['cabin', 'boat', 'body', 'home.dest'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Visualization\n",
    "# Basic EDA on the Data\n",
    "\n",
    "numCol = dfData.shape[1]\n",
    "lCols  = dfData.columns\n",
    "numAx  = int(np.ceil(np.sqrt(numCol)))\n",
    "\n",
    "# hIsCatLikData = lambda dsX: (pd.api.types.is_categorical_dtype(dsX) or pd.api.types.is_bool_dtype(dsX) or pd.api.types.is_object_dtype(dsX) or pd.api.types.is_integer_dtype(dsX)) #<! Deprecated\n",
    "hIsCatLikData = lambda dsX: (isinstance(dsX.dtype, pd.CategoricalDtype) or pd.api.types.is_bool_dtype(dsX) or pd.api.types.is_object_dtype(dsX) or pd.api.types.is_integer_dtype(dsX))\n",
    "\n",
    "hF, hAs = plt.subplots(nrows = numAx, ncols = numAx, figsize = (16, 12))\n",
    "hAs = hAs.flat\n",
    "\n",
    "for ii in range(numCol):\n",
    "    colName = dfData.columns[ii]\n",
    "    if hIsCatLikData(dfData[colName]):\n",
    "        sns.histplot(data = dfData, x = colName, hue = 'survived', stat = 'count', discrete = True, common_norm = True, multiple = 'dodge', ax = hAs[ii])\n",
    "    else:\n",
    "        sns.kdeplot(data = dfData, x = colName, hue = 'survived', fill = True, common_norm = True, ax = hAs[ii])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Have a look on the features and try to estimate their importance for the estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling Missing Data\n",
    "\n",
    "For the rest of the missing values we'll use a simple method of interpolation:\n",
    "\n",
    " - Categorical Data: Using the mode value.\n",
    " - Numeric Data: Using the median / mean.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> We could employ much more delicate and sophisticated data.  \n",
    "For instance, use the mean value of the same `pclass`. Namely profiling the data by other features to interpolate the missing feature.\n",
    "* <font color='brown'>(**#**)</font> Data imputing can be done by using a model as well: Regression for _continuous_ data, Classification for _categorical_ data.\n",
    "* <font color='brown'>(**#**)</font> The relevant classed in SciKit Learn: [`SimpleImputer`](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html), [`IterativeImputer`](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html), [`KNNImputer`](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Value by Dictionary\n",
    "dNaNs = {'embarked': dfData['embarked'].mode()[0], 'age': dfData['age'].median(), 'fare': dfData['fare'].median()}\n",
    "\n",
    "dfData = dfData.fillna(value = dNaNs, inplace = False) #<! We can use the `inplace` for efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> The above is equivalent of using [`SimpleImputer`](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html).\n",
    "* <font color='red'>(**?**)</font> Explain the **Data Leakage** above. How can it be avoided?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null / NA / NaN Matrix\n",
    "\n",
    "mInvData = dfData.isna() #<! The logical matrix of invalid values\n",
    "\n",
    "hF, hA = plt.subplots(figsize = FIG_SIZE_DEF)\n",
    "sns.heatmap(data = mInvData, square = False, ax = hA)\n",
    "hA.set_title('Invalid Data Map')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion of Categorical Data\n",
    "\n",
    "In this notebook we'll use the [`RandomForestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) implementation of Random Forest.  \n",
    "At the moment, it doesn't support Categorical Data, hence we'll use Dummy Variables (One Hot Encoding).\n",
    "\n",
    "* <font color='brown'>(**#**)</font> Pay attention that one hot encoding is an inferior approach to having a real support for categorical data.\n",
    "* <font color='brown'>(**#**)</font> For 2 values categorical data (Binary feature) there is no need fo any special support.\n",
    "* <font color='brown'>(**#**)</font> Currently, the implementation of [`DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) doesn't support categorical values which are not ordinal (As it treats them as numeric). Hence we must use `OneHotEncoder`.  \n",
    "See status at https://github.com/scikit-learn/scikit-learn/pull/12866.\n",
    "* <font color='brown'>(**#**)</font> The _One Hot Encoding_ is not perfect. See [Are Categorical Variables Getting Lost in Your Random Forest](https://web.archive.org/web/20200307172925/https://roamanalytics.com/2016/10/28/are-categorical-variables-getting-lost-in-your-random-forests/) (Also [Notebook - Are Categorical Variables Getting Lost in Your Random Forest](https://notebook.community/roaminsight/roamresearch/BlogPosts/Categorical_variables_in_tree_models/categorical_variables_post)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features Encoding\n",
    "# 1. The feature 'embarked' -> One Hot Encoding.\n",
    "# 1. The feature 'sex' -> Mapping (Female -> 0, Male -> 1).\n",
    "dfData = pd.get_dummies(dfData, columns = ['embarked'], drop_first = False)\n",
    "dfData['sex'] = dfData['sex'].map({'female': 0, 'male': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Data Type\n",
    "dfData = dfData.astype(dtype = {'pclass': np.uint8, 'sex': np.uint8, 'sibsp': np.uint8, 'parch': np.uint8, 'survived': np.uint8})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Random Forest Model\n",
    "\n",
    "The Random Forest models basically creates weak classifiers by limiting their access to the data and features.  \n",
    "This basically also limits their correlation which means we can use their mean in order to reduce the variance of the estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split to Features and Labels\n",
    "\n",
    "dfX = dfData.drop(columns = ['survived'])\n",
    "dsY = dfData['survived']\n",
    "\n",
    "print(f'The features data shape: {dfX.shape}')\n",
    "print(f'The labels data shape: {dsY.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Data\n",
    "\n",
    "dfXTrain, dfXTest, dsYTrain, dsYTest = train_test_split(dfX, dsY, train_size = trainRatio, random_state = seedNum, shuffle = True, stratify = dsY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the Model & Train\n",
    "\n",
    "oRndForestsCls = RandomForestClassifier(n_estimators = numEst, criterion = spliCrit, max_leaf_nodes = maxLeafNodes, oob_score = outBagScore)\n",
    "oRndForestsCls = oRndForestsCls.fit(dfXTrain, dsYTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores of the Model\n",
    "\n",
    "print(f'The train accuracy     : {oRndForestsCls.score(dfXTrain, dsYTrain):0.2%}')\n",
    "print(f'The out of bag accuracy: {oRndForestsCls.oob_score_:0.2%}')\n",
    "print(f'The test accuracy      : {oRndForestsCls.score(dfXTest, dsYTest):0.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='blue'>(**!**)</font> Try different values for the model's hyper parameter (Try defaults as well)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contribution to Impurity Decrease\n",
    "\n",
    "This is the default method for feature importance of decision trees based methods.  \n",
    "It basically sums the amount of impurity reduced by splits by each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Feature Importance\n",
    "vFeatImp = oRndForestsCls.feature_importances_\n",
    "vIdxSort = np.argsort(vFeatImp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Feature Importance\n",
    "hF, hA = plt.subplots(figsize = (16, 8))\n",
    "hA.bar(x = dfXTrain.columns[vIdxSort], height = vFeatImp[vIdxSort])\n",
    "hA.set_title('Features Importance of the Model')\n",
    "hA.set_xlabel('Feature Name')\n",
    "hA.set_title('Importance')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Do we need all 3: `embarked_C`, `embarked_Q` and `embarked_S`? Look at the options of `get_dummies()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation Effect\n",
    "\n",
    "This is a more general method to measure the importance of a feature.  \n",
    "We basically replace the values with \"noise\" to see how much performance has been deteriorated.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The SciKit Learn's function which automates the process is [`permutation_importance()`](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html).\n",
    "* <font color='brown'>(**#**)</font> This is highly time consuming operation. Hence the speed of decision trees based methods creates a synergy.\n",
    "* <font color='brown'>(**#**)</font> The importance is strongly linked to the estimator in use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Permutations\n",
    "oFeatImpPermTrain = permutation_importance(oRndForestsCls, dfXTrain, dsYTrain, n_repeats = numRepeats)\n",
    "oFeatImpPermTest  = permutation_importance(oRndForestsCls, dfXTest, dsYTest, n_repeats = numRepeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Data Frame\n",
    "\n",
    "dT = {'Feature': [], 'Importance': [], 'Data': []}\n",
    "\n",
    "for (dataName, mScore) in [('Train', oFeatImpPermTrain.importances), ('Test', oFeatImpPermTest.importances)]:\n",
    "    for ii, featName in enumerate(dfX.columns):\n",
    "        for jj in range(numRepeats):\n",
    "            dT['Feature'].append(featName)\n",
    "            dT['Importance'].append(mScore[ii, jj])\n",
    "            dT['Data'].append(dataName)\n",
    "\n",
    "dfFeatImpPerm = pd.DataFrame(dT)\n",
    "dfFeatImpPerm = dfFeatImpPerm.sort_values(by = ['Data', 'Importance'], ascending = False)\n",
    "dfFeatImpPerm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a List of Feature Names Ordered by Median\n",
    "\n",
    "dsMedianFeature = dfFeatImpPerm[dfFeatImpPerm['Data'] == 'Train'].groupby('Feature')['Importance'].median()\n",
    "lFeatOrderMed   = dsMedianFeature.sort_values(ascending = False).index.to_list() #<! Index is the feature name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Results\n",
    "hF, hA = plt.subplots(figsize = (12, 8))\n",
    "sns.boxplot(data = dfFeatImpPerm, x = 'Feature', y = 'Importance', hue = 'Data', order = lFeatOrderMed, ax = hA)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='green'>(**@**)</font> Try extracting better results by using the dropped features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
