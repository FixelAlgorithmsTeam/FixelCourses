{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Fixel Algorithms](https://i.imgur.com/AqKHVZ0.png)](https://fixelalgorithms.gitlab.io/)\n",
    "\n",
    "# AI Program\n",
    "\n",
    "## Machine Learning - Supervised Learning - Classification - Features Transform \n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 1.0.000 | 16/03/2024 | Royi Avital | First version                                                      |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/AIProgram/2024_02/0037FeaturesTransform.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T09:30:06.492269Z",
     "start_time": "2022-02-02T09:30:06.220934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Image Processing\n",
    "\n",
    "# Machine Learning\n",
    "\n",
    "# Miscellaneous\n",
    "import math\n",
    "import os\n",
    "from platform import python_version\n",
    "import random\n",
    "import timeit\n",
    "\n",
    "# Typing\n",
    "from typing import Callable, Dict, List, Optional, Set, Tuple, Union\n",
    "\n",
    "# Visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "from ipywidgets import Dropdown, FloatSlider, interact, IntSlider, Layout, SelectionSlider\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought.\n",
    "\n",
    "Code Notations:\n",
    "\n",
    "```python\n",
    "someVar    = 2; #<! Notation for a variable\n",
    "vVector    = np.random.rand(4) #<! Notation for 1D array\n",
    "mMatrix    = np.random.rand(4, 3) #<! Notation for 2D array\n",
    "tTensor    = np.random.rand(4, 3, 2, 3) #<! Notation for nD array (Tensor)\n",
    "tuTuple    = (1, 2, 3) #<! Notation for a tuple\n",
    "lList      = [1, 2, 3] #<! Notation for a list\n",
    "dDict      = {1: 3, 2: 2, 3: 1} #<! Notation for a dictionary\n",
    "oObj       = MyClass() #<! Notation for an object\n",
    "dfData     = pd.DataFrame() #<! Notation for a data frame\n",
    "dsData     = pd.Series() #<! Notation for a series\n",
    "hObj       = plt.Axes() #<! Notation for an object / handler / function handler\n",
    "```\n",
    "\n",
    "### Code Exercise\n",
    "\n",
    " - Single line fill\n",
    "\n",
    " ```python\n",
    " vallToFill = ???\n",
    " ```\n",
    "\n",
    " - Multi Line to Fill (At least one)\n",
    "\n",
    " ```python\n",
    " # You need to start writing\n",
    " ????\n",
    " ```\n",
    "\n",
    " - Section to Fill\n",
    "\n",
    "```python\n",
    "#===========================Fill This===========================#\n",
    "# 1. Explanation about what to do.\n",
    "# !! Remarks to follow / take under consideration.\n",
    "mX = ???\n",
    "\n",
    "???\n",
    "#===============================================================#\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# %matplotlib inline\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "# Matplotlib default color palette\n",
    "lMatPltLibclr = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "# sns.set_theme() #>! Apply SeaBorn theme\n",
    "\n",
    "runInGoogleColab = 'google.colab' in str(get_ipython())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "FIG_SIZE_DEF    = (8, 8)\n",
    "ELM_SIZE_DEF    = 50\n",
    "CLASS_COLOR     = ('b', 'r')\n",
    "EDGE_COLOR      = 'k'\n",
    "MARKER_SIZE_DEF = 10\n",
    "LINE_WIDTH_DEF  = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courses Packages\n",
    "\n",
    "from DataVisualization import PlotBinaryClassData, PlotDecisionBoundaryClosure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Auxiliary Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Feature Engineering is the _art of classic machine learning_.   \n",
    "Given that most models are known to all, the _feature engineering_ step is the one most important along with the hyper parameter optimization.\n",
    "\n",
    "It mostly composed of:\n",
    "\n",
    " -  Feature Transform / Extraction  \n",
    "    Applying operators to generate additional features out of the given features.  \n",
    "    The operations can be: Polynomial, Statistical, Normalization, Change of Coordinates, etc...\n",
    " -  Dimensionality Reduction  \n",
    "    A specific case of transform which reduce the number of features to maximize the inner structure of the data. \n",
    " -  Feature Selection  \n",
    "    A specific case of dimensionality reduction where only a sub set of the features are used.  \n",
    "    They are selected by statistical tests or closed loop evaluation.\n",
    "\n",
    "Some also include _Pre Process_ steps as handling missing data and outlier rejection as part of the _feature engineering_ step.\n",
    "\n",
    "The motivation of a specific processing is a result of:\n",
    "\n",
    "1.  Domain Knowledge  \n",
    "    The knowledge about the origins and the domain of data.  \n",
    "    For instance, if one works on RF Data, analyzing the Fourier Domain features is a _domain knowledge_.\n",
    "2.  AutoML  \n",
    "    Building a loop which evaluates the hyper parameters of the feature engineering steps to maximize the score.  \n",
    "    Commonly some automated feature generators are incorporated into the loop.\n",
    "\n",
    "![](https://i.stack.imgur.com/4uPUo.png)\n",
    "\n",
    "Some domains have their own specific approaches. For instance, in _Time Series Forecasting_ there are methods which assists with dealing with the periodicity of the data.  \n",
    "\n",
    "* <font color='brown'>(**#**)</font> See [Data Science - List of Feature Engineering Techniques](https://datascience.stackexchange.com/questions/12984).\n",
    "* <font color='brown'>(**#**)</font> See [Data Science - Tools for Feature Engineering](https://datascience.stackexchange.com/questions/8286).\n",
    "* <font color='brown'>(**#**)</font> See [SciKit Learn's Time Related Feature Engineering](https://scikit-learn.org/stable/auto_examples/applications/plot_cyclical_feature_engineering.html).\n",
    "* <font color='brown'>(**#**)</font> [FeatureTools](https://github.com/alteryx/featuretools) is a well known tool for feature generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel SVM by Feature Transform\n",
    "\n",
    "In this notebook we'll imitate the effect of the _Kernel Trick_ using features transform.  \n",
    "We'll use a _XOR Data Set_, where data are located in the 4 quadrants of the $\\mathbb{R}^{2}$ space.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> Some useful tutorials on Feature Engineering are given in: [Feature Engine](https://github.com/feature-engine/feature_engine), [Feature Engine Examples](https://github.com/feature-engine/feature-engine-examples), [Python Feature Engineering Cookbook - Jupyter Notebooks](https://github.com/PacktPublishing/Python-Feature-Engineering-Cookbook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# Data Generation\n",
    "numSamples = 250 #<! Per Quarter\n",
    "\n",
    "# Model\n",
    "paramC      = 1\n",
    "kernelType  = 'linear'\n",
    "lC          = [0.1, 0.25, 0.75, 1, 1.5, 2, 3]\n",
    "\n",
    "# Data Visualization\n",
    "numGridPts = 500\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate / Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Data \n",
    "\n",
    "mX1  = np.random.rand(numSamples, 2) - 0.5 + np.array([ 1,  1]).T\n",
    "mX2  = np.random.rand(numSamples, 2) - 0.5 + np.array([-1, -1]).T\n",
    "mX3  = np.random.rand(numSamples, 2) - 0.5 + np.array([-1,  1]).T\n",
    "mX4  = np.random.rand(numSamples, 2) - 0.5 + np.array([ 1, -1]).T\n",
    "\n",
    "mX = np.concatenate((mX1, mX2, mX3, mX4), axis = 0)\n",
    "vY = np.concatenate((np.full(2 * numSamples, 1), np.full(2 * numSamples, 0)))\n",
    "\n",
    "\n",
    "PlotDecisionBoundary = PlotDecisionBoundaryClosure(numGridPts, -1.5, 1.5, -1.5, 1.5)\n",
    "\n",
    "print(f'The features data shape: {mX.shape}')\n",
    "print(f'The labels data shape: {vY.shape}')\n",
    "print(f'The unique values of the labels: {np.unique(vY)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Data\n",
    "\n",
    "hA = PlotBinaryClassData(mX, vY, axisTitle = 'Samples Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Linear SVM Model\n",
    "\n",
    "* <font color='red'>(**?**)</font> Given the data, what do you expect the best accuracy will be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Linear Model\n",
    "\n",
    "vAcc = np.zeros(shape = len(lC))\n",
    "\n",
    "for ii, C in enumerate(lC):\n",
    "    oLinSvc  = SVC(C = C, kernel = kernelType).fit(mX, vY)\n",
    "    vAcc[ii] = oLinSvc.score(mX, vY)\n",
    "\n",
    "bestModelIdx    = np.argmax(vAcc)\n",
    "bestC           = lC[bestModelIdx]\n",
    "\n",
    "oLinSvc = SVC(C = bestC, kernel = kernelType).fit(mX, vY)\n",
    "\n",
    "print(f'The best model with C = {bestC:0.2f} achieved accuracy of {vAcc[bestModelIdx]:0.2%}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Decision Boundary\n",
    "\n",
    "hF, hA = plt.subplots(figsize = FIG_SIZE_DEF)\n",
    "hA = PlotDecisionBoundary(oLinSvc.predict, hA)\n",
    "hA = PlotBinaryClassData(mX, vY, hA = hA, axisTitle = 'Classifier Decision Boundary')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Transform\n",
    "\n",
    "In this section we'll a new feature: ${x}_{3} = {x}_{1} \\cdot {x}_{2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a set of features with the new feature\n",
    "mXX = np.column_stack((mX, mX[:, 0] * mX[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution by Linear SVM Classifier\n",
    "\n",
    "In this section we'll try optimize the best Linear SVM model for the problem.  \n",
    "Yet, we'll train it on the features with the additional transformed one.\n",
    "\n",
    "Then we'll show the decision boundary of the best model.\n",
    "\n",
    "* <font color='red'>(**?**)</font> What do you expect the decision boundary to look like this time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Linear Model\n",
    "\n",
    "vAcc = np.zeros(shape = len(lC))\n",
    "\n",
    "for ii, C in enumerate(lC):\n",
    "    oLinSvc  = SVC(C = C, kernel = kernelType).fit(mXX, vY) #<! Pay attention we train on `mXX`\n",
    "    vAcc[ii] = oLinSvc.score(mXX, vY)\n",
    "\n",
    "bestModelIdx    = np.argmax(vAcc)\n",
    "bestC           = lC[bestModelIdx]\n",
    "\n",
    "oLinSvc = SVC(C = bestC, kernel = kernelType).fit(mXX, vY)\n",
    "\n",
    "print(f'The best model with C = {bestC:0.2f} achieved accuracy of {vAcc[bestModelIdx]:0.2%}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Why was the above `C` gave the best results?\n",
    "* <font color='red'>(**?**)</font> What's the accuracy of all other models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Decision Boundary\n",
    "\n",
    "hPredict = lambda mX: oLinSvc.predict(np.column_stack((mX, mX[:, 0] * mX[:, 1])))\n",
    "\n",
    "hF, hA = plt.subplots(figsize = FIG_SIZE_DEF)\n",
    "hA = PlotDecisionBoundary(hPredict, hA)\n",
    "hA = PlotBinaryClassData(mX, vY, hA = hA, axisTitle = 'Classifier Decision Boundary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution by Kernel SVM - Polynomial\n",
    "\n",
    "In this section we'll apply a Kernel SVM with Polynomial kernel.\n",
    "\n",
    "* <font color='red'>(**?**)</font> What feature transform is needed for this model?\n",
    "* <font color='red'>(**?**)</font> What's the minimum degree of the polynomial to solve this problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Polynomial Model\n",
    "\n",
    "pDegree     = 4\n",
    "kernelType  = 'poly'\n",
    "\n",
    "vAcc = np.zeros(shape = len(lC))\n",
    "\n",
    "for ii, C in enumerate(lC):\n",
    "    oSvc     = SVC(C = C, kernel = kernelType, degree = pDegree).fit(mX, vY)\n",
    "    vAcc[ii] = oSvc.score(mX, vY)\n",
    "\n",
    "bestModelIdx    = np.argmax(vAcc)\n",
    "bestC           = lC[bestModelIdx]\n",
    "\n",
    "oSvc = SVC(C = bestC, kernel = kernelType, degree = pDegree).fit(mX, vY)\n",
    "\n",
    "print(f'The best model with C = {bestC:0.2f} achieved accuracy of {vAcc[bestModelIdx]:0.2%}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Decision Boundary\n",
    "\n",
    "hF, hA = plt.subplots(figsize = FIG_SIZE_DEF)\n",
    "hA = PlotDecisionBoundary(oSvc.predict, hA)\n",
    "hA = PlotBinaryClassData(mX, vY, hA = hA, axisTitle = 'Classifier Decision Boundary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='green'>(**@**)</font> Do the above with the `rbf` and `sigmoid` kernels.\n",
    "* <font color='blue'>(**!**)</font> Run the above with the kernel `poly` and set `degree` to 100. What happened?\n",
    "* <font color='red'>(**?**)</font> How will the complexity of the calculation grow with the polynomial degree? \n",
    "* <font color='brown'>(**#**)</font> The issues above are the motivation for the _Kernel Trick_."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
