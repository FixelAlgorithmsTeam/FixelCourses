{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Fixel Algorithms](https://i.imgur.com/AqKHVZ0.png)](https://fixelalgorithms.gitlab.io)\n",
    "\n",
    "# AI Program\n",
    "\n",
    "## Machine Learning - Deep Learning - PyTorch Schedulers\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 1.0.000 | 11/05/2024 | Royi Avital | First version                                                      |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/AIProgram/2024_02/0089DeepLearningPyTorchSchedulers.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T09:30:06.492269Z",
     "start_time": "2022-02-02T09:30:06.220934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn            as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torch.optim.lr_scheduler import LRScheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchinfo\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "import torchvision\n",
    "\n",
    "# Miscellaneous\n",
    "import copy\n",
    "import math\n",
    "import os\n",
    "from platform import python_version\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Typing\n",
    "from typing import Callable, Dict, Generator, List, Optional, Self, Set, Tuple, Union\n",
    "\n",
    "# Visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython\n",
    "from IPython.display import HTML, Image\n",
    "from IPython.display import display\n",
    "from ipywidgets import Dropdown, FloatSlider, interact, IntSlider, Layout, SelectionSlider\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought.\n",
    "\n",
    "Code Notations:\n",
    "\n",
    "```python\n",
    "someVar    = 2; #<! Notation for a variable\n",
    "vVector    = np.random.rand(4) #<! Notation for 1D array\n",
    "mMatrix    = np.random.rand(4, 3) #<! Notation for 2D array\n",
    "tTensor    = np.random.rand(4, 3, 2, 3) #<! Notation for nD array (Tensor)\n",
    "tuTuple    = (1, 2, 3) #<! Notation for a tuple\n",
    "lList      = [1, 2, 3] #<! Notation for a list\n",
    "dDict      = {1: 3, 2: 2, 3: 1} #<! Notation for a dictionary\n",
    "oObj       = MyClass() #<! Notation for an object\n",
    "dfData     = pd.DataFrame() #<! Notation for a data frame\n",
    "dsData     = pd.Series() #<! Notation for a series\n",
    "hObj       = plt.Axes() #<! Notation for an object / handler / function handler\n",
    "```\n",
    "\n",
    "### Code Exercise\n",
    "\n",
    " - Single line fill\n",
    "\n",
    "```python\n",
    "valToFill = ???\n",
    "```\n",
    "\n",
    " - Multi Line to Fill (At least one)\n",
    "\n",
    "```python\n",
    "# You need to start writing\n",
    "?????\n",
    "```\n",
    "\n",
    " - Section to Fill\n",
    "\n",
    "```python\n",
    "#===========================Fill This===========================#\n",
    "# 1. Explanation about what to do.\n",
    "# !! Remarks to follow / take under consideration.\n",
    "mX = ???\n",
    "\n",
    "?????\n",
    "#===============================================================#\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# %matplotlib inline\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "# Matplotlib default color palette\n",
    "lMatPltLibclr = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "# sns.set_theme() #>! Apply SeaBorn theme\n",
    "\n",
    "runInGoogleColab = 'google.colab' in str(get_ipython())\n",
    "\n",
    "# Improve performance by benchmarking\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Reproducibility (Per PyTorch Version on the same device)\n",
    "# torch.manual_seed(seedNum)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark     = False #<! Makes things slower\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "FIG_SIZE_DEF    = (8, 8)\n",
    "ELM_SIZE_DEF    = 50\n",
    "CLASS_COLOR     = ('b', 'r')\n",
    "EDGE_COLOR      = 'k'\n",
    "MARKER_SIZE_DEF = 10\n",
    "LINE_WIDTH_DEF  = 2\n",
    "\n",
    "D_CLASSES_CIFAR_10  = {0: 'Airplane', 1: 'Automobile', 2: 'Bird', 3: 'Cat', 4: 'Deer', 5: 'Dog', 6: 'Frog', 7: 'Horse', 8: 'Ship', 9: 'Truck'}\n",
    "L_CLASSES_CIFAR_10  = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
    "T_IMG_SIZE_CIFAR_10 = (32, 32, 3)\n",
    "\n",
    "DATA_FOLDER_PATH    = 'Data'\n",
    "TENSOR_BOARD_BASE   = 'TB'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Auxiliary Modules for Google Colab\n",
    "if runInGoogleColab:\n",
    "    !wget https://raw.githubusercontent.com/FixelAlgorithmsTeam/FixelCourses/master/AIProgram/2024_02/DataManipulation.py\n",
    "    !wget https://raw.githubusercontent.com/FixelAlgorithmsTeam/FixelCourses/master/AIProgram/2024_02/DataVisualization.py\n",
    "    !wget https://raw.githubusercontent.com/FixelAlgorithmsTeam/FixelCourses/master/AIProgram/2024_02/DeepLearningPyTorch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courses Packages\n",
    "\n",
    "from DataVisualization import PlotLabelsHistogram, PlotMnistImages\n",
    "from DeepLearningPyTorch import NNMode\n",
    "from DeepLearningPyTorch import InitWeightsKaiNorm, TrainModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Auxiliary Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Schedulers\n",
    "\n",
    "PyTorch _Schedulers_ are functions which alters the learning rate by event: Iteration index value update, loss function value update, etc...  \n",
    "The scheduling of the _learning rate_ can assist with better convergence, both in speed and \"quality\" (Wide basin).\n",
    "\n",
    "One could implement schedulers manually as part of the training loop, yet PyTorch offers some built in recipes which are easier to use.\n",
    "\n",
    "The notebook presents:\n",
    "\n",
    " * The concept of _Schedulers_.\n",
    " * Compares the result of training loop with different schedulers.\n",
    "\n",
    "\n",
    "</br>\n",
    "\n",
    "* <font color='brown'>(**#**)</font> [YouTube - Sebastian Raschka - Learning Rate Schedulers in PyTorch](https://www.youtube.com/watch?v=tB1rz4L93JA).\n",
    "* <font color='brown'>(**#**)</font> [PyTorch Training Performance Guide - LR Schedulers, Adaptive Optimizers](https://residentmario.github.io/pytorch-training-performance-guide/lr-sched-and-optim.html).\n",
    "* <font color='brown'>(**#**)</font> [Guide to Pytorch Learning Rate Scheduling](https://www.kaggle.com/code/isbhargav/guide-to-pytorch-learning-rate-scheduling).\n",
    "* <font color='brown'>(**#**)</font> [Distill - Why Momentum Really Works](https://distill.pub/2017/momentum)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# Data\n",
    "\n",
    "# Model\n",
    "dropP = 0.5 #<! Dropout Layer\n",
    "\n",
    "# Training\n",
    "batchSize   = 256\n",
    "numWork     = 2 #<! Number of workers\n",
    "nEpochs     = 10\n",
    "\n",
    "# Visualization\n",
    "numImg = 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate / Load Data\n",
    "\n",
    "Load the [CIFAR 10 Data Set](https://en.wikipedia.org/wiki/CIFAR-10).  \n",
    "It is composed of 60,000 RGB images of size `32x32` with 10 classes uniformly spread.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The dataset is retrieved using [Torch Vision](https://pytorch.org/vision/stable/index.html)'s built in datasets.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "\n",
    "# PyTorch \n",
    "dsTrain = torchvision.datasets.CIFAR10(root = DATA_FOLDER_PATH, train = True,  download = True, transform = torchvision.transforms.ToTensor())\n",
    "dsTest  = torchvision.datasets.CIFAR10(root = DATA_FOLDER_PATH, train = False, download = True, transform = torchvision.transforms.ToTensor())\n",
    "lClass  = dsTrain.classes\n",
    "\n",
    "\n",
    "print(f'The training data set data shape: {dsTrain.data.shape}')\n",
    "print(f'The test data set data shape: {dsTest.data.shape}')\n",
    "print(f'The unique values of the labels: {np.unique(lClass)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> The dataset is indexible (Subscriptable). It returns a tuple of the features and the label.\n",
    "* <font color='brown'>(**#**)</font> While data is arranged as `H x W x C` the transformer, when accessing the data, will convert it into `C x H x W`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Element of the Data Set\n",
    "\n",
    "mX, valY = dsTrain[0]\n",
    "\n",
    "print(f'The features shape: {mX.shape}')\n",
    "print(f'The label value: {valY}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Data\n",
    "\n",
    "tX = dsTrain.data #<! NumPy Tensor (NDarray)\n",
    "mX = np.reshape(tX, (tX.shape[0], -1))\n",
    "vY = dsTrain.targets #<! NumPy Vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Data\n",
    "\n",
    "hF = PlotMnistImages(mX, vY, numImg, tuImgSize = T_IMG_SIZE_CIFAR_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of Labels\n",
    "\n",
    "hA = PlotLabelsHistogram(vY, lClass = L_CLASSES_CIFAR_10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Process Data\n",
    "\n",
    "This section normalizes the data to have zero mean and unit variance per **channel**.  \n",
    "It is required to calculate:\n",
    "\n",
    " * The average pixel value per channel.\n",
    " * The standard deviation per channel.\n",
    "\n",
    "</br>\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The values calculated on the train set and applied to both sets.\n",
    "* <font color='brown'>(**#**)</font> The the data will be used to pre process the image on loading by the `transformer`.\n",
    "* <font color='brown'>(**#**)</font> There packages which specializes in transforms: [`Kornia`](https://github.com/kornia/kornia), [`Albumentations`](https://github.com/albumentations-team/albumentations).  \n",
    "  They are commonly used for _Data Augmentation_ at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> What do you expect the mean value to be?\n",
    "* <font color='red'>(**?**)</font> What do you expect the standard deviation value to be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Standardization Parameters\n",
    "vMean = np.mean(dsTrain.data / 255.0, axis = (0, 1, 2))\n",
    "vStd  = np.std(dsTest.data / 255.0, axis = (0, 1, 2))\n",
    "\n",
    "print('µ =', vMean)\n",
    "print('σ =', vStd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Transformer\n",
    "\n",
    "oDataTrns = torchvision.transforms.Compose([           #<! Chaining transformations\n",
    "    torchvision.transforms.ToTensor(),                 #<! Convert to Tensor (C x H x W), Normalizes into [0, 1] (https://pytorch.org/vision/main/generated/torchvision.transforms.ToTensor.html)\n",
    "    torchvision.transforms.Normalize(vMean, vStd), #<! Normalizes the Data (https://pytorch.org/vision/main/generated/torchvision.transforms.Normalize.html)\n",
    "    ])\n",
    "\n",
    "# Update the DS transformer\n",
    "dsTrain.transform = oDataTrns\n",
    "dsTest.transform  = oDataTrns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Normalized\" Image\n",
    "\n",
    "mX, valY = dsTrain[5]\n",
    "\n",
    "hF, hA = plt.subplots()\n",
    "hImg = hA.imshow(np.transpose(mX, (1, 2, 0)))\n",
    "hF.colorbar(hImg)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loaders\n",
    "\n",
    "The dataloader is the functionality which loads the data into memory in batches.  \n",
    "Its challenge is to bring data fast enough so the Hard Disk is not the training bottleneck.  \n",
    "In order to achieve that, Multi Threading / Multi Process is used.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The multi process, by the `num_workers` parameter is not working well _out of the box_ on Windows.  \n",
    "  See [Errors When Using `num_workers > 0` in `DataLoader`](https://discuss.pytorch.org/t/97564), [On Windows `DataLoader` with `num_workers > 0` Is Slow](https://github.com/pytorch/pytorch/issues/12831).  \n",
    "  A way to overcome it is to define the training loop as a function in a different module (File) and import it (https://discuss.pytorch.org/t/97564/4, https://discuss.pytorch.org/t/121588/21). \n",
    "* <font color='brown'>(**#**)</font> The `num_workers` should be set to the lowest number which feeds the GPU fast enough.  \n",
    "  The idea is preserve as much as CPU resources to other tasks.\n",
    "* <font color='brown'>(**#**)</font> On Windows keep the `persistent_workers` parameter to `True` (_Windows_ is slower on forking processes / threads).\n",
    "* <font color='brown'>(**#**)</font> The Dataloader is a generator which can be looped on.\n",
    "* <font color='brown'>(**#**)</font> In order to make it iterable it has to be wrapped with `iter()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader\n",
    "\n",
    "dlTrain  = torch.utils.data.DataLoader(dsTrain, shuffle = True, batch_size = 1 * batchSize, num_workers = numWork, persistent_workers = True)\n",
    "dlTest   = torch.utils.data.DataLoader(dsTest, shuffle = False, batch_size = 2 * batchSize, num_workers = numWork, persistent_workers = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Why is the size of the batch twice as big for the test dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate on the Loader\n",
    "# The first batch.\n",
    "tX, vY = next(iter(dlTrain)) #<! PyTorch Tensors\n",
    "\n",
    "print(f'The batch features dimensions: {tX.shape}')\n",
    "print(f'The batch labels dimensions: {vY.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping\n",
    "for ii, (tX, vY) in zip(range(1), dlTest): #<! https://stackoverflow.com/questions/36106712\n",
    "    print(f'The batch features dimensions: {tX.shape}')\n",
    "    print(f'The batch labels dimensions: {vY.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model\n",
    "\n",
    "The model is defined as a sequential model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "# Defining a sequential model.\n",
    "\n",
    "numFeatures = np.prod(tX.shape[1:])\n",
    "\n",
    "oModel = nn.Sequential(\n",
    "    nn.Identity(),\n",
    "        \n",
    "    nn.Conv2d(3,   32,  3, bias = False), nn.BatchNorm2d(32),                   nn.ReLU(),\n",
    "    nn.Conv2d(32,  64,  3, bias = False), nn.BatchNorm2d(64),  nn.MaxPool2d(2), nn.ReLU(),\n",
    "    nn.Conv2d(64,  128, 3, bias = False), nn.BatchNorm2d(128), nn.MaxPool2d(2), nn.ReLU(),\n",
    "    nn.Conv2d(128, 256, 3, bias = False), nn.BatchNorm2d(256),                  nn.ReLU(),\n",
    "    nn.Conv2d(256, 256, 3, bias = False), nn.BatchNorm2d(256),                  nn.ReLU(),\n",
    "    \n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(256, len(lClass)),\n",
    ")\n",
    "\n",
    "torchinfo.summary(oModel, tX.shape, col_names = ['kernel_size', 'output_size', 'num_params'], device = 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Why is `bias = False` used above?\n",
    "* <font color='brown'>(**#**)</font> Using a multiplication by 8 number of channels accelerate the run time (In most cases).\n",
    "* <font color='brown'>(**#**)</font> Pay attention to model size and the RAM fo the GPU. Rule of thumb, up to ~40%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "This section trains the model using different schedulers:\n",
    "\n",
    " - Updates the training function.\n",
    " - Updates the _epoch_ function to log information at mini batch level.\n",
    " - Create a class for a logger of TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Device\n",
    "\n",
    "runDevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') #<! The 1st CUDA device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Score Function\n",
    "\n",
    "hL = nn.CrossEntropyLoss()\n",
    "hS = MulticlassAccuracy(num_classes = len(lClass), average = 'micro')\n",
    "hL = hL.to(runDevice) #<! Not required!\n",
    "hS = hS.to(runDevice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schedulers\n",
    "\n",
    "![](https://raw.githubusercontent.com/FixelAlgorithmsTeam/FixelCourses/master/DeepLearningMethods/07_PyTorch2/Schedulers.PNG)\n",
    "\n",
    "The motivation of the scheduling is to increase the chances to:\n",
    "\n",
    "1. Moving fast towards minimum.\n",
    "2. Avoid being stuck in \"bad\" local minimum (Sharp, Narrow).\n",
    "3. Finding a \"good\" local minimum (Deep, Wide).\n",
    "\n",
    "There are few common policies:\n",
    "\n",
    "1. Linear  \n",
    "   Linearly interpolate between a starting _learning rate_ and a final _learning rate_.  \n",
    "   It can be used for constant _learning rate_ as well.\n",
    "2. Exponential   \n",
    "   Multiplies the _learning rate_ by a constant at each step.\n",
    "3. Cosine Annealing  \n",
    "   Similar to the Linear policy with smoother operation by using the fall of a cosine.\n",
    "4. Cyclic  \n",
    "   The _learning rate_ is a damped (Optionally) saw tooth function.  \n",
    "   Going up occasionally empirically proved to be effective avoiding \"bad\" stationary points.\n",
    "5. One Cycle  \n",
    "   Goes up and down a single time asymmetrically.\n",
    "\n",
    "\n",
    "### Step Policy  \n",
    "\n",
    "It used to be common to update the _learning rate_ at the end of an epoch.  \n",
    "Yet as data sets have become large it commonly updated at the end of each mini batch since the number of epochs might be low.\n",
    "\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The implementation in this notebook applies the step per mini batch iteration.  \n",
    "  Yet the function in `DeepLearningPyTorch.py` is at the _epoch_ level.\n",
    "* <font color='brown'>(**#**)</font> More schedulers are available at the [`torch.optim`](https://pytorch.org/docs/stable/optim.html) page.\n",
    "* <font color='brown'>(**#**)</font> PyTorch has the flexibility of assigning different learning rate per module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schedulers (Demo)\n",
    "\n",
    "nIter           = 5_0000\n",
    "baseLearnRate   = 0.1\n",
    "\n",
    "lScheduler = [\n",
    "    ('Constant', torch.optim.lr_scheduler.LinearLR, {'start_factor': 1.0}),\n",
    "    ('Linear', torch.optim.lr_scheduler.LinearLR, {'start_factor': 1.0, 'end_factor': 0.01, 'total_iters': nIter}),\n",
    "    ('Exponential', torch.optim.lr_scheduler.ExponentialLR, {'gamma': 0.99985}),\n",
    "    ('Cosine', torch.optim.lr_scheduler.CosineAnnealingLR, {'T_max': nIter} ),\n",
    "    ('Cyclic', torch.optim.lr_scheduler.CyclicLR, {'base_lr': 1e-6, 'max_lr': baseLearnRate, 'step_size_up': nIter // 6, 'step_size_down' : nIter // 6, 'mode':'triangular2', 'cycle_momentum': False}),\n",
    "    ('OneCycle', torch.optim.lr_scheduler.OneCycleLR, {'max_lr': baseLearnRate, 'total_steps': nIter}),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the Step Size\n",
    "# Scheduler require an optimizer.\n",
    "# Optimizer requires parameters.\n",
    "\n",
    "numSched    = len(lScheduler)\n",
    "mStepSize   = np.full(shape = (numSched, nIter + 1), fill_value = np.nan)\n",
    "\n",
    "for ii, (_, SchedCls, dParams) in enumerate(lScheduler):\n",
    "    # ii: The iteration used for the scheduler\n",
    "    oModelTmp = copy.deepcopy(oModel) #<! Dummy model\n",
    "    oOpt = torch.optim.SGD(oModelTmp.parameters(), lr = baseLearnRate) #<! Define optimizer\n",
    "    oSched = SchedCls(oOpt, **dParams)\n",
    "    for jj in range(nIter):\n",
    "        mStepSize[ii, jj] = oSched.get_last_lr()[0]\n",
    "        oSched.step()\n",
    "    jj += 1\n",
    "    mStepSize[ii, jj] = oSched.get_last_lr()[0] #<! Last iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Learning Rate\n",
    "\n",
    "hF, hA = plt.subplots(figsize = (10, 5))\n",
    "\n",
    "for ii, (schedStr, *_) in enumerate(lScheduler):\n",
    "    hA.plot(mStepSize[ii], label = schedStr)\n",
    "hA.legend()\n",
    "hA.set_title(f'Learning Rate Schedulers, Base Learning Rate: {baseLearnRate: 0.2f}')\n",
    "hA.set_xlabel('Iteration')\n",
    "hA.set_ylabel('Learning Rate')\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> Schedulers are set per iteration (Batch) or epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logger \n",
    "# Wrapper of TensorBoard's `SummaryWriter` with index for iteration and epoch.\n",
    "\n",
    "class TBLogger():\n",
    "    def __init__( self, logDir: Optional[str] = None ) -> None:\n",
    "\n",
    "        self.oTBWriter  = SummaryWriter(log_dir = logDir)\n",
    "        self.iiEpcoh    = 0\n",
    "        self.iiItr      = 0\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def close( self ) -> None:\n",
    "\n",
    "        self.oTBWriter.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Epoch\n",
    "def RunEpoch( oModel: nn.Module, dlData: DataLoader, hL: Callable, hS: Callable, oOpt: Optional[Optimizer] = None, oSch: Optional[LRScheduler] = None, opMode: NNMode = NNMode.TRAIN, oTBLogger: Optional[TBLogger] = None ) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Runs a single Epoch (Train / Test) of a model.  \n",
    "    Input:\n",
    "        oModel      - PyTorch `nn.Module` object.\n",
    "        dlData      - PyTorch `Dataloader` object.\n",
    "        hL          - Callable for the Loss function.\n",
    "        hS          - Callable for the Score function.\n",
    "        oOpt        - PyTorch `Optimizer` object.\n",
    "        oSch        - PyTorch `Scheduler` (`LRScheduler`) object.\n",
    "        opMode      - An `NNMode` to set the mode of operation.\n",
    "        oTBLogger   - An `TBLogger` object.\n",
    "    Output:\n",
    "        valLoss     - Scalar of the loss.\n",
    "        valScore    - Scalar of the score.\n",
    "        learnRate   - Scalar of the average learning rate over the epoch.\n",
    "    Remarks:\n",
    "      - The `oDataSet` object returns a Tuple of (mX, vY) per batch.\n",
    "      - The `hL` function should accept the `vY` (Reference target) and `mZ` (Output of the NN).  \n",
    "        It should return a Tuple of `valLoss` (Scalar of the loss) and `mDz` (Gradient by the loss).\n",
    "      - The `hS` function should accept the `vY` (Reference target) and `mZ` (Output of the NN).  \n",
    "        It should return a scalar `valScore` of the score.\n",
    "      - The optimizer / scheduler are required for training mode.\n",
    "    \"\"\"\n",
    "    \n",
    "    epochLoss   = 0.0\n",
    "    epochScore  = 0.0\n",
    "    numSamples  = 0\n",
    "    #!!!\n",
    "    epochLr     = 0.0\n",
    "    #!!!\n",
    "    numBatches = len(dlData)\n",
    "\n",
    "    runDevice = next(oModel.parameters()).device #<! CPU \\ GPU\n",
    "\n",
    "    if opMode == NNMode.TRAIN:\n",
    "        oModel.train(True) #<! Equivalent of `oModel.train()`\n",
    "    elif opMode == NNMode.INFERENCE:\n",
    "        oModel.eval() #<! Equivalent of `oModel.train(False)`\n",
    "    else:\n",
    "        raise ValueError(f'The `opMode` value {opMode} is not supported!')\n",
    "    \n",
    "    for ii, (mX, vY) in enumerate(dlData):\n",
    "        # Move Data to Model's device\n",
    "        mX = mX.to(runDevice) #<! Lazy\n",
    "        vY = vY.to(runDevice) #<! Lazy\n",
    "\n",
    "\n",
    "        batchSize = mX.shape[0]\n",
    "        \n",
    "        if opMode == NNMode.TRAIN:\n",
    "            # Forward\n",
    "            mZ      = oModel(mX) #<! Model output\n",
    "            valLoss = hL(mZ, vY) #<! Loss\n",
    "            \n",
    "            # Backward\n",
    "            oOpt.zero_grad()    #<! Set gradients to zeros\n",
    "            valLoss.backward()  #<! Backward\n",
    "            oOpt.step()         #<! Update parameters\n",
    "\n",
    "            #!!!\n",
    "            learnRate = oSch.get_last_lr()[0]\n",
    "            oSch.step() #<! Update learning rate\n",
    "            #!!!\n",
    "            \n",
    "            oModel.eval() #<! Set layers for inference mode\n",
    "\n",
    "        else: #<! Value of `opMode` was already validated\n",
    "            with torch.no_grad():\n",
    "                # No computational \n",
    "                mZ      = oModel(mX) #<! Model output\n",
    "                valLoss = hL(mZ, vY) #<! Loss\n",
    "                \n",
    "                learnRate = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Score\n",
    "            valScore = hS(mZ, vY)\n",
    "            # Normalize so each sample has the same weight\n",
    "            epochLoss  += batchSize * valLoss.item()\n",
    "            epochScore += batchSize * valScore.item()\n",
    "            epochLr    += batchSize * learnRate\n",
    "            numSamples += batchSize\n",
    "\n",
    "            #!!!\n",
    "            if (oTBLogger is not None) and (opMode == NNMode.TRAIN):\n",
    "                # Logging at Iteration level for training\n",
    "                oTBLogger.iiItr += 1\n",
    "                oTBLogger.oTBWriter.add_scalar('Train Loss', valLoss.item(), oTBLogger.iiItr)\n",
    "                oTBLogger.oTBWriter.add_scalar('Train Score', valScore.item(), oTBLogger.iiItr)\n",
    "                oTBLogger.oTBWriter.add_scalar('Learning Rate', learnRate, oTBLogger.iiItr)\n",
    "            #!!!\n",
    "\n",
    "        print(f'\\r{\"Train\" if opMode == NNMode.TRAIN else \"Val\"} - Iteration: {ii:3d} ({numBatches}): loss = {valLoss:.6f}', end = '')\n",
    "    \n",
    "    print('', end = '\\r')\n",
    "            \n",
    "    return epochLoss / numSamples, epochScore / numSamples, epochLr / numSamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "def TrainModel( oModel: nn.Module, dlTrain: DataLoader, dlVal: DataLoader, oOpt: Optimizer, oSch: LRScheduler, numEpoch: int, hL: Callable, hS: Callable, oTBLogger: Optional[TBLogger] = None ) -> Tuple[nn.Module, List, List, List, List]:\n",
    "\n",
    "    lTrainLoss  = []\n",
    "    lTrainScore = []\n",
    "    lValLoss    = []\n",
    "    lValScore   = []\n",
    "    #!!!\n",
    "    lLearnRate  = []\n",
    "    #!!!\n",
    "\n",
    "    # Support R2\n",
    "    bestScore = -1e9 #<! Assuming higher is better\n",
    "\n",
    "    for ii in range(numEpoch):\n",
    "        startTime                       = time.time()\n",
    "        #!!!\n",
    "        trainLoss, trainScr, trainLr    = RunEpoch(oModel, dlTrain, hL, hS, oOpt, oSch, opMode = NNMode.TRAIN, oTBLogger = oTBLogger) #<! Train\n",
    "        #!!!\n",
    "        valLoss,   valScr, _            = RunEpoch(oModel, dlVal, hL, hS, opMode = NNMode.INFERENCE)    #<! Score Validation\n",
    "        epochTime                       = time.time() - startTime\n",
    "\n",
    "        # Aggregate Results\n",
    "        lTrainLoss.append(trainLoss)\n",
    "        lTrainScore.append(trainScr)\n",
    "        lValLoss.append(valLoss)\n",
    "        lValScore.append(valScr)\n",
    "        #!!!\n",
    "        lLearnRate.append(trainLr)\n",
    "        #!!!\n",
    "\n",
    "        if oTBLogger is not None:\n",
    "            #!!!\n",
    "            oTBLogger.iiEpcoh += 1\n",
    "            oTBLogger.oTBWriter.add_scalars('Loss (Epoch)', {'Train': trainLoss, 'Validation': valLoss}, ii)\n",
    "            oTBLogger.oTBWriter.add_scalars('Score (Epoch)', {'Train': trainScr, 'Validation': valScr}, ii)\n",
    "            oTBLogger.oTBWriter.add_scalar('Learning Rate (Epoch)', trainLr, ii)\n",
    "            oTBLogger.oTBWriter.flush()\n",
    "            #!!!\n",
    "        \n",
    "        # Display (Babysitting)\n",
    "        print('Epoch '              f'{(ii + 1):4d} / ' f'{numEpoch}:', end = '')\n",
    "        print(' | Train Loss: '     f'{trainLoss          :6.3f}', end = '')\n",
    "        print(' | Val Loss: '       f'{valLoss            :6.3f}', end = '')\n",
    "        print(' | Train Score: '    f'{trainScr           :6.3f}', end = '')\n",
    "        print(' | Val Score: '      f'{valScr             :6.3f}', end = '')\n",
    "        print(' | Epoch Time: '     f'{epochTime          :5.2f}', end = '')\n",
    "\n",
    "        # Save best model (\"Early Stopping\")\n",
    "        if valScr > bestScore:\n",
    "            bestScore = valScr\n",
    "            print(' | <-- Checkpoint!', end = '')\n",
    "            try:\n",
    "                #!!!\n",
    "                dCheckpoint = {'Model' : oModel.state_dict(), 'Optimizer' : oOpt.state_dict(), 'Scheduler': oSch.state_dict()}\n",
    "                #!!!\n",
    "                torch.save(dCheckpoint, 'BestModel.pt')\n",
    "            except:\n",
    "                pass\n",
    "        print(' |')\n",
    "    \n",
    "    # Load best model (\"Early Stopping\")\n",
    "    dCheckpoint = torch.load('BestModel.pt')\n",
    "    oModel.load_state_dict(dCheckpoint['Model'])\n",
    "\n",
    "    return oModel, lTrainLoss, lTrainScore, lValLoss, lValScore, lLearnRate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Schedulers\n",
    "\n",
    "nIter         = nEpochs * len(dlTrain)\n",
    "baseLearnRate = 1e-2\n",
    "\n",
    "lScheduler = [\n",
    "    ('Constant', torch.optim.lr_scheduler.LinearLR, {'start_factor': 1.0}),\n",
    "    ('Linear', torch.optim.lr_scheduler.LinearLR, {'start_factor': 1.0, 'end_factor': 0.01, 'total_iters': nIter}),\n",
    "    ('Exponential', torch.optim.lr_scheduler.ExponentialLR, {'gamma': 0.997}),\n",
    "    ('Cosine', torch.optim.lr_scheduler.CosineAnnealingLR, {'T_max': nIter} ),\n",
    "    ('Cyclic', torch.optim.lr_scheduler.CyclicLR, {'base_lr': 1e-6, 'max_lr': baseLearnRate, 'step_size_up': nIter // 6, 'step_size_down': nIter // 6, 'mode': 'triangular2', 'cycle_momentum': False}),\n",
    "    ('OneCycle', torch.optim.lr_scheduler.OneCycleLR, {'max_lr': baseLearnRate, 'total_steps': nIter}),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> Some schedulers (For instance `OneCycleLR`) do not allow iterations beyond what is defined.\n",
    "* <font color='brown'>(**#**)</font> Some schedulers are score / loss event driven. See `torch.optim.ReduceLROnPlateau`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "\n",
    "dModelHist = {}\n",
    "\n",
    "for ii, (schedName, SchedCls, dSchedParam) in enumerate(lScheduler):\n",
    "    print(f'Training with the {schedName} scheduler')\n",
    "    oRunModel = copy.deepcopy(oModel)\n",
    "    oRunModel = oRunModel.to(runDevice) #<! Transfer model to device\n",
    "    oOpt = torch.optim.AdamW(oRunModel.parameters(), lr = baseLearnRate, betas = (0.9, 0.99), weight_decay = 1e-4) #<! Define optimizer\n",
    "    oScd = SchedCls(oOpt, **dSchedParam)\n",
    "    oTBLogger = TBLogger(logDir = os.path.join(TENSOR_BOARD_BASE, f'{schedName}'))\n",
    "    _, lTrainLoss, lTrainScore, lValLoss, lValScore, lLearnRate = TrainModel(oRunModel, dlTrain, dlTest, oOpt, oScd, nEpochs, hL, hS, oTBLogger)\n",
    "    dModelHist[schedName] = lTrainLoss, lTrainScore, lValLoss, lValScore, lLearnRate\n",
    "    oTBLogger.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> A tuned combination of the optimizer and scheduler hyper parameter might give a different result.\n",
    "* <font color='blue'>(**!**)</font> Display results: Learning Rate, Train Loss, Validation Score using MatPlotLib."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
