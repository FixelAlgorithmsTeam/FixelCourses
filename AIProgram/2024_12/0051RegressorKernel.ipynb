{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Fixel Algorithms](https://i.imgur.com/AqKHVZ0.png)](https://fixelalgorithms.gitlab.io)\n",
    "\n",
    "# AI Program\n",
    "\n",
    "## Machine Learning - Supervised Learning - Regression - Kernel Regression - Exercise\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 1.1.000 | 21/09/2025 | Royi Avital | Added error analysis by features                                   |\n",
    "| 1.0.002 | 11/03/2025 | Royi Avital | Added features description                                         |\n",
    "| 1.0.001 | 10/04/2024 | Royi Avital | Added a regression plot                                            |\n",
    "| 1.0.001 | 10/04/2024 | Royi Avital | Added note about SciKit Learn's `KernelDensity`                    |\n",
    "| 1.0.000 | 07/04/2024 | Royi Avital | First version                                                      |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/AIProgram/2024_02/0051RegressorKernel.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T09:30:06.492269Z",
     "start_time": "2022-02-02T09:30:06.220934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Miscellaneous\n",
    "from platform import python_version\n",
    "import random\n",
    "\n",
    "# Typing\n",
    "from typing import Callable, Dict, List, Optional, Self, Set, Tuple, Union\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "# Visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought.\n",
    "\n",
    "Code Notations:\n",
    "\n",
    "```python\n",
    "someVar    = 2; #<! Notation for a variable\n",
    "vVector    = np.random.rand(4) #<! Notation for 1D array\n",
    "mMatrix    = np.random.rand(4, 3) #<! Notation for 2D array\n",
    "tTensor    = np.random.rand(4, 3, 2, 3) #<! Notation for nD array (Tensor)\n",
    "tuTuple    = (1, 2, 3) #<! Notation for a tuple\n",
    "lList      = [1, 2, 3] #<! Notation for a list\n",
    "dDict      = {1: 3, 2: 2, 3: 1} #<! Notation for a dictionary\n",
    "oObj       = MyClass() #<! Notation for an object\n",
    "dfData     = pd.DataFrame() #<! Notation for a data frame\n",
    "dsData     = pd.Series() #<! Notation for a series\n",
    "hObj       = plt.Axes() #<! Notation for an object / handler / function handler\n",
    "```\n",
    "\n",
    "### Code Exercise\n",
    "\n",
    " - Single line fill\n",
    "\n",
    "```python\n",
    "valToFill = ???\n",
    "```\n",
    "\n",
    " - Multi Line to Fill (At least one)\n",
    "\n",
    "```python\n",
    "# You need to start writing\n",
    "?????\n",
    "```\n",
    "\n",
    " - Section to Fill\n",
    "\n",
    "```python\n",
    "#===========================Fill This===========================#\n",
    "# 1. Explanation about what to do.\n",
    "# !! Remarks to follow / take under consideration.\n",
    "mX = ???\n",
    "\n",
    "?????\n",
    "#===============================================================#\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# %matplotlib inline\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "# Matplotlib default color palette\n",
    "lMatPltLibclr = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "# sns.set_theme() #>! Apply SeaBorn theme\n",
    "\n",
    "runInGoogleColab = 'google.colab' in str(get_ipython())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "FIG_SIZE_DEF    = (8, 8)\n",
    "ELM_SIZE_DEF    = 50\n",
    "CLASS_COLOR     = ('b', 'r')\n",
    "EDGE_COLOR      = 'k'\n",
    "MARKER_SIZE_DEF = 10\n",
    "LINE_WIDTH_DEF  = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courses Packages\n",
    "\n",
    "from DataVisualization import PlotRegressionResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Auxiliary Functions\n",
    "\n",
    "def CosineKernel( vU: np.ndarray ) -> np.ndarray:\n",
    "    return (np.abs(vU) < 1) * (1 + np.cos(np.pi * vU))\n",
    "\n",
    "def GaussianKernel( vU: np.ndarray ) -> np.ndarray:\n",
    "    return np.exp(-0.5 * np.square(vU))\n",
    "\n",
    "def TriangularKernel( vU: np.ndarray ) -> np.ndarray:\n",
    "    return (np.abs(vU) < 1) * (1 - np.abs(vU))\n",
    "\n",
    "def UniformKernel( vU: np.ndarray ) -> np.ndarray:\n",
    "    return 1 * (np.abs(vU) < 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Regression\n",
    "\n",
    "In this exercise we'll build an estimator with the Sci Kit Learn API.  \n",
    "It will be based on the concept of Kernel Regression.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The _Kernel Regression_ is a non parametric method. The optimization is about its _hyper parameters_.\n",
    "\n",
    "\n",
    "We'll us the [Boston House Prices Dataset](https://www.openml.org/search?type=data&status=active&id=531) (See also [Kaggle - Boston House Prices](https://www.kaggle.com/datasets/vikrishnan/boston-house-prices)).  \n",
    "It has 13 features and one target. 2 of the features are categorical features.\n",
    "\n",
    "The objective is to estimate the `MEDV` of the estimation by optimizing the following hyper parameters:\n",
    " - The type of the kernel\n",
    " - The `h` parameter.\n",
    "\n",
    "I this exercise we'll do the following:\n",
    "\n",
    "1. Load the `Boston House Prices Dataset` data set using `fetch_openml()`.\n",
    "2. Create a an estimator (Regressor) class using SciKit API:\n",
    "  - Implement the constructor.\n",
    "  - Implement the `fit()`, `predict()` and `score()` methods.\n",
    "3. Optimize hyper parameters using _Leave One Out_ cross validation.\n",
    "4. Display the output of the model.\n",
    "\n",
    "We should get an _R2_ score above 0.75.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> In order to set the `h` parameter, one should have a look on the distance matrix of the data to get the relevant Dynamic Range of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "lKernelType = ['Cosine', 'Gaussian', 'Triangular', 'Uniform']\n",
    "#===========================Fill This===========================#\n",
    "# 1. Set the range of values of `h` (Bandwidth).\n",
    "lH          = ???\n",
    "#===============================================================#\n",
    "\n",
    "lKernels = [('Cosine', CosineKernel), ('Gaussian', GaussianKernel), ('Triangular', TriangularKernel), ('Uniform', UniformKernel)]\n",
    "\n",
    "# Data Visualization\n",
    "gridNoiseStd = 0.05\n",
    "numGridPts = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate / Load Data\n",
    "\n",
    "Loading the [Boston House Prices Dataset](https://www.openml.org/search?type=data&status=active&id=531) (See also [Kaggle - Boston House Prices](https://www.kaggle.com/datasets/vikrishnan/boston-house-prices)).  \n",
    "The data has 13 features and one target. 2 of the features are categorical features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Failing SSL Certificate ('[SSL: CERTIFICATE_VERIFY_FAILED')\n",
    "# In case `fetch_openml()` fails with SSL Certificate issue, run this.\n",
    "# import ssl\n",
    "# ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "\n",
    "dfX, dsY = fetch_openml('boston', version = 1, return_X_y = True, as_frame = True, parser = 'auto')\n",
    "\n",
    "print(f'The features data shape: {dfX.shape}')\n",
    "print(f'The labels data shape: {dsY.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Information\n",
    "\n",
    " - `CRIM`: Per capita crime rate by town.\n",
    " - `ZN`: Proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    " - `INDUS`: Proportion of non-retail business acres per town.\n",
    " - `CHAS`: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n",
    " - `NOX`: Nitric Oxides concentration (parts per 10 million).\n",
    " - `RM`: Average number of rooms per dwelling.\n",
    " - `AGE`: Proportion of owner-occupied units built prior to 1940.\n",
    " - `DIS`: Weighted distances to ﬁve Boston employment centers.\n",
    " - `RAD`: Index of accessibility to radial highways.\n",
    " - `TAX`: Full value property tax rate per $10,000.\n",
    " - `PTRATIO`: Pupil Teacher ratio by town 12. B: 1000(Bk−0.63)2 where Bk is the proportion of blacks by town 13. LSTAT: % lower status of the population.\n",
    " - `MEDV` (Target): Median value of owner occupied homes in $1000s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Features Data\n",
    "dfX.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Labels Data\n",
    "dsY.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info o the Data\n",
    "dfX.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Data\n",
    "\n",
    "# We'll display the correlation matrix of the data.\n",
    "# We'll add the target variable as the last variable (`MEDV`).\n",
    "\n",
    "dfData = pd.concat([dfX, dsY], axis = 1)\n",
    "\n",
    "hF, hA = plt.subplots(figsize = (10, 7))\n",
    "sns.heatmap(dfData.corr(numeric_only = True).abs(), annot = True, cmap = 'viridis', ax = hA)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Would you use the above to drop some features?\n",
    "* <font color='red'>(**?**)</font> Do we see all features above? How should we handle those missing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training / Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Training Data \n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Convert the `dfX` data frame into a matrix `mX`. Drop the categorical columns.\n",
    "# 2. Convert the `dsY` data frame into a vector `vY`.\n",
    "# !! You may use the `to_numpy()` method useful.\n",
    "mX = ??? #<! Drop the categorical data\n",
    "vY = ???\n",
    "#===============================================================#\n",
    "\n",
    "print(f'The features data shape: {mX.shape}')\n",
    "print(f'The labels data shape: {vY.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> The training is done with _K-Fold Cross Validation_ hence no need for actual split of the data.\n",
    "* <font color='brown'>(**#**)</font> We dropped the `CHAS` feature which is binary.  \n",
    "Binary features are like _One Hot Encoding_, so we could keep it.\n",
    "* <font color='red'>(**?**)</font> Why are binary features good as input while multi value categorical are not? Think about metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the Data\n",
    "# Since we use `h` it makes sense to keep the dynamic range fo values in tact.\n",
    "# In this case we'll center the data and normalize to have a unit standard deviation.\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Construct the scaler using `StandardScaler` class.\n",
    "# 2. Apply the scaler on the data.\n",
    "oStdScaler = ???\n",
    "mX = ???\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Regressor\n",
    "\n",
    "The kernel regression operation is defined by:\n",
    "\n",
    "$$\\boxed{\\hat{f} \\left( x \\right) = \\frac{\\sum_{i = 1}^{N} w_{x} \\left( {x}_{i} \\right) {y}_{i}}{\\sum_{i = 1}^{N} {w}_{x} \\left( {x}_{i} \\right)}}$$\n",
    "\n",
    "Where ${w}_{x} \\left( {x}_{i} \\right) = k \\left( \\frac{ x - x_{i} }{ h } \\right)$.\n",
    "\n",
    "In this exercise we'll use Leave One Out validation policy with the `cross_val_predict()` function.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Regression Estimator\n",
    "\n",
    "We could create the linear polynomial fit estimator using a `Pipeline` of `PolynomialFeatures` and `LinearRegression`.  \n",
    "Yet since this is a simple task it is a good opportunity to exercise the creation of a _SciKit Estimator_.\n",
    "\n",
    "We need to provide 4 main methods:\n",
    "\n",
    "1. The `__init()__` Method: The constructor of the object. It should set the kernel type and parameter `h`.\n",
    "2. The `fit()` Method: The pre processing phase. It keeps a **copy** of the fit data. It should set the `h` parameter if not set.\n",
    "3. The `predict()` Method: Prediction of the values.\n",
    "4. The `score()` Method: Calculates the _R2_ score.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> Make sure you read and understand the `ApplyKernelRegression()` function below.\n",
    "* <font color='brown'>(**#**)</font> A similar model is implemented as [`KernelDensity`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KernelDensity.html) class in SciKit Learn.  \n",
    "Yet it is not equivalent as in SciKit Learn it is used only to estimate a _PDF_ given data and sample from it. There is no `predict()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Kernel Regression\n",
    "# Applies the regression given a callable kernel.\n",
    "# It avoids division by 0 in case no reference points are given within the kernel domain.\n",
    "\n",
    "def ApplyKernelRegression( hKernel: Callable[[NDArray], NDArray], paramH: float, mG: NDArray, vY: NDArray, mX: NDArray, metricType: str = 'euclidean', zeroThr: float = 1e-9 ) -> NDArray:\n",
    "\n",
    "    mD = sp.spatial.distance.cdist(mX, mG, metric = metricType)\n",
    "    mW = hKernel(mD / paramH)\n",
    "    vK = mW @ vY #<! For numerical stability, removing almost zero values\n",
    "    vW = np.sum(mW, axis = 1)\n",
    "    vI = np.abs(vW) < zeroThr #<! Calculate only when there's real data\n",
    "    vK[vI] = 0\n",
    "    vW[vI] = 1 #<! Remove cases of dividing by 0\n",
    "    vYPred = vK / vW\n",
    "\n",
    "    return vYPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Kernel Regressor Class\n",
    "\n",
    "class KernelRegressor(RegressorMixin, BaseEstimator):\n",
    "    def __init__(self, kernelType: str = 'Gaussian', paramH: float = None, metricType: str = 'euclidean', lKernels: List = lKernels):\n",
    "        #===========================Fill This===========================#\n",
    "        # 1. Add `kernelType` as an attribute of the object.\n",
    "        # 2. Define the kernel from `lKernels` as `self.hKernel`.\n",
    "        # 3. Add `paramH` as an attribute of the object.\n",
    "\n",
    "        # !! Verify the input string of the kernel is within `lKernels`.\n",
    "        self.kernelType = ???\n",
    "        self.hKernel    = ??? #<! After verifying `kernelType` is within `lKernels`\n",
    "        self.paramH     = ???\n",
    "        #===============================================================#\n",
    "        # We must set all input parameters as attributes\n",
    "        self.metricType = metricType\n",
    "        self.lKernels   = lKernels\n",
    "        \n",
    "    \n",
    "    def fit(self, mX: NDArray, vY: NDArray) -> Self:\n",
    "        \n",
    "        if np.ndim(mX) != 2:\n",
    "            raise ValueError(f'The input `mX` must be an array like of size (n_samples, n_features) !')\n",
    "        \n",
    "        if mX.shape[0] !=  vY.shape[0]:\n",
    "            raise ValueError(f'The input `mX` must be an array like of size (n_samples, n_features) and `vY` must be (n_samples) !')\n",
    "        \n",
    "        #===========================Fill This===========================#\n",
    "        # 1. Extract the number of samples.\n",
    "        # 2. Set the bandwidth using Silverman's rule of thumb if it is not set (`None`).\n",
    "        # 3. Keep a copy of `mX` as a reference grid of features `mG`.\n",
    "        # 4. Keep a copy of `vY` as a reference values.\n",
    "        numSamples = ???\n",
    "        if self.paramH is None:\n",
    "            # Using Silverman's rule of thumb.\n",
    "            # It is optimized for Density Estimation for Univariate Gaussian like data.\n",
    "            σ = ???\n",
    "            self.paramH = 1.06 * σ * (numSamples ** (-0.2))\n",
    "        \n",
    "        self.mG = ??? #<! Copy!\n",
    "        self.vY = ??? #<! Copy!\n",
    "        #===============================================================#\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, mX: NDArray) -> NDArray:\n",
    "\n",
    "        if np.ndim(mX) != 2:\n",
    "            raise ValueError(f'The input `mX` must be an array like of size (n_samples, n_features) !')\n",
    "\n",
    "        if mX.shape[1] != self.mG.shape[1]:\n",
    "            raise ValueError(f'The input `mX` must be an array like of size (n_samples, n_features) where `n_features` matches the number of feature in `fit()` !')\n",
    "\n",
    "        return ApplyKernelRegression(self.hKernel, self.paramH, self.mG, self.vY, mX, self.metricType)\n",
    "    \n",
    "    def score(self, mX: NDArray, vY: NDArray) -> float:\n",
    "        # Return the R2 as the score\n",
    "\n",
    "        if (np.size(vY) != np.size(mX, axis = 0)):\n",
    "            raise ValueError(f'The number of samples in `mX` must match the number of labels in `vY`.')\n",
    "\n",
    "        #===========================Fill This===========================#\n",
    "        # 1. Apply the prediction on the input features.\n",
    "        # 2. Calculate the R2 score (You may use `r2_score()`).\n",
    "        vYPred  = ???\n",
    "        valR2   = ???\n",
    "        #===============================================================#\n",
    "\n",
    "        return valR2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Model and Optimize Hyper Parameters\n",
    "\n",
    "In this section we'll optimize the model according to the `R2` score.  \n",
    "\n",
    "We'll use the `r2_score()` function to calculate the score.  \n",
    "The process to optimize the _Hyper Parameters_ will be as following:\n",
    "\n",
    "1. Build a data frame to keep the scoring of the different hyper parameters combination.\n",
    "2. Optimize the model:\n",
    "  - Construct a model using the current combination of _hyper parameters_.\n",
    "  - Apply a cross validation process to predict the data using `cross_val_predict()`.\n",
    "  - As the cross validation iterator (The `cv` parameter) use `KFold` to implement _Leave One Out_ policy.\n",
    "3. Calculate the score of the predicted classes.\n",
    "4. Store the result in the performance data frame.\n",
    "\n",
    "\n",
    "* <font color='red'>(**?**)</font> While the `R2` score is used to optimize the Hyper Parameter, what loss is used to optimize the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Data Frame\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Calculate the number of combinations.\n",
    "# 2. Create a nested loop to create the combinations between the parameters.\n",
    "# 3. Store the combinations as the columns of a data frame.\n",
    "\n",
    "# For Advanced Python users: Use iteration tools for create the cartesian product\n",
    "numComb = ???\n",
    "dData   = ???\n",
    "\n",
    "?????\n",
    "#===============================================================#\n",
    "\n",
    "dfModelScore = pd.DataFrame(data = dData)\n",
    "dfModelScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the Model\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Iterate over each row of the data frame `dfModelScore`. Each row defines the hyper parameters.\n",
    "# 2. Construct the model.\n",
    "# 3. Train it on the Train Data Set.\n",
    "# 4. Calculate the score.\n",
    "# 5. Store the score into the data frame column.\n",
    "\n",
    "for ii in range(numComb):\n",
    "    kernelType = ???\n",
    "    paramH     = ???\n",
    "\n",
    "    print(f'Processing model {ii + 1:03d} out of {numComb} with `Kernel Type` = {kernelType} and `h` = {paramH}.')\n",
    "\n",
    "    oKerReg = ???\n",
    "    \n",
    "    vYPred = ???\n",
    "\n",
    "    scoreR2 = ???\n",
    "    dfModelScore.loc[ii, 'R2'] = ???\n",
    "    print(f'Finished processing model {ii + 1:03d} with `R2 = {scoreR2}.')\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Sorted Results (Descending)\n",
    "# Pandas allows sorting data by any column using the `sort_values()` method\n",
    "# The `head()` allows us to see only the the first values\n",
    "dfModelScore.sort_values(by = ['R2'], ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Train Data F1 as a Heat Map\n",
    "# We can pivot the data set created to have a 2D matrix of the score as a function of parameters.\n",
    "\n",
    "hF, hA = plt.subplots(figsize = (9, 9))\n",
    "\n",
    "# hA = sns.heatmap(data = dfModelScore.pivot(index = 'h', columns = 'Kernel Type', values = 'R2'), robust = True, linewidths = 1, annot = True, fmt = '0.2f', norm = LogNorm(), ax = hA)\n",
    "hA = sns.heatmap(data = dfModelScore.pivot(index = 'h', columns = 'Kernel Type', values = 'R2'), robust = True, linewidths = 1, annot = True, fmt = '0.2f', cmap = 'viridis', ax = hA)\n",
    "hA.set_title('R2 of the Cross Validation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> What's the actual model in production for Kernel Regression?\n",
    "* <font color='brown'>(**#**)</font> In production we'd extract the best hyper parameters and then train again on the whole data.\n",
    "* <font color='brown'>(**#**)</font> Usually, for best hyper parameters, it is better to use cross validation with low number of folds.  \n",
    "Using Leave One Out is better for estimating real world performance. The logic is that the best hyper parameters should be selected when they are tested with low correlation of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Results\n",
    "\n",
    "Results of the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal Model\n",
    "\n",
    "# Extract best model Hyper Parameters\n",
    "bestModelIdx = dfModelScore['R2'].idxmax()\n",
    "kernelType   = dfModelScore.loc[bestModelIdx, 'Kernel Type']\n",
    "paramH       = dfModelScore.loc[bestModelIdx, 'h']\n",
    "\n",
    "# Construct & Train best model\n",
    "oKerReg = KernelRegressor(kernelType = kernelType, paramH = paramH)\n",
    "oKerReg = oKerReg.fit(mX, vY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Regression Results\n",
    "\n",
    "hF, hA = plt.subplots(figsize = (10, 8))\n",
    "hA = PlotRegressionResults(vY, oKerReg.predict(mX), hA = hA)\n",
    "hA.set_xlabel('Input Price [1000$]')\n",
    "hA.set_ylabel('Predicted Price [1000$]');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Is the error uniform along the price?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis per Feature\n",
    "\n",
    "We can analyze the performance of the model with regard of the features.  \n",
    "The idea is plot a 2D scatter with ${x}_{1}$ and ${x}_{2}$ as features and the size and color of the marker to represent the error.  \n",
    "This will allow identifying patterns which generates higher error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Data Frame for Analysis per Feature\n",
    "featureName001 = 'CRIM'\n",
    "featureName002 = 'TAX'\n",
    "\n",
    "featureName001 = 'ZN'\n",
    "featureName002 = 'NOX'\n",
    "\n",
    "featureName001 = 'AGE'\n",
    "featureName002 = 'PTRATIO'\n",
    "\n",
    "dfFeatAnl = pd.DataFrame(data = {featureName001: dfData[featureName001], featureName002: dfX[featureName002], 'True Price': vY, 'Predicted Price': oKerReg.predict(mX), 'AbsError': np.abs(vY - oKerReg.predict(mX))})\n",
    "dfFeatAnl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Scatter of Error per Feature\n",
    "hF, hA = plt.subplots(figsize = (9, 6))\n",
    "sns.scatterplot(data = dfFeatAnl, x = featureName001, y = featureName002, size = 'AbsError', hue = 'AbsError', palette = 'viridis', sizes = (20, 200), alpha = 0.7, edgecolor = 'k', ax = hA)\n",
    "hA.set_title('Error Analysis per Feature');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> What can be done given the results?\n",
    "\n",
    "<!-- If one can generate a simple rule which classify cases with large error, a dedicated model for those cases can be added. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized Cross Validation for Kernel Regression\n",
    "\n",
    "One way to optimize the process is by pre calculating the distance matrix for the whole data.  \n",
    "Then using a sub set of it according to the subset for training.\n",
    "\n",
    "For instance, let's recreate the _Leave One Out_:\n",
    "\n",
    "1. Calculate the Distance Matrix $\\boldsymbol{D}_{x} \\in \\mathbb{R}^{N \\times N}$ such that $ \\boldsymbol{D}_{x} \\left[ i, j \\right] = \\left\\| \\boldsymbol{x}_{i}-\\boldsymbol{x}_{j} \\right\\| _{2}$.\n",
    "2. Calculate the weights matrix $\\boldsymbol{W} \\in \\mathbb{R}^{N \\times N}$ such that $\\boldsymbol{W} \\left[ i, j \\right] = k \\left( \\frac{1}{h} \\boldsymbol{D}_{x} \\left[ i, j \\right] \\right)$.\n",
    "3. Estimate $\\boldsymbol{x}_{i}$ without using $\\boldsymbol{x}_{i}$ we set $\\boldsymbol{W} \\left[ i, i \\right] = 0$.\n",
    "4. Apply kernel regression $\\hat{\\boldsymbol{y}} = \\left( \\boldsymbol{W} \\boldsymbol{y} \\right) \\oslash \\left( \\boldsymbol{W} \\boldsymbol{1} \\right)$. Where $\\oslash$ is element wise division."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorized Leave One Out Kernel Regression\n",
    "\n",
    "numSamples = len(vY) #<! Number of Samples\n",
    "hK = GaussianKernel #<! Kernel\n",
    "paramH = 0.3 #<! Bandwidth\n",
    "mD = sp.spatial.distance.cdist(mX, mX, metric = 'euclidean') #<! Distance Matrix\n",
    "mW = hK(mD / paramH) #<! Weights matrix\n",
    "\n",
    "# Zeroing the diagonal to prevent the weight of the sample\n",
    "mW[range(numSamples), range(numSamples)] = 0 #<! Leave One Out\n",
    "vYPred = (mW @ vY) / np.sum(mW, axis = 1) #<! Kernel Regression\n",
    "\n",
    "print(f'The Leave One Out R2 Score for Kernel Type = {hK} and Bandwidth = {paramH} is {r2_score(vY, vYPred) }.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='green'>(**@**)</font> Plot the regression of the best model (See previous notebooks)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
