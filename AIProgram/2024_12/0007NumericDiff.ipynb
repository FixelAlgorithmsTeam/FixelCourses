{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Fixel Algorithms](https://i.imgur.com/AqKHVZ0.png)](https://fixelalgorithms.gitlab.io/)\n",
    "\n",
    "# AI Program\n",
    "\n",
    "## Essential Matrix Calculus - Numerical Differentiation\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 1.0.000 | 06/02/2024 | Royi Avital | First version                                                      |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/AIProgram/2024_02/0007NumericDiff.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T09:30:06.492269Z",
     "start_time": "2022-02-02T09:30:06.220934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "import autograd.numpy as anp\n",
    "from autograd import grad\n",
    "from autograd import elementwise_grad as egrad\n",
    "\n",
    "# Miscellaneous\n",
    "import os\n",
    "import math\n",
    "from platform import python_version\n",
    "import random\n",
    "\n",
    "# Typing\n",
    "from typing import Callable, List, Tuple, Union\n",
    "\n",
    "# Visualization\n",
    "from matplotlib.colors import LogNorm, Normalize, PowerNorm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython\n",
    "from IPython.display import Image, display\n",
    "from ipywidgets import Dropdown, FloatSlider, interact, IntSlider, Layout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought.\n",
    "\n",
    "Code Notations:\n",
    "\n",
    "```python\n",
    "someVar    = 2; #<! Notation for a variable\n",
    "vVector    = np.random.rand(4) #<! Notation for 1D array\n",
    "mMatrix    = np.random.rand(4, 3) #<! Notation for 2D array\n",
    "tTensor    = np.random.rand(4, 3, 2, 3) #<! Notation for nD array (Tensor)\n",
    "tuTuple    = (1, 2, 3) #<! Notation for a tuple\n",
    "lList      = [1, 2, 3] #<! Notation for a list\n",
    "dDict      = {1: 3, 2: 2, 3: 1} #<! Notation for a dictionary\n",
    "oObj       = MyClass() #<! Notation for an object\n",
    "dfData     = pd.DataFrame() #<! Notation for a data frame\n",
    "dsData     = pd.Series() #<! Notation for a series\n",
    "hObj       = plt.Axes() #<! Notation for an object / handler / function handler\n",
    "```\n",
    "\n",
    "### Code Exercise\n",
    "\n",
    " - Single line fill\n",
    "\n",
    " ```python\n",
    " vallToFill = ???\n",
    " ```\n",
    "\n",
    " - Multi Line to Fill (At least one)\n",
    "\n",
    " ```python\n",
    " # You need to start writing\n",
    " ????\n",
    " ```\n",
    "\n",
    " - Section to Fill\n",
    "\n",
    "```python\n",
    "#===========================Fill This===========================#\n",
    "# 1. Explanation about what to do.\n",
    "# !! Remarks to follow / take under consideration.\n",
    "mX = ???\n",
    "\n",
    "???\n",
    "#===============================================================#\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "%matplotlib inline\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "# Matplotlib default color palette\n",
    "lMatPltLibclr = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "# sns.set_theme() #>! Apply SeaBorn theme\n",
    "# sns.set_palette(\"tab10\")\n",
    "\n",
    "runInGoogleColab = 'google.colab' in str(get_ipython())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "FIG_SIZE_DEF    = (8, 8)\n",
    "ELM_SIZE_DEF    = 50\n",
    "CLASS_COLOR     = ('b', 'r')\n",
    "EDGE_COLOR      = 'k'\n",
    "MARKER_SIZE_DEF = 10\n",
    "LINE_WIDTH_DEF  = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Course Packages\n",
    "\n",
    "from NumericDiff import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Differentiation\n",
    "\n",
    "This notebook explores the use of [_Numerical Differentiation_](https://en.wikipedia.org/wiki/Numerical_differentiation) to calculate the gradient of a function.\n",
    "\n",
    "The gradient of a multivariate scalar function, $f : \\mathbb{R}^{n} \\to \\mathbb{R}$, is given by:\n",
    "\n",
    "$$ {{\\nabla}_{x} f \\left( \\boldsymbol{x} \\right)}_{i} = \\lim_{t \\to 0} \\frac{ f \\left( \\boldsymbol{x} + t \\boldsymbol{e}_{i} \\right) - f \\left( \\boldsymbol{x} \\right) }{t} $$\n",
    "\n",
    "Where $\\boldsymbol{e}_{i} = \\left[ 0, 0, \\ldots, 0, \\underbrace{1}_{\\text{i -th index}}, 0, \\ldots, 0 \\right]$. \n",
    "\n",
    "This can be approximated by [_Finite Difference_](https://en.wikipedia.org/wiki/Finite_difference) with specific [_Finite Difference Coefficient_](https://en.wikipedia.org/wiki/Finite_difference_coefficient).  \n",
    "There 3 common approaches:\n",
    "\n",
    " - Forward: ${{\\nabla}_{x} f \\left( \\boldsymbol{x} \\right)}_{i} \\approx \\frac{ f \\left( \\boldsymbol{x} + h \\boldsymbol{e}_{i} \\right) - f \\left( \\boldsymbol{x} \\right) }{h}$.\n",
    " - Backward: ${{\\nabla}_{x} f \\left( \\boldsymbol{x} \\right)}_{i} \\approx \\frac{ f \\left( \\boldsymbol{x} \\right) - f \\left( \\boldsymbol{x} - h \\boldsymbol{e}_{i} \\right) }{h}$.\n",
    " - Central: ${{\\nabla}_{x} f \\left( \\boldsymbol{x} \\right)}_{i} \\approx \\frac{ f \\left( \\boldsymbol{x} + h \\boldsymbol{e}_{i} \\right) - f \\left( \\boldsymbol{x} - h \\boldsymbol{e}_{i} \\right) }{2 h}$.\n",
    "\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The notebook use the `NumericDiff.py` file for the actual calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Size Sensitivity Analysis\n",
    "\n",
    "In this section we'll analyze the sensitivity of the numerical differentiation to the step size, $h$.\n",
    "\n",
    "We'll use the function:\n",
    "\n",
    "$$ f \\left( \\boldsymbol{X} \\right) = \\left \\langle \\boldsymbol{A}, \\sin \\left[ \\boldsymbol{X} \\right] \\right \\rangle $$\n",
    "\n",
    "Where:\n",
    "\n",
    " - $\\boldsymbol{X} \\in \\mathbb{R}^{d \\times d}$.\n",
    " - The function $\\sin \\left[ \\cdot \\right]$ is the element wise $\\sin$ function: $\\boldsymbol{M} = \\sin \\left[ \\boldsymbol{X} \\right] \\implies \\boldsymbol{M} \\left[ i, j \\right] = \\sin \\left( \\boldsymbol{X} \\left[ i, j\\right] \\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla f \\left( X \\right) \\left[ \\boldsymbol{H} \\right] & = \\left \\langle A, \\left( \\cos \\left[ X \\right] \\right) \\circ H \\right \\rangle && \\text{Since $\\frac{d \\sin \\left( x \\right)}{dx} = \\cos \\left( x \\right)$} \\\\\n",
    "& = \\left \\langle \\cos \\left[ \\boldsymbol{X} \\right] \\circ \\boldsymbol{A}, H \\right \\rangle && \\text{Adjoint} \\\\\n",
    "& \\Rightarrow \\nabla f \\left( X \\right) = \\cos \\left[ \\boldsymbol{X} \\right] \\circ A\n",
    "&& \\blacksquare\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> The function can be evaluated efficiently using element wise multiplication and summation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "numSteps = 1000\n",
    "\n",
    "numRows = 5\n",
    "numCols = 1; #<! Like a vector\n",
    "\n",
    "vStepSize = np.logspace(-4, -11, numSteps)\n",
    "\n",
    "lMethods    = [DiffMode.BACKWARD, DiffMode.CENTRAL, DiffMode.FORWARD]\n",
    "lMethodName = ['Forward', 'Backward', 'Central']\n",
    "\n",
    "# Data \n",
    "mA = np.random.randn(numRows, numCols)\n",
    "mX = np.random.randn(numRows, numCols)\n",
    "\n",
    "# Function\n",
    "hF = lambda mX: np.sum(mA * np.sin(mX))\n",
    "\n",
    "# Analytic Gradient\n",
    "hGradF = lambda mX: np.cos(mX) * mA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensitivity Analysis\n",
    "\n",
    "numMethods = len(lMethods)\n",
    "\n",
    "vG = hGradF(mX)\n",
    "mE = np.zeros(shape = (numSteps, numMethods)) #<! Error\n",
    "\n",
    "for jj in range(numMethods):\n",
    "  for ii in range(numSteps):\n",
    "    mE[ii, jj] = 20 * np.log10(np.linalg.norm(vG - CalcFunGrad(mX, hF, diffMode = lMethods[jj], ε = vStepSize[ii]), np.inf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Results\n",
    "\n",
    "hF, hA = plt.subplots(figsize = (16, 8))\n",
    "\n",
    "for ii in range(numMethods):\n",
    "  hA.plot(vStepSize, mE[:, ii], lw = 2, label = f'{lMethodName[ii]}')\n",
    "\n",
    "hA.set_title('Numerical Differentiation Error - Max Absolute Error')\n",
    "hA.set_xlabel('Step Size')\n",
    "hA.set_ylabel('Error [dB]')\n",
    "\n",
    "hA.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Complex Step Trick\n",
    "\n",
    "In general, the finite differences step size si a function of the argument and the function itself.  \n",
    "There are many cases where the method becomes highly sensitive and with the finite floating point accuracy it might cause some errors.\n",
    "\n",
    "It turns out that for _real analytic functions_ (Think of a convergent Taylor Series) we can do a trick:\n",
    "\n",
    "$$ f \\left( x + ih \\right) = f \\left( x \\right) + f' \\left( x \\right) i h + \\frac{f'' \\left( x \\right)}{2} {\\left(ih \\right)}^{2} + \\mathcal{O}(h^3) \\implies \\mathrm{Im} \\,\\left( \\frac{ f \\left( x + ih \\right)}{h} \\right) = f' \\left( x \\right) + \\mathcal{O}(h^2). $$\n",
    "\n",
    "Which is much more stable regardless of the value of the step size.\n",
    "\n",
    "Yet, there are some cases to handle:\n",
    " - Use `abs()` which uses the definition `abs(x + i y) = sign(x) * (x + i y)`.\n",
    " - Use `min()` / `max()` which only use the real part for comparison.\n",
    " - Use `.'` instead of `'` to apply _transpose_ instead of _hermitian transpose_.\n",
    "\n",
    "Resources:\n",
    " - [Sebastien Boisgerault - Complex Step Differentiation](https://direns.mines-paristech.fr/Sites/Complex-analysis/Complex-Step%20Differentiation/).\n",
    " - [Nick Higham - What Is the Complex Step Approximation](https://nhigham.com/2020/10/06/what-is-the-complex-step-approximation/).\n",
    " - [Derek Elkins - Complex Step Differentiation](https://www.hedonisticlearning.com/posts/complex-step-differentiation.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "In order to verify the robustness of the problem we'll use:\n",
    "\n",
    "$$ f \\left( x \\right) = {e}^{x} $$\n",
    "\n",
    "At $x = 0$, which will allow us to use a perfect reference and the relative error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "numSteps = 1500\n",
    "\n",
    "vStepSize = np.logspace(-3, -15, numSteps)\n",
    "\n",
    "lMethods    = [DiffMode.BACKWARD, DiffMode.CENTRAL, DiffMode.FORWARD, DiffMode.COMPLEX]\n",
    "lMethodName = ['Forward', 'Backward', 'Central', 'Complex']\n",
    "\n",
    "# Data \n",
    "valX = 0.0\n",
    "\n",
    "# Function\n",
    "hF = lambda x: np.exp(x)\n",
    "\n",
    "# Analytic Gradient\n",
    "gradF = 1; #<! At x = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensitivity Analysis\n",
    "\n",
    "numMethods = len(lMethods)\n",
    "\n",
    "mE = np.zeros(shape = (numSteps, numMethods)) #<! Error\n",
    "\n",
    "for jj in range(numMethods):\n",
    "  for ii in range(numSteps):\n",
    "    mE[ii, jj] = 20 * np.log10(abs(gradF - CalcFunGrad(valX, hF, diffMode = lMethods[jj], ε = vStepSize[ii])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Results\n",
    "\n",
    "hF, hA = plt.subplots(figsize = (16, 8))\n",
    "\n",
    "for ii in range(numMethods):\n",
    "  hA.plot(vStepSize, mE[:, ii], lw = 2, label = f'{lMethodName[ii]}')\n",
    "\n",
    "hA.set_title('Numerical Differentiation Error - Relative Error')\n",
    "hA.set_xlabel('Step Size')\n",
    "hA.set_ylabel('Error [dB]')\n",
    "hA.set_xscale('log')\n",
    "hA.invert_xaxis()\n",
    "\n",
    "hA.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
