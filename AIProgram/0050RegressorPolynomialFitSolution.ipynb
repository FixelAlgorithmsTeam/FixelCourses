{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Fixel Algorithms](https://i.imgur.com/AqKHVZ0.png)](https://fixelalgorithms.gitlab.io)\n",
    "\n",
    "# AI Program\n",
    "\n",
    "## Machine Learning - Supervised Learning - Regression - Decision Tree Regression - Exercise\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 1.0.000 | 31/12/2025 | Royi Avital | First version                                                      |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/AIProgram/2024_02/0046RegressorPolynomialFit.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T09:30:06.492269Z",
     "start_time": "2022-02-02T09:30:06.220934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Miscellaneous\n",
    "from platform import python_version\n",
    "import random\n",
    "\n",
    "# Typing\n",
    "from typing import Callable, Dict, List, Optional, Self, Set, Tuple, Union\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought.\n",
    "\n",
    "Code Notations:\n",
    "\n",
    "```python\n",
    "someVar    = 2; #<! Notation for a variable\n",
    "vVector    = np.random.rand(4) #<! Notation for 1D array\n",
    "mMatrix    = np.random.rand(4, 3) #<! Notation for 2D array\n",
    "tTensor    = np.random.rand(4, 3, 2, 3) #<! Notation for nD array (Tensor)\n",
    "tuTuple    = (1, 2, 3) #<! Notation for a tuple\n",
    "lList      = [1, 2, 3] #<! Notation for a list\n",
    "dDict      = {1: 3, 2: 2, 3: 1} #<! Notation for a dictionary\n",
    "oObj       = MyClass() #<! Notation for an object\n",
    "dfData     = pd.DataFrame() #<! Notation for a data frame\n",
    "dsData     = pd.Series() #<! Notation for a series\n",
    "hObj       = plt.Axes() #<! Notation for an object / handler / function handler\n",
    "```\n",
    "\n",
    "### Code Exercise\n",
    "\n",
    " - Single line fill\n",
    "\n",
    "```python\n",
    "valToFill = ???\n",
    "```\n",
    "\n",
    " - Multi Line to Fill (At least one)\n",
    "\n",
    "```python\n",
    "# You need to start writing\n",
    "?????\n",
    "```\n",
    "\n",
    " - Section to Fill\n",
    "\n",
    "```python\n",
    "#===========================Fill This===========================#\n",
    "# 1. Explanation about what to do.\n",
    "# !! Remarks to follow / take under consideration.\n",
    "mX = ???\n",
    "\n",
    "?????\n",
    "#===============================================================#\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# %matplotlib inline\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "# Matplotlib default color palette\n",
    "lMatPltLibclr = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "# sns.set_theme() #>! Apply SeaBorn theme\n",
    "\n",
    "runInGoogleColab = 'google.colab' in str(get_ipython())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "FIG_SIZE_DEF    = (8, 8)\n",
    "ELM_SIZE_DEF    = 50\n",
    "CLASS_COLOR     = ('b', 'r')\n",
    "EDGE_COLOR      = 'k'\n",
    "MARKER_SIZE_DEF = 10\n",
    "LINE_WIDTH_DEF  = 2\n",
    "\n",
    "PEOPLE_CSV_URL = 'https://github.com/FixelAlgorithmsTeam/FixelCourses/raw/master/DataSets/People.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courses Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Auxiliary Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Fit\n",
    "\n",
    "This notebooks compares the performance of a _Regression Decision Tree_ vs. a _Linear Regressor_.  \n",
    "\n",
    "The data is based on [`People.csv`](https://github.com/FixelAlgorithmsTeam/FixelCourses/blob/master/DataSets/People.csv) data set.  \n",
    "It includes 1000 samples of peoples: Sex, Age, Height [CM], Weight [KG].  \n",
    "\n",
    "The objective is to estimate the weight given the height and sex.  \n",
    "The Sex is a categorical feature which is to work with with Decision Tree while Linear Model sometimes struggle with.\n",
    "\n",
    "This notebook goes through:\n",
    "\n",
    "1. Load the [`People.csv`](https://github.com/FixelAlgorithmsTeam/FixelCourses/blob/master/DataSets/People.csv) data set using `pd.csv_read()`.\n",
    "2. Build a baseline regressor based on a _pipeline_ of _Polynomial Features_ and _Linear Regression_.\n",
    "3. Build a regressor based on (Ensemble) Decision Tree Regressor.\n",
    "4. Optimize the Hyper Parameters of both.\n",
    "4. Compare results.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> In order to let the classifier know the data is binary / categorical we'll use a **Data Frame** as the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# Training Parameters\n",
    "numFolds = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate / Load Data\n",
    "\n",
    "Loads the online `csv` file directly as a Data Frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "\n",
    "dfPeople = pd.read_csv(PEOPLE_CSV_URL)\n",
    "dfPeople['Sex'] = dfPeople['Sex'].map({'f': 'Female', 'm': 'Male'})\n",
    "\n",
    "dfPeople.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair Plot\n",
    "\n",
    "sns.pairplot(data = dfPeople, hue = 'Sex')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> How would you model the data for the task of estimation of the weight of a person given his sex, age and height?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Training Data \n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Extract the 'Sex' and 'Height' columns into a dataframe `dfX`.\n",
    "# 2. Extract the 'Weight' column into a series `dsY`.\n",
    "dfX = dfPeople[['Sex', 'Height']].copy()\n",
    "dsY = dfPeople['Weight'].copy()\n",
    "#===============================================================#\n",
    "\n",
    "print(f'The features data shape: {dfX.shape}')\n",
    "print(f'The labels data shape: {dsY.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Data\n",
    "\n",
    "hF, hA = plt.subplots(figsize = FIG_SIZE_DEF)\n",
    "sns.scatterplot(data = dfPeople, x = 'Height', y = 'Weight', hue = 'Sex', ax = hA);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Which polynomial order fits the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert String to Numeric\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Convert the 'Sex' column from string to numeric.\n",
    "# 2. Define the `Sex` column as categorical.\n",
    "# !! You may find the method `astype(category)` useful.\n",
    "dfX['Sex'] = dfX['Sex'].map({'Female': 0, 'Male': 1})\n",
    "dfX['Sex'] = dfX['Sex'].astype('category') #<! LightGBM can handle categorical variables directly\n",
    "#===============================================================#\n",
    "\n",
    "dfX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Frame Info\n",
    "# Should show that the 'Sex' column is categorical\n",
    "dfX.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressors\n",
    "\n",
    "This section implements 2 types of regressors:\n",
    "\n",
    " - Parametric Regressor  \n",
    "   In the form of Polynomial Regression model.\n",
    " - Non Parametric Regressor  \n",
    "   In the form of (Ensemble) Regression Decision Tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regressor\n",
    "\n",
    "The PolyFit optimization problem is given by:\n",
    "\n",
    "$$ \\arg \\min_{\\boldsymbol{w}} {\\left\\| \\boldsymbol{\\Phi} \\boldsymbol{w} - \\boldsymbol{y} \\right\\|}_{2}^{2} $$\n",
    "\n",
    "Where\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\Phi} = \\begin{bmatrix} — & {p}_{d} \\left( \\boldsymbol{x}_{1} \\right) & — \\\\\n",
    "— & {p}_{d} \\left( \\boldsymbol{x}_{2} \\right) & — \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "— & {p}_{d} \\left( \\boldsymbol{x}_{n} \\right) & —\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Where ${p}_{d} \\left( \\cdot \\right)$ is a transformer of the sample vector to a polynomial features of degree $d$.  \n",
    "\n",
    "This section implements the _Ridge Regression_ model:\n",
    "\n",
    "$$ \\arg \\min_{\\boldsymbol{w}} {\\left\\| \\boldsymbol{\\Phi} \\boldsymbol{w} - \\boldsymbol{y} \\right\\|}_{2}^{2} + \\lambda {\\left\\| \\boldsymbol{w} \\right\\|}_{2}^{2} $$\n",
    "\n",
    "Where $\\lambda$ is the _Regularization Factor_.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> For arbitrary $\\Phi$ the above becomes a _linear regression_ problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression Model with Polynomial Features\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Build a pipeline based on `PolynomialFeatures` and `Ridge`.\n",
    "# !! Set configuration to optimize the memory efficiency.\n",
    "# !! The degree of the polynomial features and regularization will be set by Hyper Parameter tuning.\n",
    "oPolyFitReg = Pipeline([\n",
    "    ('PolyFeatures', PolynomialFeatures(include_bias = False)),\n",
    "    ('RidgeReg', Ridge(fit_intercept = True))\n",
    "])\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Regressor\n",
    "\n",
    "Decision Tree Regressor is a non parametric regression method since:\n",
    " - It does not make assumptions about the underlying distribution of the data.\n",
    " - The number of parameters is not fixed in advance, but rather grows and adapts with the complexity of the training data.\n",
    "\n",
    "A single Decision Tree creates a _piece wise constant_ approximation of the data.  \n",
    "This section builds an _Ensemble_ of such trees using Gradient Boosting as implemented in the [LightGBM](https://github.com/microsoft/LightGBM) library.  \n",
    "\n",
    "* <font color='brown'>(**#**)</font> Both [LightGBM](https://github.com/microsoft/LightGBM) (`LGBMRegressor` / `LGBMClassifier` / `LGBMRanker`) and [XGBoost](https://github.com/dmlc/xgboost) (`XGBRegressor` / `XGBClassifier` / `XGBRanker`) offer SciKit Learn compatible implementation of Ensemble (Graident Boosting) of Decision Trees.\n",
    "\n",
    "#### Categorical Features\n",
    "\n",
    "Decision Tree can handle _categorical_ features natively.   \n",
    "The way it is used is to break the categories into subsets by [_Optimal Partitioning_](https://xgboost.readthedocs.io/en/latest/tutorials/categorical.html#optimal-partitioning) (See [Walter D. Fisher - On Grouping for Maximum Homogeneity](https://www.semanticscholar.org/paper/040c3e7d4baac625b6072cf9bf6be697f26d3cab)).   \n",
    "Basically it identifies all splits and chooses the ones which minimized impurity (Classification) or error (Regression).  \n",
    "This method yield more efficient tree structure than having one Hot Encoding in addition to better performance (Run time and memory).\n",
    "\n",
    "* <font color='brown'>(**#**)</font> [StackExchange AI - Parametric vs. Non Parametric Models](https://ai.stackexchange.com/questions/23777).\n",
    "* <font color='brown'>(**#**)</font> [LightGBM](https://github.com/microsoft/LightGBM) fully supports _categorical_ features out of the box. Currently, [XGBoost](https://github.com/dmlc/xgboost) requires to [enable the feature explicitly](https://xgboost.readthedocs.io/en/latest/tutorials/categorical.html#training-with-scikit-learn-interface).\n",
    "* <font color='brown'>(**#**)</font> [StackExchange Cross Validated - How Decision Trees Split by Categorical Features](https://stats.stackexchange.com/questions/443780), [StackExchange Data Science - How Decision Trees Split by Categorical Features](https://datascience.stackexchange.com/questions/57256), [StackExchange Data Science - Why Decision Trees Split Require Categorical Features to Be Encoded](https://datascience.stackexchange.com/questions/52066), [StackExchange Data Science - Why Decision Trees Can Handle Categorical Features without Encoding](https://datascience.stackexchange.com/questions/18056)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Regression Model\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Build a Decision Tree Regression Model based on LightGBM's `LGBMRegressor`.\n",
    "# !! Set `random_state` to `seedNum` for reproducibility.\n",
    "# !! The depth and regularization will be set by Hyper Parameter tuning.\n",
    "# !! Treat `LGBMRegressor` as a regular SciKit Learn estimator with `fit()`, `predict()` and `score()` methods.\n",
    "oDecTreeReg = LGBMRegressor(random_state = seedNum)\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "This section trains the models and optimize the Hyper Parameter using Grid Search with Cross Validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize Hyper Parameters with Grid Search and Cross Validation\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Construct an object for Grid Search.\n",
    "# 2. Optimize the hyperparameters: Polynomial Degree, Regularization Factor.\n",
    "# !! Use `numFolds` for the number of folds in Cross Validation.\n",
    "# !! Start with small number of combination, then focus on the area of interest (Higher score).\n",
    "oGridSearchCv = GridSearchCV(\n",
    "    estimator = oPolyFitReg,\n",
    "    param_grid = {\n",
    "        'PolyFeatures__degree': [1, 2, 3, 4],\n",
    "        'RidgeReg__alpha': np.linspace(0, 5, 21).tolist()\n",
    "    },\n",
    "    cv = numFolds,\n",
    "    n_jobs = -1,\n",
    "    verbose = 1\n",
    ")\n",
    "oGridSearchCv = oGridSearchCv.fit(dfX, dsY)\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Optimal Model\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Extract the best model from the grid search object `oGridSearchCv`.\n",
    "oPolyFitReg = oGridSearchCv.best_estimator_\n",
    "#===============================================================#\n",
    "\n",
    "print(f'Optimal Hyper Parameters: {oGridSearchCv.best_params_}')\n",
    "print(f'Model Score: {oPolyFitReg.score(dfX, dsY):0.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Regression Model Fitting\n",
    "# Though an Ensemble is used, this section will ignore that for simplicity.\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Construct an object for Grid Search.\n",
    "# 2. Optimize the hyperparameters: Maximum Depth, Regularization of L1 (`reg_alpha`), Regularization of L2 (`reg_lambda`).\n",
    "# !! Use `numFolds` for the number of folds in Cross Validation.\n",
    "# !! Start with small number of combination, then focus on the area of interest (Higher score).\n",
    "oGridSearchCv = GridSearchCV(\n",
    "    estimator = oDecTreeReg,\n",
    "    param_grid = {\n",
    "        'max_depth': [2, 3],\n",
    "        'reg_alpha': np.linspace(0, 0.75, 11).tolist(),\n",
    "        'reg_lambda': np.linspace(4.25, 5, 11).tolist()\n",
    "    },\n",
    "    cv = numFolds,\n",
    "    n_jobs = -1,\n",
    "    verbose = 1\n",
    ")\n",
    "oGridSearchCv = oGridSearchCv.fit(dfX, dsY)\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Optimal Model\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Extract the best model from the grid search object `oGridSearchCv`.\n",
    "oDecTreeReg = oGridSearchCv.best_estimator_\n",
    "#===============================================================#\n",
    "\n",
    "print(f'Optimal Hyper Parameters: {oGridSearchCv.best_params_}')\n",
    "print(f'Model Score: {oDecTreeReg.score(dfX, dsY):0.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "\n",
    "In this section we'll analyze the results of the 2 models.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Model Score\n",
    "# The R2 score of the models (The default score for regressor on SciKit Learn)\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Calculate both models score using the R2 score.\n",
    "modelR2Score001 = oPolyFitReg.score(dfX, dsY)\n",
    "modelR2Score002 = oDecTreeReg.score(dfX, dsY)\n",
    "#===============================================================#\n",
    "\n",
    "print(f'The     Parametric model score (R2): {modelR2Score001:0.4f}.')\n",
    "print(f'The Non Parametric model score (R2): {modelR2Score002:0.4f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Frame to Show Results\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Build a Data Frame `dfResults` that contains the following columns:\n",
    "#    - 'Sex'.\n",
    "#    - 'Height'.\n",
    "#    - 'Weight'.\n",
    "#    - 'Prediction' - The prediction of a model.\n",
    "#    - 'Model' - 1 for the Parametric Model, 2 for the Non Parametric Model.\n",
    "# !! Each row of the original data frame `dfPeople` should appear twice in `dfResults` (One per model prediction).\n",
    "dfResults = dfPeople[['Sex', 'Height', 'Weight']].copy()\n",
    "dfResults = pd.concat((dfResults, dfResults), axis = 0, ignore_index = True)\n",
    "dfResults['Prediction'] = np.concatenate((oPolyFitReg.predict(dfX), oDecTreeReg.predict(dfX)), axis = 0)\n",
    "dfResults['Model'] = np.concatenate((np.ones(dfX.shape[0]), 2 * np.ones(dfX.shape[0])), axis = 0)\n",
    "#===============================================================#\n",
    "\n",
    "dfResults['Model'] = dfResults['Model'].map({1: 'Polynomial Regression', 2: 'Decision Tree Regression'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Regression Error Plot\n",
    "\n",
    "hF, hA = plt.subplots(figsize = (12, 8))\n",
    "\n",
    "sns.lineplot(data = dfResults, x = 'Weight', y = 'Weight', ax = hA, color = 'r')\n",
    "sns.scatterplot(data = dfResults, x = 'Weight', y = 'Prediction', hue = 'Sex', style = 'Model', ax = hA)\n",
    "hA.set_title('Models Predictions Error Plot')\n",
    "hA.set_xlabel('Weight Label')\n",
    "hA.set_ylabel('Weight Prediction');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> How come the improvement is not as big as one could expect?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIProgram",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
