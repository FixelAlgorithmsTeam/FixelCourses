{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Fixel Algorithms](https://fixelalgorithms.co/images/CCExt.png)](https://fixelalgorithms.gitlab.io)\n",
    "\n",
    "# AI Program\n",
    "\n",
    "## Deep Learning - Convolution Neural Network - MNIST Stroke (Classification)\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 1.0.000 | 30/08/2025 | Royi Avital | First version                                                      |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/AIProgram/2024_02/0002PointLine.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T17:52:07.921383Z",
     "start_time": "2022-02-02T17:52:07.649130Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Scientific Python\n",
    "\n",
    "# Image Processing & Computer Vision\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn            as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torch.optim.lr_scheduler import LRScheduler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchinfo\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "import torchvista\n",
    "\n",
    "# Python Library\n",
    "from enum import auto, Enum, unique\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "from platform import python_version\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Miscellaneous\n",
    "import onedrivedownloader\n",
    "\n",
    "# Typing \n",
    "from typing import Callable, Dict, List, Literal, Optional, Self, Tuple\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "# Visualization\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought.\n",
    "\n",
    "Code Notations:\n",
    "\n",
    "```python\n",
    "someVar    = 2; #<! Notation for a variable\n",
    "vVector    = np.random.rand(4) #<! Notation for 1D array\n",
    "mMatrix    = np.random.rand(4, 3) #<! Notation for 2D array\n",
    "tTensor    = np.random.rand(4, 3, 2, 3) #<! Notation for nD array (Tensor)\n",
    "tuTuple    = (1, 2, 3) #<! Notation for a tuple\n",
    "lList      = [1, 2, 3] #<! Notation for a list\n",
    "dDict      = {1: 3, 2: 2, 3: 1} #<! Notation for a dictionary\n",
    "oObj       = MyClass() #<! Notation for an object\n",
    "dfData     = pd.DataFrame() #<! Notation for a data frame\n",
    "dsData     = pd.Series() #<! Notation for a series\n",
    "hObj       = plt.Axes() #<! Notation for an object / handler / function handler\n",
    "```\n",
    "\n",
    "### Code Exercise\n",
    "\n",
    " - Single line fill\n",
    "\n",
    "```python\n",
    "valToFill = ???\n",
    "```\n",
    "\n",
    " - Multi Line to Fill (At least one)\n",
    "\n",
    " ```python\n",
    " # You need to start writing\n",
    " ?????\n",
    " ```\n",
    "\n",
    " - Section to Fill\n",
    "\n",
    "```python\n",
    "#===========================Fill This===========================#\n",
    "# 1. Explanation about what to do.\n",
    "# !! Remarks to follow / take under consideration.\n",
    "mX = ???\n",
    "\n",
    "?????\n",
    "#===============================================================#\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# %matplotlib inline\n",
    "\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "# Matplotlib default color palette\n",
    "lMatPltLibclr = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "# sns.set_theme() #>! Apply SeaBorn theme\n",
    "# sns.set_palette('tab10')\n",
    "\n",
    "runInGoogleColab = 'google.colab' in str(get_ipython())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "FIG_SIZE_DEF    = (8, 8)\n",
    "ELM_SIZE_DEF    = 50\n",
    "CLASS_COLOR     = ('b', 'r')\n",
    "EDGE_COLOR      = 'k'\n",
    "MARKER_SIZE_DEF = 10\n",
    "LINE_WIDTH_DEF  = 2\n",
    "\n",
    "TU_MNIST_IMG_SIZE = (28, 28)\n",
    "\n",
    "D_CLASSES_MNIST = {0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9'}\n",
    "L_CLASSES_MNIST = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "Ï€ = math.pi\n",
    "\n",
    "BASE_NAME   = 'FixelCourses'\n",
    "DATA_FOLDER = 'DataSets'\n",
    "\n",
    "BASE_PATH = os.getcwd()[:(len(os.getcwd()) - (os.getcwd()[::-1].lower().find(BASE_NAME.lower()[::-1])))]\n",
    "DATA_PATH = os.path.join(BASE_PATH, DATA_FOLDER)\n",
    "\n",
    "# See https://docs.python.org/3/library/enum.html\n",
    "@unique\n",
    "class NNMode(Enum):\n",
    "    TRAIN     = auto()\n",
    "    INFERENCE = auto() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Course Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary Functions\n",
    "\n",
    "def ParseMnistStrokeSample( sampleIdx: int, /, *, dataPath: str = '.', dataSet: Literal['Train', 'Test'] = 'Train' ) -> Tuple[List[NDArray], int]:\n",
    "\n",
    "    fileName = f'{dataSet}_{(sampleIdx):05d}.json' #<! Filenames are 0-59_999\n",
    "\n",
    "    with open(os.path.join(dataPath, fileName), 'r') as hFile:\n",
    "        dData = json.load(hFile)\n",
    "    \n",
    "    lS       = dData['strokes']\n",
    "    labelIdx = dData['label']\n",
    "\n",
    "    numStrokes = len(lS)\n",
    "    lXy        = []\n",
    "    for ii in range(numStrokes):\n",
    "        lSi = lS[ii]\n",
    "        numPts = len(lSi)\n",
    "        mXy = np.zeros(shape = (numPts, 2))\n",
    "        for jj in range(numPts):\n",
    "            mXy[jj] = lSi[jj]['x'], lSi[jj]['y']\n",
    "        \n",
    "        lXy.append(mXy)\n",
    "\n",
    "    return lXy, labelIdx\n",
    "\n",
    "def PlotStroke( lXy: List[NDArray], /, *, labelIdx: Optional[int] = None, hA: Optional[plt.Axes] = None, tFigSize: Tuple[float, float] = (6.4, 4.8) ) -> plt.Axes:\n",
    "\n",
    "    if hA is None:\n",
    "        hF, hA = plt.subplots(figsize = tFigSize)\n",
    "\n",
    "    hA.set_aspect('equal')\n",
    "\n",
    "    numStrokes = len(lXy)\n",
    "\n",
    "    for ii in range(numStrokes):\n",
    "        hA.scatter(lXy[ii][:, 0], lXy[ii][:, 1], label = f'Stroke: {(ii + 1):02d}')\n",
    "\n",
    "    tuYLim = hA.get_ylim()\n",
    "    if tuYLim[1] > tuYLim[0]:\n",
    "        hA.invert_yaxis()\n",
    "    \n",
    "    hA.set_xlim((0, 27))\n",
    "    hA.set_ylim((27, 0))\n",
    "\n",
    "    if labelIdx is not None:\n",
    "        hA.set_title(f'Label: {labelIdx}')\n",
    "    \n",
    "    return hA\n",
    "\n",
    "def TransformStroke( lXy: List[NDArray], numGridPts: int, /, *, interpCls: Optional[Callable] = sp.interpolate.make_smoothing_spline ) -> NDArray:\n",
    "    # There are more advanced approaches to Parametric Spline Curves.\n",
    "    # See Uniform parameterization (This implementation), Cord Length (Chordal) parametrization, Centripetal parametrization, etc...\n",
    "\n",
    "    mXY = np.concatenate(lXy, axis = 0) #<! Concatenate all strokes into a single \"stroke\"\n",
    "    \n",
    "    if interpCls is None:\n",
    "        # No interpolation\n",
    "        return mXY\n",
    "    \n",
    "    # Interpolation\n",
    "    vT  = np.linspace(0, 1, mXY.shape[0]) #<! Parametric curve\n",
    "    vTT = np.linspace(0, 1, numGridPts) #<! Parametric curve to be aligned to\n",
    "\n",
    "    oIntrp = interpCls(vT, mXY) #<! Interpolator\n",
    "\n",
    "    mXY = oIntrp(vTT)\n",
    "\n",
    "    return mXY\n",
    "\n",
    "def PlotLabelsHistogram( vY: NDArray, hA: Optional[plt.Axes] = None, lClass: Optional[List] = None, xLabelRot: Optional[int] = None ) -> plt.Axes:\n",
    "\n",
    "    if hA is None:\n",
    "        hF, hA = plt.subplots(figsize = (8, 6))\n",
    "    \n",
    "    vLabels, vCounts = np.unique(vY, return_counts = True)\n",
    "\n",
    "    hA.bar(vLabels, vCounts, width = 0.9, align = 'center')\n",
    "    hA.set_title('Histogram of Classes / Labels')\n",
    "    hA.set_xlabel('Class')\n",
    "    hA.set_xticks(vLabels, [f'{labelVal}' for labelVal in vLabels])\n",
    "    hA.set_ylabel('Count')\n",
    "    if lClass is not None:\n",
    "        hA.set_xticklabels(lClass)\n",
    "    \n",
    "    if xLabelRot is not None:\n",
    "        for xLabel in hA.get_xticklabels():\n",
    "            xLabel.set_rotation(xLabelRot)\n",
    "\n",
    "    return hA\n",
    "\n",
    "def PlotConfusionMatrix(vY: NDArray, vYPred: NDArray, normMethod: str = None, hA: Optional[plt.Axes] = None, \n",
    "                        lLabels: Optional[List] = None, dScore: Optional[Dict] = None, titleStr: str = 'Confusion Matrix', \n",
    "                        xLabelRot: Optional[int] = None, valFormat: Optional[str] = None) -> Tuple[plt.Axes, NDArray]:\n",
    "\n",
    "    # Calculation of Confusion Matrix\n",
    "    mConfMat = confusion_matrix(vY, vYPred, normalize = normMethod)\n",
    "    oConfMat = ConfusionMatrixDisplay(mConfMat, display_labels = lLabels)\n",
    "    oConfMat = oConfMat.plot(ax = hA, values_format = valFormat)\n",
    "    hA = oConfMat.ax_\n",
    "    if dScore is not None:\n",
    "        titleStr += ':'\n",
    "        for scoreName, scoreVal in  dScore.items():\n",
    "            titleStr += f' {scoreName} = {scoreVal:0.2},'\n",
    "        titleStr = titleStr[:-1]\n",
    "    hA.set_title(titleStr)\n",
    "    hA.grid(False)\n",
    "    if xLabelRot is not None:\n",
    "        for xLabel in hA.get_xticklabels():\n",
    "            xLabel.set_rotation(xLabelRot)\n",
    "\n",
    "    return hA, mConfMat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D Convolutional Neural Network (CNN)\n",
    "\n",
    "A CNN is an effective tool for supervised learning tasks, Regression / Classification, of 1D Signals.  \n",
    "This notebook explores a Fully Convolutional NN for a **Classification** task.\n",
    "\n",
    "### Signals\n",
    "\n",
    "Signals in the context of 1D Convolutional Neural Network (CNN) are _2D_ signals:\n",
    "1. The Samples Dimensions  \n",
    "   Usually describe the dimension which the signal is sampled along.  \n",
    "   <font color='magenta'>Example</font>: Time, 1D Position.\n",
    "2. The Channels Dimension  \n",
    "   Usually used to collate multiple signals on the same domain.  \n",
    "   <font color='magenta'>Example</font>: EEG Signal, Multi Sensor RF Array.\n",
    "\n",
    "</br>\n",
    "\n",
    "* <font color='brown'>(**#**)</font> In PyTorch 1D input to a net will have the shape: `(B, C, D)`.  \n",
    "  Where: `B` - Batch size, `C` - Number of channels, `D` - Number of elements in the feature vector.\n",
    "\n",
    "### 1D Convolution in CNN\n",
    "\n",
    "In Deep Learning context, 1D convolution is summing a 2D array yet the movement is along a single dimension (The last dimension).  \n",
    "The _1D Convolution Layer_ is geometrically defined by:\n",
    "\n",
    " - Number of Filters / Number of Output Channels  \n",
    "   Sets how many independent filters are defined by the layer.\n",
    " - Kernel Size  \n",
    "   Sets the length of the filter a long the last dimension of each filter.\n",
    "\n",
    "The kernel array per filter has a shape of `(C, K)` where `C` is the number of channels of the input and `K` is the length of the filter.  \n",
    "Each filter might also have a bias component which sets the number of parameters per filter to ${C}_{i} \\times K + 1$.  \n",
    "The total number of parameters of the layer is given by ${C}_{o} \\left( {C}_{i} \\times K + 1 \\right)$.  \n",
    "With ${C}_{i}$ as the number of channels in the input and ${C}_{o}$ the number o channels of the output.\n",
    "\n",
    "![](https://i.imgur.com/KLa7McT.png)\n",
    "<!-- ![](https://i.postimg.cc/fbW3NDZb/Diagrams-Convolution-1-D.png) -->\n",
    "\n",
    "Each output sample is a linear combination of the samples around an input element along all the channels.  \n",
    "Intuitively, one can think on each filter specializing on locating a local pattern.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The above neglects the batch dimension which in PyTorch is the first dimension.\n",
    "* <font color='brown'>(**#**)</font> Channels are common for 2D signals such as Images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch 1D Layers\n",
    "\n",
    "There are some common building blocks for 1D CNN's:\n",
    "\n",
    " - [`Conv1d`](https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv1d.html)   \n",
    "   Defines a 1D convolution layer.    \n",
    "   <font color='magenta'>Example</font>: `Conv1d(in_channels = 2, out_channels = 4, kernel_size = 3, stride = 1, padding = 'same', bias = True, padding_mode = 'replicate')`.\n",
    " - [`ReLU`](https://docs.pytorch.org/docs/stable/generated/torch.nn.ReLU.html)    \n",
    "   Defines the ReLU Activation Layer.  \n",
    "   <font color='magenta'>Example</font>: `ReLU(inplace = False)`.\n",
    " - [`MaxPool1d`](https://docs.pytorch.org/docs/stable/generated/torch.nn.MaxPool1d.html)  \n",
    "   Defines the Max Pool layer. Useful for non linear decimation of the signal.  \n",
    "   <font color='magenta'>Example</font>: `MaxPool1d(kernel_size = 2, stride = 2)`.\n",
    " - [`BatchNorm1d`](https://docs.pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html)  \n",
    "   Normalizes the input per channel using the statistics of the whole batch.  \n",
    "   During training it learns the statistics and normalizes. During inference applies the normalization.  \n",
    "   Since it learns the scaling and bias, layers before it may omit the bias term.  \n",
    "   <font color='magenta'>Example</font>: `BatchNorm1d(num_features = numChannelsIn)`.\n",
    "\n",
    "</br>\n",
    "\n",
    "* <font color='brown'>(**#**)</font> Similar to 2D CNN's, Residual blocks are highly effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# Data\n",
    "fileUrl     = 'https://technionmail-my.sharepoint.com/:u:/g/personal/royia_technion_ac_il/EUXCDJ40oItKofZ9E5tmSfMB_QZlZ3-N_-uc7WYGafQf8Q?e=rxEgx0' #<! OneDrive\n",
    "dataSetName = 'MNISTStroke'\n",
    "\n",
    "# Features\n",
    "numGridPts   = 32\n",
    "interpModel  = sp.interpolate.PchipInterpolator\n",
    "flatFeatures = False\n",
    "\n",
    "# Training\n",
    "batchSize   = 256\n",
    "numWork     = 0 #<! Number of workers\n",
    "nEpochs     = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate / Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The MNIST Stroke Dataset\n",
    "\n",
    "The MNIST Stroke Data Set (See [_MNIST Digits Stroke Sequence Data_](https://github.com/edwin-de-jong/mnist-digits-stroke-sequence-data)) is based on the MNIST data set.  \n",
    "It was generated by applying the following steps:\n",
    " - Binarizae the image by a threshold (Per class).\n",
    " - Apply [thinning](https://en.wikipedia.org/wiki/Thinning_(morphology)) (Morphological operation) on the binary image.\n",
    " - Extract the coordinates of each pixel into a sequence (Sequences).\n",
    "\n",
    "The output is a sequence of coordinates in 2D.  \n",
    "The sequence imitates pen strokes.\n",
    "\n",
    "![](https://i.imgur.com/e23EWlj.png)\n",
    "<!-- ![](https://i.postimg.cc/3r4Jv9Z9/Diagrams-MNIST-Stroke.png) -->\n",
    "\n",
    "The task of is **classification of the digits by the sequence of strokes** of each image.\n",
    "\n",
    "\n",
    "* <font color='red'>(**?**)</font> A sequence may contain more than a single stroke.  \n",
    "  The order of the strokes is determined by the [Travelling Salesman Problem](https://en.wikipedia.org/wiki/Travelling_salesman_problem) algorithm.\n",
    "* <font color='red'>(**?**)</font> The sequences may have a different lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate / Load Data \n",
    "\n",
    "mX, vY = fetch_openml('mnist_784', version = 1, return_X_y = True, as_frame = False, parser = 'auto')\n",
    "vY = vY.astype(np.int_) #<! The labels are strings, convert to integer\n",
    "\n",
    "print(f'The features data shape: {mX.shape}')\n",
    "print(f'The labels data shape: {vY.shape}')\n",
    "print(f'The unique values of the labels: {np.unique(vY)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T17:53:07.834772Z",
     "start_time": "2022-02-02T17:53:07.448832Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate / Load Data \n",
    "\n",
    "# Download the MNIST Stroke Data Set from the course OneDrive\n",
    "if (not os.path.isdir(os.path.join(DATA_PATH, dataSetName))):\n",
    "    dataSetPath = onedrivedownloader.download(fileUrl, filename = os.path.join(DATA_PATH, dataSetName + '.zip'), unzip = True, unzip_path = DATA_PATH)\n",
    "    dataSetPath = os.path.join(dataSetPath, dataSetName)\n",
    "else:\n",
    "    dataSetPath = os.path.join(DATA_PATH, dataSetName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse Single File\n",
    "\n",
    "sampleIdx = random.randrange(60_000) #<! Number of samples in the training set\n",
    "lXy, labelIdx = ParseMnistStrokeSample(sampleIdx, dataPath = dataSetPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Sample\n",
    "hF, vHa = plt.subplots(nrows = 1, ncols = 2, figsize = (12.8, 4.8))\n",
    "vHa = vHa.flat\n",
    "\n",
    "hA = vHa[0]\n",
    "hA.imshow(np.reshape(mX[sampleIdx], TU_MNIST_IMG_SIZE), cmap = 'gray', vmin = 0, vmax = 255)\n",
    "\n",
    "hA = vHa[1]\n",
    "hA = PlotStroke(lXy, hA = hA)\n",
    "hA.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Classification Task on a Sequence\n",
    "\n",
    "The model architecture, given an input tensor of $\\left( C, D \\right)$ outputs $K$ probability values, one per class:  \n",
    " - $C$ - The number of channels in the 1D input data (Sequence).\n",
    " - $D$ - The number of elements in the 1D input signal (Sequence).\n",
    " - $K$ - The number of classes in the dataset.\n",
    "\n",
    "![](https://i.imgur.com/AcW2pTN.png)\n",
    "<!-- ![](https://i.postimg.cc/FsZJgy4P/Diagrams-CNN-Classifier.png) -->\n",
    "\n",
    "The anatomy of the _Classification Model_:\n",
    " - The Input  \n",
    "   The input is (Usually) a tensor of the data which is 2D tensor (3D with the batch dimension).  \n",
    "   It it composed of the channels and the elements per channel.\n",
    " - The Feature Extractor / Transform  \n",
    "   **Learnable** _Feature Transform_ which aims to generate linear separation in dimension $F$.  \n",
    "   Composed of 1D (Fully) CNN. Its output is a vector in dimension $F$.\n",
    " - The Classifier  \n",
    "   A NN composed of Linear Layers. The last linear layer has no activation.  \n",
    "   Its output are called _Logits_. Applying [_SoftMax_](https://en.wikipedia.org/wiki/Softmax_function) on the _Logits_ transform them into probabilities.  \n",
    "   The composition of the NN Model + _softMax_ generates a generalized _Logistic Regression_ classifier.\n",
    "\n",
    "</br>\n",
    "\n",
    "* <font color='red'>(**?**)</font> Using a Fully Convolutional CNN in the extractor and Adaptive Pooling at its end allows building models which are tolerable for variable input size.  \n",
    "* <font color='red'>(**?**)</font> Even with size independent models, the scaling of the features in the data are important (Searching a ball in a Soccer game image).\n",
    "* <font color='red'>(**?**)</font> In most cases the _SoftMax_ layer is part of the loss in order to make the process more stable numerically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling the Variable Length of MNIST Stroke\n",
    "\n",
    "The model is feed with batches.  \n",
    "In order to be efficient, batches should be a tensor which forces all samples in the batch to have same dimensions.  \n",
    "In some cases the data is naturally represented by \n",
    "\n",
    "There are some techniques to align all signals into the same length.\n",
    "\n",
    "Idea:\n",
    " - Data Level:\n",
    "    - Padding  \n",
    "      Pad, usually with $0$, all signals to the same length (Of the longest signal).  \n",
    "      Run time wise it might be inefficient.\n",
    "    - Cropping  \n",
    "      Crop all signals to the length of the shortest.  \n",
    "      Might cause loss of information which is vital.\n",
    "    - Interpolation  \n",
    "      Interpolate all signals to the same length.  \n",
    "      Assumes the interpolation model fits the data.\n",
    "    - Clustering  \n",
    "      Specifically in this case one could use clustering to describe a sample by predefined number of centroids.\n",
    "    - Sampling  \n",
    "      Sampling data by the `Dataset` by the same length into batches.\n",
    " - Model\n",
    "    - Fully Convolutional Model + Adaptive Pooling Layer  \n",
    "      A model based on Fully CNN + Adaptive Pooling Layer can generate a vector with predefined number of elements regardless of the input size.  \n",
    "      Yet it requires adjusting the minimum length according to the decimation factor and thinking about scaling issues of the details in the input.  \n",
    "      See [`AdaptiveAvgPool1d`](https://docs.pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool1d.html) or [`AdaptiveMaxPool1d`](https://docs.pytorch.org/docs/stable/generated/torch.nn.AdaptiveMaxPool1d.html) as an examples of such adaptive layers.\n",
    "    - Native Model for _Variable Length_ Sequence  \n",
    "      Some architectures support variable length input by design.  \n",
    "      For example RNN and Transformer based models.\n",
    "\n",
    "</br>\n",
    "\n",
    "* <font color='red'>(**?**)</font> In this case specifically the order of the strokes / extraction of coordinates is also a significant factor.  \n",
    "  For instance, even if data is interpolated into 8 points, there are $8!$ ways to order each sample.   \n",
    "  In practice the ambiguity is smaller, still it is something the model needs to overcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolation\n",
    "\n",
    "lXXYY = [TransformStroke(lXy, numGridPts, interpCls = interpModel)]\n",
    "\n",
    "hF, vHa = plt.subplots(nrows = 1, ncols = 3, figsize = (19.2, 4.8))\n",
    "vHa = vHa.flat\n",
    "\n",
    "hA = vHa[0]\n",
    "hA.imshow(np.reshape(mX[sampleIdx], TU_MNIST_IMG_SIZE), cmap = 'gray', vmin = 0, vmax = 255)\n",
    "\n",
    "hA = vHa[1]\n",
    "hA = PlotStroke(lXy, hA = hA)\n",
    "hA.legend();\n",
    "\n",
    "hA = vHa[2]\n",
    "hA = PlotStroke(lXXYY, hA = hA)\n",
    "hA.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolator Effect\n",
    "\n",
    "tuInterpModel = (\n",
    "    ('Cubic Spline', sp.interpolate.CubicSpline),\n",
    "    ('Akima', sp.interpolate.Akima1DInterpolator),\n",
    "    ('PChip', sp.interpolate.PchipInterpolator),\n",
    "    ('BSpline', sp.interpolate.make_interp_spline),\n",
    "    ('Piece Wise Linear', lambda x, y: sp.interpolate.make_interp_spline(x, y, k = 1)),\n",
    "    ('Smooth Spline', sp.interpolate.make_smoothing_spline),\n",
    ")\n",
    "\n",
    "hF, vHa = plt.subplots(nrows = 1, ncols = 1 + len(tuInterpModel), figsize = (18, 4))\n",
    "vHa = vHa.flat\n",
    "\n",
    "hA = vHa[0]\n",
    "hA = PlotStroke(lXy, hA = hA)\n",
    "hA.set_title('Original Strokes')\n",
    "hA.legend();\n",
    "\n",
    "for ii, (interpModelName, oInterpModel) in enumerate(tuInterpModel):\n",
    "    hA = vHa[ii + 1]\n",
    "    lXXYY = [TransformStroke(lXy, numGridPts, interpCls = oInterpModel)]\n",
    "    PlotStroke(lXXYY, hA = hA)\n",
    "    hA.set_title(interpModelName);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Training Phase\n",
    "\n",
    "The recipe for the training phase is:\n",
    "\n",
    "1. Create the `Dataset` Class for the Dataset  \n",
    "   The data set supplies a sample and the matching label per sample index.  \n",
    "   It may retrieve the sample from memory or from other storage.  \n",
    "   It can apply a transformation on the sample (Data Augmentation / Feature Transform).\n",
    "2. Create the `DataLoader` for the Dataset  \n",
    "   A wrapper which accumulate samples into batches.  \n",
    "   Optimize the process so the DL accelerator si constantly fed with batches.\n",
    "3. Define the Model  \n",
    "   Define the model architecture.  \n",
    "   Once it is defined, validate it by running a sample through it.\n",
    "4. Define the Loss and Score  \n",
    "   Define loss and score functions to be used during training.\n",
    "5. Define the Optimizer and Scheduler (Optional)  \n",
    "   The optimizer defines the policy to update the weights given the loss.  \n",
    "   It runs the _Backpropagation_ phase and update the model weights.  \n",
    "   The _Scheduler_ (Optional) sets the _learning rate_ during the iterations / epochs.\n",
    "5. Implement the Training Loop  \n",
    "   The loop which goes through the epochs and optimizes the model.  \n",
    "   It gets batches from the `DataLoader`, process them through the model, evaluates the loss and runs the optimizer.  \n",
    "   It may calculate measures of progress such as the score or loss over time.  \n",
    "   If given, it updates the _Learning Rate_ using the scheduler.  \n",
    "   It may be adaptive by evaluating the score and loss on a validation dataset.\n",
    "6. Analyze the Training Phase  \n",
    "   Analyzes (Plots) the progress of Loss and Score during training.  \n",
    "   Used to understand the model achievements during training.  \n",
    "7. Present the Model Scoring  \n",
    "   Deeper analysis of the model performance on the available sets (Test Set if available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Data Loader\n",
    "class MNISTStrokeDataset(Dataset):\n",
    "    oDefInt = sp.interpolate.PchipInterpolator\n",
    "    def __init__( self, dataPath: str, dataSet: Literal['Test', 'Train'], /, *, numGridPts: int = 32, interpModel: Callable = oDefInt, flatFeatures: bool = False ) -> None:\n",
    "        TEST_FILE_NAME  = 'TEST.pkl'\n",
    "        TRAIN_FILE_NAME = 'TRAIN.pkl'\n",
    "\n",
    "        if dataSet not in ['Test', 'Train']:\n",
    "            raise ValueError(f'The value of `\"dataSet\"` = {dataSet} must be either `\"dataSet\"` or `\"Test\"`')\n",
    "        \n",
    "        match dataSet:\n",
    "            case 'Test':\n",
    "                dataFileName = TEST_FILE_NAME\n",
    "            case 'Train':\n",
    "                dataFileName = TRAIN_FILE_NAME\n",
    "        \n",
    "        dataFilePath = os.path.join(dataPath, dataFileName)\n",
    "        if os.path.isfile(dataFilePath):\n",
    "            # Load RAW data\n",
    "            with open(dataFilePath, 'rb') as hFile:\n",
    "                dData = pickle.load(hFile)\n",
    "                lS = dData['lStrokes']\n",
    "                lY = dData['lY']                    \n",
    "        else:\n",
    "            # Generate RAW data and save\n",
    "            lFiles = os.listdir(dataPath)\n",
    "            lFiles = [f for f in lFiles if dataSet in f]\n",
    "            lFiles.sort()\n",
    "\n",
    "            lS = [] #<! Strokes per Image\n",
    "            lY = [] #<! Labels\n",
    "\n",
    "            for ii in range(len(lFiles)):\n",
    "                lXy, labelIdx = ParseMnistStrokeSample(ii, dataPath = dataPath, dataSet = dataSet)\n",
    "                lS.append(lXy)\n",
    "                lY.append(labelIdx)\n",
    "            \n",
    "            dData = {'lStrokes': lS, 'lY': lY}\n",
    "            # Save RAW data\n",
    "            with open(dataFilePath, 'wb') as hFile:\n",
    "                pickle.dump(dData, hFile)\n",
    "        \n",
    "        lX = [] #<! Features\n",
    "        for ii in range(len(lS)):\n",
    "            lXy = lS[ii]\n",
    "            mXY = TransformStroke(lXy, numGridPts, interpCls = interpModel)\n",
    "            lX.append(mXY)\n",
    "        \n",
    "        self.dataPath     = dataPath\n",
    "        self.dataSet      = dataSet\n",
    "        self.numGridPts   = numGridPts\n",
    "        self.interpModel  = interpModel\n",
    "        self.flatFeatures = flatFeatures\n",
    "        \n",
    "        self.lS = lS\n",
    "        self.lX = lX\n",
    "        self.lY = lY\n",
    "        self.numSamples = len(lX)\n",
    "\n",
    "    def __len__( self: Self ) -> int:\n",
    "        \n",
    "        return self.numSamples\n",
    "\n",
    "    def __getitem__( self: Self, idx: int ) -> Tuple[NDArray, int]:\n",
    "        \n",
    "        mX   = self.lX[idx] #<! Features (numGridPts, 2)\n",
    "        valY = self.lY[idx] #<! Label\n",
    "\n",
    "        mX = mX.astype(np.float32) #<! PyTorch default float on GPU's\n",
    "\n",
    "        # Set the channels\n",
    "        # Signal should be (numChannels, numSamples)\n",
    "        if self.flatFeatures:\n",
    "            # Return a flat vector of features\n",
    "            mX = np.reshape(np.ravel(mX), (1, -1)) #<! Set channel to 1: (1, 2 * numGridPts)\n",
    "        else:\n",
    "            mX = np.transpose(mX, (1, 0)) #<! Set the channels (2, numGridPts)\n",
    "        \n",
    "        # PyTorch's Dataloader collates into tensors only NumPy's elements\n",
    "        valY = np.int64(valY)\n",
    "\n",
    "        return mX, valY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Sets\n",
    "# Define PyTorch Dataset\n",
    "dsTrain = MNISTStrokeDataset(dataSetPath, 'Train', numGridPts = numGridPts, interpModel = interpModel, flatFeatures = flatFeatures)\n",
    "dsTest  = MNISTStrokeDataset(dataSetPath, 'Test', numGridPts = numGridPts, interpModel = interpModel, flatFeatures = flatFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataSet as Iterator\n",
    "\n",
    "sampleIdx = random.randrange(len(dsTrain))\n",
    "mX, valY = dsTrain[sampleIdx]\n",
    "\n",
    "hF, hA = plt.subplots(nrows = 1, ncols = 1, figsize = (6.4, 4.8))\n",
    "\n",
    "hA = PlotStroke([mX.T], hA = hA)\n",
    "hA.set_title(f'Label: {valY}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loaders\n",
    "\n",
    "# Data is small, no real need for workers\n",
    "dlTrain = DataLoader(dsTrain, shuffle = True, batch_size = 1 * batchSize, num_workers = numWork, drop_last = True, persistent_workers = False)\n",
    "dlTest  = DataLoader(dsTest, shuffle = False, batch_size = 2 * batchSize, num_workers = numWork, persistent_workers = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate on the Loader\n",
    "# The first batch.\n",
    "tX, vY = next(iter(dlTrain)) #<! PyTorch Tensors\n",
    "\n",
    "print(f'The batch features dimensions: {tX.shape}')\n",
    "print(f'The batch labels dimensions: {vY.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "# Defining a sequential model.\n",
    "\n",
    "numFeatures = mX.shape[1]\n",
    "\n",
    "def GetModel( numChannels: int, numCls: int ) -> nn.Module:\n",
    "    # Assumes the input size is >= 32 samples.\n",
    "    # Assumes the output shape (`numCls`) smaller than 256.\n",
    "    oModel = nn.Sequential(\n",
    "        nn.Identity(),\n",
    "        \n",
    "        nn.Conv1d(in_channels = numChannels,   out_channels = 32,  kernel_size = 3), nn.MaxPool1d(kernel_size = 2), nn.ReLU(),\n",
    "        nn.Conv1d(in_channels = 32,            out_channels = 64,  kernel_size = 3),                                nn.ReLU(),\n",
    "        nn.Conv1d(in_channels = 64,            out_channels = 128, kernel_size = 3), nn.MaxPool1d(kernel_size = 2), nn.ReLU(),\n",
    "        nn.Conv1d(in_channels = 128,           out_channels = 256, kernel_size = 3),                                nn.ReLU(),\n",
    "                \n",
    "        nn.AdaptiveAvgPool1d(output_size = 1), #<! A trick to support arbitrary input size before the \"Linear Layer\" section\n",
    "        nn.Flatten          (),\n",
    "        nn.Linear           (in_features = 256,        out_features = 2 * numCls),\n",
    "        nn.Linear           (in_features = 2 * numCls, out_features = numCls),\n",
    "    )\n",
    "    \n",
    "    return oModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> The _SoftMax_ layer is better be part of the Cross Entropy Loss function. See [PyTorch's `CrossEntropyLoss`](https://docs.pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Summary\n",
    "\n",
    "numChannels = 1 if flatFeatures else 2\n",
    "numCls      = len(L_CLASSES_MNIST)\n",
    "\n",
    "oModel = GetModel(numChannels, numCls)\n",
    "torchinfo.summary(oModel, tX.shape, col_names = ['kernel_size', 'input_size', 'output_size', 'num_params'], device = 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchvista.trace_model(oModel, torch.randn(tX.shape), height = 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Model\n",
    "# Apply a test run.\n",
    "\n",
    "with torch.inference_mode():\n",
    "    vYHat = oModel(tX)\n",
    "\n",
    "print(f'The input dimensions : {tX.shape}')\n",
    "print(f'The output dimensions: {vYHat.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU Availability\n",
    "\n",
    "runDevice   = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') #<! The 1st CUDA device\n",
    "oModel      = oModel.to(runDevice) #<! Transfer model to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Score Function\n",
    "\n",
    "hL = nn.CrossEntropyLoss()\n",
    "# Macro: Calculate per class, average over classes\n",
    "# Micro: Calculate over all data samples\n",
    "hS = MulticlassAccuracy(num_classes = numCls, average = 'micro')\n",
    "hL = hL.to(runDevice) #<! Not required!\n",
    "hS = hS.to(runDevice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunEpoch( oModel: nn.Module, dlData: DataLoader, hL: Callable, hS: Callable, oOpt: Optional[Optimizer] = None, opMode: NNMode = NNMode.TRAIN ) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Runs a single Epoch (Train / Test) of a model.  \n",
    "    Input:\n",
    "        oModel      - PyTorch `nn.Module` object.\n",
    "        dlData      - PyTorch `Dataloader` object.\n",
    "        hL          - Callable for the Loss function.\n",
    "        hS          - Callable for the Score function.\n",
    "        oOpt        - PyTorch `Optimizer` object.\n",
    "        opMode      - An `NNMode` to set the mode of operation.\n",
    "    Output:\n",
    "        valLoss     - Scalar of the loss.\n",
    "        valScore    - Scalar of the score.\n",
    "    Remarks:\n",
    "      - The `oDataSet` object returns a Tuple of (mX, vY) per batch.\n",
    "      - The `hL` function should accept the `vY` (Reference target) and `mZ` (Output of the NN).  \n",
    "        It should return a Tuple of `valLoss` (Scalar of the loss) and `mDz` (Gradient by the loss).\n",
    "      - The `hS` function should accept the `vY` (Reference target) and `mZ` (Output of the NN).  \n",
    "        It should return a scalar `valScore` of the score.\n",
    "      - The optimizer is required for training mode.\n",
    "    \"\"\"\n",
    "    \n",
    "    epochLoss  = 0.0\n",
    "    epochScore = 0.0\n",
    "    numSamples = 0\n",
    "    numBatches = len(dlData)\n",
    "\n",
    "    runDevice = next(oModel.parameters()).device #<! CPU \\ GPU\n",
    "\n",
    "    if opMode == NNMode.TRAIN:\n",
    "        oModel.train(True) #<! Equivalent of `oModel.train()`\n",
    "        trainMode = True\n",
    "    elif opMode == NNMode.INFERENCE:\n",
    "        oModel.eval() #<! Equivalent of `oModel.train(False)`\n",
    "        trainMode = False\n",
    "    else:\n",
    "        raise ValueError(f'The `opMode` value {opMode} is not supported!')\n",
    "    \n",
    "    for ii, (mX, vY) in enumerate(dlData):\n",
    "        # Move Data to Model's device\n",
    "        mX = mX.to(runDevice) #<! Lazy\n",
    "        vY = vY.to(runDevice) #<! Lazy\n",
    "\n",
    "        batchSize = mX.shape[0]\n",
    "        \n",
    "        if opMode == NNMode.TRAIN:\n",
    "            # Forward\n",
    "            mZ      = oModel(mX) #<! Model output\n",
    "            valLoss = hL(mZ, vY) #<! Loss\n",
    "            \n",
    "            # Backward\n",
    "            oOpt.zero_grad()   #<! Set gradients to zeros\n",
    "            valLoss.backward() #<! Backward\n",
    "            oOpt.step()        #<! Update parameters\n",
    "            oModel.eval()      #<! Inference mode for layers\n",
    "        else: #<! Value of `opMode` was already validated\n",
    "            with torch.inference_mode(): #<! The `torch.inference_mode()` scope is more optimized than `torch.no_grad()` \n",
    "                # No computational graph\n",
    "                mZ      = oModel(mX) #<! Model output\n",
    "                valLoss = hL(mZ, vY) #<! Loss\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            # Score\n",
    "            oModel.eval() #<! Ensure Evaluation Mode (Dropout / Normalization layers)\n",
    "            valScore = hS(mZ, vY)\n",
    "            # Normalize so each sample has the same weight\n",
    "            epochLoss  += batchSize * valLoss.item()\n",
    "            epochScore += batchSize * valScore.item()\n",
    "            numSamples += batchSize\n",
    "            oModel.train(trainMode) #<! Restore original mode\n",
    "\n",
    "        print(f'\\r{\"Train\" if trainMode else \"Val\"} - Iteration: {(ii + 1):3d} / {numBatches}, Loss: {valLoss:.6f}', end = '')\n",
    "    \n",
    "    print('', end = '\\r')\n",
    "            \n",
    "    return epochLoss / numSamples, epochScore / numSamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainModel( oModel: nn.Module, dlTrain: DataLoader, dlVal: DataLoader, oOpt: Optimizer, numEpoch: int, hL: Callable, hS: Callable, *, oSch: Optional[LRScheduler] = None ) -> Tuple[nn.Module, List, List, List, List]:\n",
    "    \"\"\"\n",
    "    Trains a model given test and validation data loaders.  \n",
    "    Input:\n",
    "        oModel      - PyTorch `nn.Module` object.\n",
    "        dlTrain     - PyTorch `Dataloader` object (Training).\n",
    "        dlVal       - PyTorch `Dataloader` object (Validation).\n",
    "        oOpt        - PyTorch `Optimizer` object.\n",
    "        numEpoch    - Number of epochs to run.\n",
    "        hL          - Callable for the Loss function.\n",
    "        hS          - Callable for the Score function.\n",
    "        oSch        - PyTorch `Scheduler` (`LRScheduler`) object.\n",
    "        oTBWriter   - PyTorch `SummaryWriter` object (TensorBoard).\n",
    "    Output:\n",
    "        lTrainLoss  - Scalar of the loss.\n",
    "        lTrainScore - Scalar of the score.\n",
    "        lValLoss    - Scalar of the score.\n",
    "        lValScore   - Scalar of the score.\n",
    "        lLearnRate  - Scalar of the score.\n",
    "    Remarks:\n",
    "      - The `oDataSet` object returns a Tuple of (mX, vY) per batch.\n",
    "      - The `hL` function should accept the `vY` (Reference target) and `mZ` (Output of the NN).  \n",
    "        It should return a Tuple of `valLoss` (Scalar of the loss) and `mDz` (Gradient by the loss).\n",
    "      - The `hS` function should accept the `vY` (Reference target) and `mZ` (Output of the NN).  \n",
    "        It should return a scalar `valScore` of the score.\n",
    "      - The optimizer is required for training mode.\n",
    "    \"\"\"\n",
    "\n",
    "    lTrainLoss  = []\n",
    "    lTrainScore = []\n",
    "    lValLoss    = []\n",
    "    lValScore   = []\n",
    "    lLearnRate  = []\n",
    "\n",
    "    # Support R2\n",
    "    bestScore = -1e9 #<! Assuming higher is better\n",
    "\n",
    "    learnRate = oOpt.param_groups[0]['lr']\n",
    "\n",
    "    for ii in range(numEpoch):\n",
    "        startTime           = time.time()\n",
    "        trainLoss, trainScr = RunEpoch(oModel, dlTrain, hL, hS, oOpt, opMode = NNMode.TRAIN) #<! Train\n",
    "        valLoss,   valScr   = RunEpoch(oModel, dlVal, hL, hS, None, opMode = NNMode.INFERENCE) #<! Score Validation\n",
    "        if oSch is not None:\n",
    "            # Adjusting the scheduler on Epoch level\n",
    "            learnRate = oSch.get_last_lr()[0]\n",
    "            oSch.step()\n",
    "        epochTime           = time.time() - startTime\n",
    "\n",
    "        # Aggregate Results\n",
    "        lTrainLoss.append(trainLoss)\n",
    "        lTrainScore.append(trainScr)\n",
    "        lValLoss.append(valLoss)\n",
    "        lValScore.append(valScr)\n",
    "        lLearnRate.append(learnRate)\n",
    "        \n",
    "        # Display (Babysitting)\n",
    "        print('Epoch '              f'{(ii + 1):4d} / ' f'{numEpoch}', end = '')\n",
    "        print(' | Train Loss: '     f'{trainLoss          :6.3f}', end = '')\n",
    "        print(' | Val Loss: '       f'{valLoss            :6.3f}', end = '')\n",
    "        print(' | Train Score: '    f'{trainScr           :6.3f}', end = '')\n",
    "        print(' | Val Score: '      f'{valScr             :6.3f}', end = '')\n",
    "        print(' | Epoch Time: '     f'{epochTime          :5.2f}', end = '')\n",
    "\n",
    "        # Save best model (\"Early Stopping\")\n",
    "        if valScr > bestScore:\n",
    "            bestScore = valScr\n",
    "            try:\n",
    "                dCheckPoint = {'Model': oModel.state_dict(), 'Optimizer': oOpt.state_dict()}\n",
    "                if oSch is not None:\n",
    "                    dCheckPoint['Scheduler'] = oSch.state_dict()\n",
    "                torch.save(dCheckPoint, 'BestModel.pt')\n",
    "                print(' | <-- Checkpoint!', end = '')\n",
    "            except:\n",
    "                print(' | <-- Failed!', end = '')\n",
    "        print(' |')\n",
    "    \n",
    "    # Load best model (\"Early Stopping\")\n",
    "    dCheckPoint = torch.load('BestModel.pt')\n",
    "    oModel.load_state_dict(dCheckPoint['Model'])\n",
    "\n",
    "    return oModel, lTrainLoss, lTrainScore, lValLoss, lValScore, lLearnRate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Optimizer\n",
    "\n",
    "oOpt = torch.optim.AdamW(oModel.parameters(), lr = 1e-3, betas = (0.9, 0.99), weight_decay = 1e-3) #<! Define optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Scheduler\n",
    "\n",
    "oSch = torch.optim.lr_scheduler.OneCycleLR(oOpt, max_lr = 5e-3, total_steps = nEpochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "\n",
    "oModel, lTrainLoss, lTrainScore, lValLoss, lValScore, lLearnRate = TrainModel(oModel, dlTrain, dlTest, oOpt, nEpochs, hL, hS, oSch = oSch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training Phase\n",
    "\n",
    "hF, vHa = plt.subplots(nrows = 1, ncols = 3, figsize = (12, 5))\n",
    "vHa = np.ravel(vHa)\n",
    "\n",
    "hA = vHa[0]\n",
    "hA.plot(lTrainLoss, lw = 2, label = 'Train')\n",
    "hA.plot(lValLoss, lw = 2, label = 'Validation')\n",
    "hA.set_title('Binary Cross Entropy Loss')\n",
    "hA.set_xlabel('Epoch')\n",
    "hA.set_ylabel('Loss')\n",
    "hA.legend()\n",
    "\n",
    "hA = vHa[1]\n",
    "hA.plot(lTrainScore, lw = 2, label = 'Train')\n",
    "hA.plot(lValScore, lw = 2, label = 'Validation')\n",
    "hA.set_title('Accuracy Score')\n",
    "hA.set_xlabel('Epoch')\n",
    "hA.set_ylabel('Score')\n",
    "hA.legend()\n",
    "\n",
    "hA = vHa[2]\n",
    "hA.plot(lLearnRate, lw = 2)\n",
    "hA.set_title('Learn Rate Scheduler')\n",
    "hA.set_xlabel('Epoch')\n",
    "hA.set_ylabel('Learn Rate');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Analysis (Scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis\n",
    "\n",
    "# Aggregate results for Train Set\n",
    "\n",
    "lYPred = []\n",
    "lY     = []\n",
    "\n",
    "for ii, (tX, vY) in enumerate(dlTrain):\n",
    "    # Move Data to Model's device\n",
    "    tX = tX.to(runDevice) #<! Lazy\n",
    "    vY = vY.to(runDevice) #<! Lazy\n",
    "        \n",
    "    with torch.inference_mode():\n",
    "        mZ = oModel(tX) #<! Model output\n",
    "        vYPred = torch.argmax(mZ, dim = 1)\n",
    "    \n",
    "    lYPred.append(vYPred.detach().cpu().numpy())\n",
    "    lY.append(vY.detach().cpu().numpy())\n",
    "\n",
    "vYPredTrain  = np.concat(lYPred, axis = 0)\n",
    "vYTruthTrain = np.concat(lY, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis\n",
    "\n",
    "# Aggregate results for Test Set\n",
    "\n",
    "lYPred = []\n",
    "lY     = []\n",
    "\n",
    "for ii, (tX, vY) in enumerate(dlTest):\n",
    "    # Move Data to Model's device\n",
    "    tX = tX.to(runDevice) #<! Lazy\n",
    "    vY = vY.to(runDevice) #<! Lazy\n",
    "        \n",
    "    with torch.inference_mode():\n",
    "        mZ = oModel(tX) #<! Model output\n",
    "        vYPred = torch.argmax(mZ, dim = 1)\n",
    "    \n",
    "    lYPred.append(vYPred.detach().cpu().numpy())\n",
    "    lY.append(vY.detach().cpu().numpy())\n",
    "\n",
    "vYPredTest  = np.concat(lYPred, axis = 0)\n",
    "vYTruthTest = np.concat(lY, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Class Scoring\n",
    "\n",
    "![](https://i.imgur.com/y2OyPYH.png)\n",
    "<!-- ![](https://i.postimg.cc/C1x60znh/Diagrams-Macro-Micro.png) -->\n",
    "\n",
    "- Macro Averaging: Computes the performance for each class, then average over classes.  \n",
    "  Sensitive to the statistics of the smaller classes.  \n",
    "  Appropriate when performance on all the classes is equally important.\n",
    "- Micro Averaging: Collect the decisions for all classes into a single confusion matrix, then compute scores from that table.  \n",
    "  Dominated by the more frequent class\n",
    "\n",
    "<!-- https://datascience.stackexchange.com/a/100198 -->\n",
    "\n",
    "* <font color='red'>(**?**)</font> In what case Macro and Micro averaging will be similar?\n",
    "\n",
    "<!-- Data is balanced -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis\n",
    "\n",
    "# Scoring Report - Micro Averaging Policy\n",
    "valAccuracy = accuracy_score(vYTruthTest, vYPredTest)\n",
    "valPrecision, valRecall, valF1, _ = precision_recall_fscore_support(vYTruthTest, vYPredTest, average = 'micro', labels = L_CLASSES_MNIST)\n",
    "\n",
    "# Print Report\n",
    "print(f'Scoring Report (Micro Averaging Policy)')\n",
    "print(f'Accuracy : {valAccuracy:.4f}')\n",
    "print(f'Precision: {valPrecision:.4f}')\n",
    "print(f'Recall   : {valRecall:.4f}')\n",
    "print(f'F1 Score : {valF1:.4f}')\n",
    "\n",
    "# Scoring Report - Macro Averaging Policy\n",
    "valPrecision, valRecall, valF1, _ = precision_recall_fscore_support(vYTruthTest, vYPredTest, average = 'macro', labels = L_CLASSES_MNIST)\n",
    "\n",
    "# Print Report\n",
    "print(f'Scoring Report (Macro Averaging Policy)')\n",
    "print(f'Precision: {valPrecision:.4f}')\n",
    "print(f'Recall   : {valRecall:.4f}')\n",
    "print(f'F1 Score : {valF1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> How come all scores are similar (Accuracy / Precision / Recall / F1)?\n",
    "\n",
    "<!-- Data is balanced -> High accuracy means all scores are high -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis\n",
    "\n",
    "# Scoring Report\n",
    "print('Classification Report')\n",
    "lClassLabel = [D_CLASSES_MNIST[k] for k in L_CLASSES_MNIST]\n",
    "print(classification_report(vYTruthTest, vYPredTest, labels = L_CLASSES_MNIST, target_names = lClassLabel)) #<! Parameter `target_names` requires strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis\n",
    "# Confusion Matrix\n",
    "\n",
    "hF, vHa = plt.subplots(nrows = 1, ncols = 2, figsize = (14, 6))\n",
    "\n",
    "hA, _ = PlotConfusionMatrix(vYTruthTrain, vYPredTrain, hA = vHa[0])\n",
    "hA.set_title(f'Train Data, Accuracy {np.mean(vYTruthTrain == vYPredTrain): 0.2%}')\n",
    "\n",
    "hA, _ = PlotConfusionMatrix(vYTruthTest, vYPredTest, hA = vHa[1])\n",
    "hA.set_title(f'Test Data, Accuracy {np.mean(vYTruthTest == vYPredTest): 0.2%}');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIProgram",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
