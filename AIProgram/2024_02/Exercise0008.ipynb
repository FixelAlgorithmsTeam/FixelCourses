{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Fixel Algorithms](https://i.imgur.com/AqKHVZ0.png)](https://fixelalgorithms.gitlab.io)\n",
    "\n",
    "# AI Program\n",
    "\n",
    "## Exercise 0008 - Deep Learning - Convolution NN for Image Classification\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 1.0.000 | 20/05/2024 | Royi Avital | First version                                                      |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/AIProgram/2024_02/Exercise0008.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T09:30:06.492269Z",
     "start_time": "2022-02-02T09:30:06.220934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn            as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchinfo\n",
    "from torchmetrics.classification import BinaryAccuracy\n",
    "import torchvision\n",
    "from torchvision.transforms import v2 as TorchVisionTrns\n",
    "\n",
    "# Miscellaneous\n",
    "import copy\n",
    "import gdown\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import urllib.request\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "# Typing\n",
    "from typing import Any, Callable, Dict, List, Optional, Self, Set, Tuple, Union\n",
    "\n",
    "# Visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought.\n",
    "\n",
    "Code Notations:\n",
    "\n",
    "```python\n",
    "someVar    = 2; #<! Notation for a variable\n",
    "vVector    = np.random.rand(4) #<! Notation for 1D array\n",
    "mMatrix    = np.random.rand(4, 3) #<! Notation for 2D array\n",
    "tTensor    = np.random.rand(4, 3, 2, 3) #<! Notation for nD array (Tensor)\n",
    "tuTuple    = (1, 2, 3) #<! Notation for a tuple\n",
    "lList      = [1, 2, 3] #<! Notation for a list\n",
    "dDict      = {1: 3, 2: 2, 3: 1} #<! Notation for a dictionary\n",
    "oObj       = MyClass() #<! Notation for an object\n",
    "dfData     = pd.DataFrame() #<! Notation for a data frame\n",
    "dsData     = pd.Series() #<! Notation for a series\n",
    "hObj       = plt.Axes() #<! Notation for an object / handler / function handler\n",
    "```\n",
    "\n",
    "### Code Exercise\n",
    "\n",
    " - Single line fill\n",
    "\n",
    " ```python\n",
    " vallToFill = ???\n",
    " ```\n",
    "\n",
    " - Multi Line to Fill (At least one)\n",
    "\n",
    " ```python\n",
    " # You need to start writing\n",
    " ????\n",
    " ```\n",
    "\n",
    " - Section to Fill\n",
    "\n",
    "```python\n",
    "#===========================Fill This===========================#\n",
    "# 1. Explanation about what to do.\n",
    "# !! Remarks to follow / take under consideration.\n",
    "mX = ???\n",
    "\n",
    "???\n",
    "#===============================================================#\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# %matplotlib inline\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "sns.set_theme() #>! Apply SeaBorn theme\n",
    "\n",
    "runInGoogleColab = 'google.colab' in str(get_ipython())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "DATA_FOLDER_PATH = 'Data'\n",
    "\n",
    "# Data Set Links: \n",
    "# - MicroSoft Kaggle Cats and Dogs Dataset - https://www.microsoft.com/en-us/download/details.aspx?id=54765 (Dog 11702, Cat 666 might be corrupted)\n",
    "# - Kaggle Competition - Dogs vs. Cats - https://www.kaggle.com/c/dogs-vs-cats\n",
    "DATA_SET_URL            = 'https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip'\n",
    "DATA_SET_FILE_NAME      = 'CatsDogs.zip'\n",
    "DATA_SET_FOLDER_NAME    = 'CatsDogs'\n",
    "\n",
    "D_CLASSES  = {0: 'Cat', 1: 'Dog'}\n",
    "L_CLASSES  = ['Cat', 'Dog']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Course Packages\n",
    "\n",
    "from DataManipulation import DownloadUrl\n",
    "from DeepLearningPyTorch import TrainModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Auxiliary Functions\n",
    "\n",
    "# Class to handle a folder with no labels: Test\n",
    "\n",
    "from torchvision.datasets.folder import IMG_EXTENSIONS, pil_loader\n",
    "\n",
    "class TestDataSet( torchvision.datasets.VisionDataset ):\n",
    "    def __init__(self, root: str = None, transforms: Callable[..., Any] | None = None, transform: Callable[..., Any] | None = None, target_transform: Callable[..., Any] | None = None) -> None:\n",
    "        super().__init__(root, transforms, transform, target_transform)\n",
    "\n",
    "\n",
    "        lF = os.listdir(root)\n",
    "        lFiles = [fileName for fileName in lF if (os.path.isfile(os.path.join(root, fileName)) and (os.path.splitext(os.path.join(root, fileName))[1] in IMG_EXTENSIONS))]\n",
    "\n",
    "        self.lFiles = lFiles\n",
    "        self.loader = pil_loader\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        \n",
    "        return len(self.lFiles)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Any:\n",
    "        \n",
    "        imgSample =  self.loader(os.path.join(self.root, self.lFiles[index]))\n",
    "        if self.transform is not None:\n",
    "            imgSample = self.transform(imgSample)\n",
    "        \n",
    "        return imgSample\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Cats vs. Dogs\n",
    "\n",
    "This exercises builds a model based on _Convolutional Neural Network_ for _Binary Image Classification_.  \n",
    "The data set is from the [Kaggle - Dogs vs. Cats](https://www.kaggle.com/c/dogs-vs-cats) competition.  \n",
    "The objective is to classify an image either as a _Cat_ or a _Dog_.  \n",
    "\n",
    "The challenge in this data set is working with images with different dimensions.\n",
    "\n",
    "The data contains 25,000 RGB images with different dimensions.  \n",
    "\n",
    "Tasks:\n",
    " - Download and arrange data properly.\n",
    " - Split data into 22,500 train samples and 2,500 validation samples.\n",
    " - Build a dataset and data loader.  \n",
    "   The data loader must support the case of different image dimensions.\n",
    " - Build a parameterized model (Layers, Activations, etc...).\n",
    " - Build an optimizer and a scheduler.\n",
    " - Build a training loop to optimize hyper parameters.\n",
    "\n",
    "Tips:\n",
    " - Use random transformation to enrich the data set.  \n",
    "   See [`torchvision.transforms.RandomRotation`](https://pytorch.org/vision/main/generated/torchvision.transforms.RandomRotation.html) as an examples.  \n",
    "   This is called [_Data Augmentation_](https://en.wikipedia.org/wiki/Data_augmentation).\n",
    " - Use [`torchvision.datasets.ImageFolder`](https://pytorch.org/vision/main/generated/torchvision.datasets.ImageFolder.html) to load the data easily.\n",
    " - Use [`torch.utils.data.random_split`](https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split) to split the data set.\n",
    " - You may handle the different image dimensions by:\n",
    "    - Build the model in a manner which is dimension insensitive.\n",
    "    - Transform the image into a pre defined size (Use padding to keep aspect ratio).\n",
    " - \n",
    "\n",
    "**Objective**: Above 96% accuracy on the validation set (See the [Competition Leader Board](https://www.kaggle.com/c/dogs-vs-cats/leaderboard)).\n",
    "\n",
    "* <font color='brown'>(**#**)</font> One may use single output with the Binary Cross Entropy Loss: [`BCELoss`](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html), [`BCEWithLogitsLoss`](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Different Images Size\n",
    "\n",
    "There are many options to handle different images size in the same batch / data.  \n",
    "There are 2 main approaches:\n",
    "\n",
    "1. Build a Model Insensitive to Image Input  \n",
    "   Build a model which assumes no knowledge of the image size.  \n",
    "   Usually it is built by Convolution Layers and Adaptive Pooling so the output is a function of the known number of channels and not the input.  \n",
    "   The challenge is to handle loading and processing the data.  \n",
    "   It usually done by padding all images to the size of the largest image.\n",
    "2. Adapt the Data  \n",
    "   Apply some combination of padding, resize and crop to have the same image size.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> For FCN models (1) one could also set the batch size to 1 so each image is on its own. Yet it hurts efficiency greatly.\n",
    "* <font color='brown'>(**#**)</font> See [PyTorch Forum - How to Create a `dataloader` with Variable Size Input](https://discuss.pytorch.org/t/8278).\n",
    "* <font color='brown'>(**#**)</font> See [StackOverflow - How to Train Network on Images of Different Sizes with PyTorch](https://stackoverflow.com/questions/72595995).\n",
    "* <font color='brown'>(**#**)</font> You may use the `TestDataSet` which is adapted to handle no labeled image folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Load the classification data set.\n",
    "\n",
    "1. Download the Data Set from [Kaggle Competition - Cat vs. Dog Dataset](https://www.kaggle.com/c/dogs-vs-cats).\n",
    "2. There are pre defined train and test (Not labeled) split.  \n",
    "3. Build a script to create a folder `Validation` and move to it part of the labeled data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
