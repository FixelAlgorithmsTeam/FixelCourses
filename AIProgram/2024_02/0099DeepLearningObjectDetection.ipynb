{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Fixel Algorithms](https://i.imgur.com/AqKHVZ0.png)](https://fixelalgorithms.gitlab.io/)\n",
    "\n",
    "# AI Program\n",
    "\n",
    "## Machine Learning - Deep Learning - Object Detection\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 1.0.000 | 16/06/2024 | Royi Avital | First version                                                      |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/AIProgram/2024_02/0099DeepLearningObjectDetection.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T09:30:06.492269Z",
     "start_time": "2022-02-02T09:30:06.220934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn            as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torch.optim.lr_scheduler import LRScheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchinfo\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "from torchmetrics.detection.iou import IntersectionOverUnion\n",
    "\n",
    "import torchvision\n",
    "from torchvision.transforms import v2 as TorchVisionTrns\n",
    "\n",
    "# Miscellaneous\n",
    "import copy\n",
    "from enum import auto, Enum, unique\n",
    "import math\n",
    "import os\n",
    "from platform import python_version\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "# Typing\n",
    "from typing import Callable, Dict, Generator, List, Optional, Self, Set, Tuple, Union\n",
    "\n",
    "# Visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython\n",
    "from IPython.display import HTML, Image\n",
    "from IPython.display import display\n",
    "from ipywidgets import Dropdown, FloatSlider, interact, IntSlider, Layout, SelectionSlider\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought.\n",
    "\n",
    "Code Notations:\n",
    "\n",
    "```python\n",
    "someVar    = 2; #<! Notation for a variable\n",
    "vVector    = np.random.rand(4) #<! Notation for 1D array\n",
    "mMatrix    = np.random.rand(4, 3) #<! Notation for 2D array\n",
    "tTensor    = np.random.rand(4, 3, 2, 3) #<! Notation for nD array (Tensor)\n",
    "tuTuple    = (1, 2, 3) #<! Notation for a tuple\n",
    "lList      = [1, 2, 3] #<! Notation for a list\n",
    "dDict      = {1: 3, 2: 2, 3: 1} #<! Notation for a dictionary\n",
    "oObj       = MyClass() #<! Notation for an object\n",
    "dfData     = pd.DataFrame() #<! Notation for a data frame\n",
    "dsData     = pd.Series() #<! Notation for a series\n",
    "hObj       = plt.Axes() #<! Notation for an object / handler / function handler\n",
    "```\n",
    "\n",
    "### Code Exercise\n",
    "\n",
    " - Single line fill\n",
    "\n",
    " ```python\n",
    " vallToFill = ???\n",
    " ```\n",
    "\n",
    " - Multi Line to Fill (At least one)\n",
    "\n",
    " ```python\n",
    " # You need to start writing\n",
    " ????\n",
    " ```\n",
    "\n",
    " - Section to Fill\n",
    "\n",
    "```python\n",
    "#===========================Fill This===========================#\n",
    "# 1. Explanation about what to do.\n",
    "# !! Remarks to follow / take under consideration.\n",
    "mX = ???\n",
    "\n",
    "???\n",
    "#===============================================================#\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# %matplotlib inline\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "# Matplotlib default color palette\n",
    "lMatPltLibclr = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "# sns.set_theme() #>! Apply SeaBorn theme\n",
    "\n",
    "runInGoogleColab = 'google.colab' in str(get_ipython())\n",
    "\n",
    "# Improve performance by benchmarking\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Reproducibility (Per PyTorch Version on the same device)\n",
    "# torch.manual_seed(seedNum)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark     = False #<! Makes things slower\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "FIG_SIZE_DEF    = (8, 8)\n",
    "ELM_SIZE_DEF    = 50\n",
    "CLASS_COLOR     = ('b', 'r')\n",
    "EDGE_COLOR      = 'k'\n",
    "MARKER_SIZE_DEF = 10\n",
    "LINE_WIDTH_DEF  = 2\n",
    "\n",
    "DATA_SET_FILE_NAME      = 'archive.zip'\n",
    "DATA_SET_FOLDER_NAME    = 'IntelImgCls'\n",
    "\n",
    "D_CLASSES  = {0: 'Red', 1: 'Green', 2: 'Blue'}\n",
    "L_CLASSES  = ['R', 'G', 'B']\n",
    "T_IMG_SIZE = (100, 100, 3)\n",
    "\n",
    "DATA_FOLDER_PATH    = 'Data'\n",
    "TENSOR_BOARD_BASE   = 'TB'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Auxiliary Modules for Google Colab\n",
    "if runInGoogleColab:\n",
    "    !wget https://raw.githubusercontent.com/FixelAlgorithmsTeam/FixelCourses/master/AIProgram/2024_02/DataManipulation.py\n",
    "    !wget https://raw.githubusercontent.com/FixelAlgorithmsTeam/FixelCourses/master/AIProgram/2024_02/DataVisualization.py\n",
    "    !wget https://raw.githubusercontent.com/FixelAlgorithmsTeam/FixelCourses/master/AIProgram/2024_02/DeepLearningPyTorch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courses Packages\n",
    "\n",
    "from DataManipulation import BBoxFormat\n",
    "from DataManipulation import GenLabeldEllipseImg\n",
    "from DataVisualization import PlotBox, PlotBBox, PlotLabelsHistogram\n",
    "from DeepLearningPyTorch import ObjectDetectionDataset, ToTensor, YoloGrid\n",
    "from DeepLearningPyTorch import GenDataLoaders, InitWeightsKaiNorm, TrainModel, TrainModelSch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='blue'>(**!**)</font> Go through `GenLabeldDataEllipse()`.\n",
    "* <font color='blue'>(**!**)</font> Go through `ObjectDetectionDataset`.\n",
    "* <font color='blue'>(**!**)</font> Go through `YoloGrid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Auxiliary Functions\n",
    "\n",
    "def GenData( numSamples: int, tuImgSize: Tuple[int, int, int], maxObj: int, boxFormat: BBoxFormat = BBoxFormat.YOLO ) -> Tuple[np.ndarray, List[np.ndarray], List[np.ndarray]]:\n",
    "\n",
    "    mX = np.empty(shape = (numSamples, *tuImgSize[::-1]))\n",
    "    lY = [None] * numSamples\n",
    "    lB = [None] * numSamples\n",
    "\n",
    "    for ii in range(numSamples):\n",
    "        numObj = np.random.randint(maxObj) + 1\n",
    "        mI, vLbl, mBB = GenLabeldEllipseImg(tuImgSize[:2], numObj, boxFormat = boxFormat)\n",
    "        mX[ii]  = np.transpose(mI, (2, 0, 1))\n",
    "        lY[ii]  = vLbl\n",
    "        lB[ii]  = mBB\n",
    "    \n",
    "    return mX, lY, lB\n",
    "        \n",
    "# Data Loader\n",
    "# Using a function to mitigate Multi Process issues:\n",
    "# https://pytorch.org/docs/stable/notes/windows.html#multiprocessing-error-without-if-clause-protection\n",
    "def DataLoaderBatch( dlData: DataLoader ) -> Tuple:\n",
    "    \n",
    "    return next(iter(dlData)) #<! PyTorch Tensors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Detection\n",
    "\n",
    "The _Object Detection_ task generalizes the _Object Localization_ task in 2 manners:\n",
    "\n",
    "1. Support for many objects at the same image.\n",
    "2. Detection as if there is any object at all.\n",
    "\n",
    "This notebook demonstrates:\n",
    " - Generating a synthetic data set.\n",
    " - Generating the _target_ data in the YOLO form.\n",
    " - Building a model for _Object Detection_.\n",
    " - Training a model with a composed objective.\n",
    "\n",
    "</br>\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The _Object Detection_ in this notebook is applies in _YOLO_ style: Single Pass, Grid and Anchors.\n",
    "* <font color='brown'>(**#**)</font> The motivation for a synthetic dataset is being able to implement the whole training process (Existing datasets are huge).  \n",
    "  Yet the ability to create synthetic dataset is a useful skill.\n",
    "* <font color='brown'>(**#**)</font> There are known datasets for object detection: [COCO Dataset](https://cocodataset.org), [PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/).   \n",
    "  They also define standards for the labeling system.  \n",
    "  Training them is on the scale of days.\n",
    "* <font color='brown'>(**#**)</font> [Object Detection Annotation Formats](https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation).\n",
    "* <font color='brown'>(**#**)</font> Review of Object Detection approaches is given by Lilian Weng: [Part 1: Gradient Vector, HOG, and SS](https://lilianweng.github.io/posts/2017-10-29-object-recognition-part-1), [Part 2: CNN, DPM and Overfeat](https://lilianweng.github.io/posts/2017-12-15-object-recognition-part-2), [Part 3: R-CNN Family](https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3), [Part 4: Fast Detection Models](https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# Data\n",
    "numSamplesTrain = 30_000\n",
    "numSamplesVal   = 10_000\n",
    "boxFormat       = BBoxFormat.YOLO\n",
    "numCls          = len(L_CLASSES) #<! Number of classes\n",
    "maxObj          = 3\n",
    "\n",
    "# Model\n",
    "gridSize = 5 #<! The gris is (gridSize x gridSize) \n",
    "\n",
    "# Training\n",
    "batchSize   = 256\n",
    "numWorkers  = 2 #<! Number of workers\n",
    "numEpochs   = 35\n",
    "λ = 20.0 #<! Localization Loss\n",
    "ϵ = 0.1 #<! Label Smoothing\n",
    "\n",
    "# Visualization\n",
    "numImg = 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate / Load Data\n",
    "\n",
    "The data is synthetic data.  \n",
    "Each image includes Ellipses where its color is the class (`R`, `G`, `B`) and the bounding rectangle.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The label, per object, is a vector of `5`: `[Class, xCenter, yCenter, boxWidth, boxHeight]`.  \n",
    "* <font color='brown'>(**#**)</font> The label is in `YOLO` format, hence it is normalized to `[0, 1]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Sample\n",
    "\n",
    "mI, vY, mBB = GenLabeldEllipseImg(T_IMG_SIZE[:2], maxObj, boxFormat = boxFormat)\n",
    "vYY = np.array([L_CLASSES[clsIdx] for clsIdx in vY])\n",
    "hA = PlotBox(mI, vYY, mBB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> One could use negative values for the bounding box. The model will extrapolate the object dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Data\n",
    "\n",
    "mXTrain, lYTrain, lBBTrain = GenData(numSamplesTrain, T_IMG_SIZE, maxObj, boxFormat = boxFormat)\n",
    "mXVal,   lYVal,   lBBVal   = GenData(numSamplesVal, T_IMG_SIZE, maxObj, boxFormat = boxFormat)\n",
    "\n",
    "print(f'The training data set data shape: {mXTrain.shape}')\n",
    "print(f'The training data set labels length: {len(lYTrain)}')\n",
    "print(f'The training data set box length: {len(lBBTrain)}')\n",
    "print(f'The validation data set data shape: {mXVal.shape}')\n",
    "print(f'The validation data set labels length: {len(lYVal)}')\n",
    "print(f'The validation data set box length: {len(lBBVal)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Why are lists used instead of arrays for the labels and the bounding boxes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Data\n",
    "\n",
    "dsTrain = ObjectDetectionDataset(mXTrain, lYTrain, lBBTrain)\n",
    "dsVal   = ObjectDetectionDataset(mXVal, lYVal, lBBVal)\n",
    "lClass  = list(np.r_[*dsTrain.lY])\n",
    "\n",
    "\n",
    "print(f'The training data set data shape: {dsTrain.tX.shape}')\n",
    "print(f'The test data set data shape: {dsVal.tX.shape}')\n",
    "print(f'The unique values of the labels: {np.unique(lClass)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> PyTorch with the `v2` transforms deals with bounding boxes using special type: `BoundingBoxes`.\n",
    "* <font color='brown'>(**#**)</font> For _data augmentation_ see:\n",
    "    - [Transforming and Augmenting Images](https://pytorch.org/vision/stable/transforms.html).\n",
    "    - [Getting Started with Transforms v2](https://pytorch.org/vision/stable/auto_examples/transforms/plot_transforms_getting_started.html).\n",
    "    - [Transforms v2: End to End Object Detection / Segmentation Example](https://pytorch.org/vision/stable/auto_examples/transforms/plot_transforms_e2e.html).\n",
    "    - [How to Write Your Own v2 Transforms](https://pytorch.org/vision/stable/auto_examples/transforms/plot_custom_transforms.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mX, mY = dsTrain[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Element of the Data Set\n",
    "\n",
    "mX, mY = dsTrain[2]\n",
    "\n",
    "vY = mY[:, 0].astype(np.int_)\n",
    "mB = mY[:, 1:]\n",
    "\n",
    "print(f'The features shape: {mX.shape}')\n",
    "print(f'The label shape: {vY.shape}')\n",
    "print(f'The bounding box shape: {mB.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> Since the labels are in the same contiguous container as the bounding box parameters, their type is `Float`.\n",
    "* <font color='brown'>(**#**)</font> The bounding box is using absolute values. In practice it is commonly normalized to the image dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Data\n",
    "\n",
    "vYY = np.array([L_CLASSES[clsIdx] for clsIdx in vY])\n",
    "hA = PlotBox(np.transpose(mX, (1, 2, 0)), vYY, mB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of Labels\n",
    "\n",
    "hA = PlotLabelsHistogram(np.r_[*dsTrain.lY], lClass = L_CLASSES)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Explain the amount of samples in the histogram per class and in total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO Style Transformer\n",
    "\n",
    "The target data should be altered into a YOLO grid style.  \n",
    "Namely the bounding box should be redefined relative to the grid cell the center of the box reside at.\n",
    "\n",
    "![](https://i.imgur.com/CE1Ef7g.png)\n",
    "\n",
    "This section implements the transformation as a PyTorch transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform into YOLO Grid\n",
    "# 1. Convert to a Torch tensor.\n",
    "# 2. Transform int YOLO Grid format.\n",
    "\n",
    "oTrns = TorchVisionTrns.Compose([\n",
    "    ToTensor(),\n",
    "    YoloGrid(gridSize),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine the Data Set\n",
    "\n",
    "dsTrain = ObjectDetectionDataset(mXTrain, lYTrain, lBBTrain, hDataTrans = oTrns)\n",
    "dsVal   = ObjectDetectionDataset(mXVal, lYVal, lBBVal, hDataTrans = oTrns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Data\n",
    "\n",
    "mX, mY = dsTrain[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vY = np.array(mY[:, -1]).astype(np.int_)\n",
    "vY.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vYY = np.array([L_CLASSES[clsIdx] for clsIdx in vY])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vY = np.array(mY[:, -1]).astype(np.int_)\n",
    "mB = mY[:, 1:-1]\n",
    "vYY = np.array([L_CLASSES[clsIdx] for clsIdx in vY])\n",
    "hA = PlotBox(np.transpose(mX, (1, 2, 0)), vYY, mB)\n",
    "hA.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loaders\n",
    "\n",
    "This section defines the data loaded.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader\n",
    "\n",
    "dlTrain = torch.utils.data.DataLoader(dsTrain, shuffle = True, batch_size = 1 * batchSize, num_workers = numWorkers, drop_last = True, persistent_workers = True)\n",
    "dlVal   = torch.utils.data.DataLoader(dsVal, shuffle = False, batch_size = 2 * batchSize, num_workers = numWorkers, persistent_workers = True)\n",
    "\n",
    "# dlTrain = torch.utils.data.DataLoader(dsTrain, shuffle = True, batch_size = 1 * batchSize, num_workers = 0, drop_last = True, persistent_workers = False)\n",
    "# dlVal   = torch.utils.data.DataLoader(dsVal, shuffle = False, batch_size = 2 * batchSize, num_workers = 0, persistent_workers = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate on the Loader\n",
    "# The first batch.\n",
    "\n",
    "tX, mY = DataLoaderBatch(dlTrain)\n",
    "\n",
    "print(f'The batch features dimensions: {tX.shape}')\n",
    "print(f'The batch labels dimensions: {mY[:, 0].shape}')\n",
    "print(f'The batch bounding box dimensions: {mY[:, 1:].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model\n",
    "\n",
    "This section defines the model.  \n",
    "\n",
    "![](https://i.imgur.com/8oqg3Du.png)  \n",
    "**Credit**: Optimized Convolutional Neural Network Architectures for Efficient On Device Vision based Object Detection.\n",
    "\n",
    "![](https://i.imgur.com/AGQaauN.png)  \n",
    "**Credit**: [Farid at @ai_fast_track](https://x.com/ai_fast_track/status/1453368771285032971)\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The following implementation has a model with a single output, both for the regression and the classification.\n",
    "* <font color='brown'>(**#**)</font> One could create 2 different outputs (_Heads_) for each task.\n",
    "* <font color='brown'>(**#**)</font> [Finally understand Anchor Boxes in Object Detection (2D and 3D)](https://www.thinkautonomous.ai/blog/anchor-boxes/) - How the prior is set by clustering the data.\n",
    "* <font color='brown'>(**#**)</font> Anchor Free Model: [Forget the Hassles of Anchor Boxes with FCOS: Fully Convolutional One Stage Object Detection](https://scribe.rip/fc0e25622e1c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "# Model generating function.\n",
    "\n",
    "def BuildModel( numCls: int ) -> nn.Module:\n",
    "\n",
    "    oModel = nn.Sequential(\n",
    "        nn.Identity(),\n",
    "        nn.Conv2d(3,   32,  3, stride = 2, padding = 0, bias = False), nn.BatchNorm2d(32 ), nn.ReLU(),\n",
    "        nn.Conv2d(32,  32,  3, stride = 1, padding = 1, bias = False), nn.BatchNorm2d(32 ), nn.ReLU(),\n",
    "        nn.Conv2d(32,  32,  3, stride = 2, padding = 0, bias = False), nn.BatchNorm2d(32 ), nn.ReLU(),\n",
    "        nn.Conv2d(32,  32,  3, stride = 1, padding = 1, bias = False), nn.BatchNorm2d(32 ), nn.ReLU(),\n",
    "        nn.Conv2d(32,  32,  3, stride = 1, padding = 1, bias = False), nn.BatchNorm2d(32 ), nn.ReLU(),\n",
    "        nn.Conv2d(32,  64,  3, stride = 2, padding = 1, bias = False), nn.BatchNorm2d(64 ), nn.ReLU(),\n",
    "        nn.Conv2d(64,  64,  3, stride = 1, padding = 1, bias = False), nn.BatchNorm2d(64 ), nn.ReLU(),\n",
    "        nn.Conv2d(64,  64,  3, stride = 1, padding = 1, bias = False), nn.BatchNorm2d(64 ), nn.ReLU(),\n",
    "        nn.Conv2d(64,  64,  3, stride = 1, padding = 1, bias = False), nn.BatchNorm2d(64 ), nn.ReLU(),\n",
    "        nn.Conv2d(64,  64,  3, stride = 1, padding = 1, bias = False), nn.BatchNorm2d(64 ), nn.ReLU(),\n",
    "        nn.Conv2d(64,  64,  3, stride = 2, padding = 1, bias = False), nn.BatchNorm2d(64 ), nn.ReLU(),\n",
    "        nn.Conv2d(64,  128, 3, stride = 1, padding = 0, bias = False), nn.BatchNorm2d(128), nn.ReLU(),\n",
    "        nn.Conv2d(128, 256, 3, stride = 1, padding = 0, bias = False), nn.BatchNorm2d(256), nn.ReLU(),\n",
    "        nn.Conv2d(256, 512, 2, stride = 1, padding = 0, bias = False), nn.BatchNorm2d(512), nn.ReLU(),\n",
    "        nn.Conv2d(512, numCls + 4, 1, stride = 1, padding = 0, bias = True),\n",
    "        nn.Flatten()\n",
    "    )\n",
    "\n",
    "    return oModel "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> What's the motivation for the depth of the model (Relatively deep)?\n",
    "* <font color='red'>(**?**)</font> Explain the actual operation of the last `Conv2D` layer. Can it be replaced with a `Linear` layer?\n",
    "* <font color='brown'>(**#**)</font> One could set the image to a power of 2. Then all convolution layers could have been with padding and stride of 2 until the size is `1x1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Model\n",
    "\n",
    "oModel = BuildModel(len(L_CLASSES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Information\n",
    "# Pay attention to the layers name.\n",
    "torchinfo.summary(oModel, (batchSize, *(T_IMG_SIZE[::-1])), col_names = ['kernel_size', 'output_size', 'num_params'], device = 'cpu', row_settings = ['depth', 'var_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Explain the dimensions of the last layer.\n",
    "* <font color='red'>(**?**)</font> Will the model work with smaller images?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "This section trains the model.  \n",
    "\n",
    "* <font color='brown'>(**#**)</font> The training loop must be adapted to the new loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Localization Loss\n",
    "\n",
    "The loss is a composite of 2 loss functions:\n",
    "\n",
    "$$\\ell\\left(\\hat{\\boldsymbol{y}},\\boldsymbol{y}\\right)=\\lambda_{\\text{MSE}}\\cdot\\ell_{\\text{MSE}}\\left(\\hat{\\boldsymbol{y}}_{\\text{bbox}},\\boldsymbol{y}_{\\text{bbox}}\\right)+\\lambda_{\\text{CE}}\\cdot\\ell_{\\text{CE}}\\left(\\hat{\\boldsymbol{y}}_{\\text{label}},\\boldsymbol{y}_{\\text{label}}\\right)$$\n",
    "\n",
    "Where $\\lambda_{\\text{MSE}}$ and $\\lambda_{\\text{CE}}$ are the weights of each loss.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> In practice a single $\\lambda$ is required.\n",
    "* <font color='brown'>(**#**)</font> The MSE is not optimal loss function. It will be replaced by the _Log Euclidean_ loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object Localization Loss\n",
    "class ObjLocLoss( nn.Module ):\n",
    "    def __init__( self, numCls: int, λ: float, ϵ: float = 0.0 ) -> None:\n",
    "        super(ObjLocLoss, self).__init__()\n",
    "\n",
    "        self.numCls     = numCls\n",
    "        self.λ          = λ\n",
    "        self.ϵ          = ϵ\n",
    "        self.oMseLoss   = nn.MSELoss()\n",
    "        self.oCeLoss    = nn.CrossEntropyLoss(label_smoothing = ϵ)\n",
    "    \n",
    "    def forward( self: Self, mYHat: torch.Tensor, mY: torch.Tensor ) -> torch.Tensor:\n",
    "\n",
    "        mseLoss = self.oMseLoss(mYHat[:, self.numCls:], mY[:, 1:])\n",
    "        ceLoss  = self.oCeLoss(mYHat[:, :self.numCls], mY[:, 0].to(torch.long))\n",
    "\n",
    "        lossVal = (self.λ * mseLoss) + ceLoss\n",
    "\t\t\n",
    "        return lossVal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Localization Score\n",
    "\n",
    "The score is defined by the _IoU_ of a valid classification:\n",
    "\n",
    "$$\\text{Score}=\\frac{1}{N}\\sum_{i=1}^{N}\\mathbb{I}\\left\\{ \\hat{y}_{i}=y_{i}\\right\\} \\cdot\\text{IoU}\\left(\\hat{B}_{i},B_{i}\\right)$$\n",
    "\n",
    "Where:\n",
    "- $\\hat{y}_{i}$ is the predicted label\n",
    "- $y_{i}$ is the correct label\n",
    "- $\\hat{B}_{i}$ is the predicted bounding box\n",
    "- $B_{i}$ is the correct bounding box\n",
    "In other words, the average IoU, considering only correct (label) prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> What are the bounds of the values of the score function?\n",
    "* <font color='red'>(**?**)</font> Is higher or lower value bette for the score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object Localization Score\n",
    "class ObjLocScore( nn.Module ):\n",
    "    def __init__( self, numCls: int ) -> None:\n",
    "        super(ObjLocScore, self).__init__()\n",
    "\n",
    "        self.numCls = numCls\n",
    "    \n",
    "    def forward( self: Self, mYHat: torch.Tensor, mY: torch.Tensor ) -> Tuple[float, float, float]:\n",
    "\n",
    "        batchSize = mYHat.shape[0]\n",
    "        \n",
    "        vY, mBox = mY[:, 0].to(torch.long), mY[:, 1:]\n",
    "\n",
    "        vIoU = torch.diag(torchvision.ops.box_iou(torchvision.ops.box_convert(mYHat[:, self.numCls:], 'cxcywh', 'xyxy'), torchvision.ops.box_convert(mBox, 'cxcywh', 'xyxy')))\n",
    "        vCor = (vY == torch.argmax(mYHat[:, :self.numCls], dim = 1)).to(torch.float32) #<! Correct labels\n",
    "\n",
    "        # valIoU      = torch.mean(vIoU).item()\n",
    "        # valAcc      = torch.mean(vCor).item()\n",
    "        valScore    = torch.inner(vIoU, vCor) / batchSize\n",
    "\t\t\n",
    "        return valScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Device\n",
    "\n",
    "runDevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') #<! The 1st CUDA device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Score Function\n",
    "\n",
    "hL = ObjLocLoss(numCls = numCls, λ = λ, ϵ = ϵ)\n",
    "hS = ObjLocScore(numCls = numCls)\n",
    "\n",
    "hL = hL.to(runDevice)\n",
    "hS = hS.to(runDevice)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "\n",
    "oModel = oModel.to(runDevice)\n",
    "oOpt = torch.optim.AdamW(oModel.parameters(), lr = 1e-5, betas = (0.9, 0.99), weight_decay = 1e-5) #<! Define optimizer\n",
    "oSch = torch.optim.lr_scheduler.OneCycleLR(oOpt, max_lr = 5e-4, total_steps = numEpochs)\n",
    "_, lTrainLoss, lTrainScore, lValLoss, lValScore, lLearnRate = TrainModel(oModel, dlTrain, dlVal, oOpt, numEpochs, hL, hS, oSch = oSch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training Phase\n",
    "\n",
    "hF, vHa = plt.subplots(nrows = 1, ncols = 3, figsize = (12, 5))\n",
    "vHa = np.ravel(vHa)\n",
    "\n",
    "hA = vHa[0]\n",
    "hA.plot(lTrainLoss, lw = 2, label = 'Train')\n",
    "hA.plot(lValLoss, lw = 2, label = 'Validation')\n",
    "hA.set_title(f'Object Localization Loss (λ = {λ:0.1f})')\n",
    "hA.set_xlabel('Epoch')\n",
    "hA.set_ylabel('Loss')\n",
    "hA.legend()\n",
    "\n",
    "hA = vHa[1]\n",
    "hA.plot(lTrainScore, lw = 2, label = 'Train')\n",
    "hA.plot(lValScore, lw = 2, label = 'Validation')\n",
    "hA.set_title('Object Localization Score')\n",
    "hA.set_xlabel('Epoch')\n",
    "hA.set_ylabel('Score')\n",
    "hA.legend()\n",
    "\n",
    "hA = vHa[2]\n",
    "hA.plot(lLearnRate, lw = 2)\n",
    "hA.set_title('Learn Rate Scheduler')\n",
    "hA.set_xlabel('Epoch')\n",
    "hA.set_ylabel('Learn Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Prediction\n",
    "# TODO: Check classification\n",
    "\n",
    "rndIdx = np.random.randint(numSamplesVal)\n",
    "\n",
    "mX, vY = dsVal[rndIdx]\n",
    "valY    = int(vY[0])\n",
    "vB      = vY[1:]\n",
    "with torch.no_grad():\n",
    "    tX = torch.tensor(mX)\n",
    "    tX = torch.unsqueeze(tX, 0)\n",
    "    tX = tX.to(runDevice)\n",
    "    mYHat = oModel(tX).detach().cpu().numpy()\n",
    "\n",
    "vYHat       = mYHat[0]\n",
    "valYHat     = np.argmax(vYHat[:numCls])\n",
    "vBHat       = vYHat[numCls:]\n",
    "\n",
    "hA = PlotBox(np.transpose(mX, (1, 2, 0)), L_CLASSES[valY], vB)\n",
    "hA = PlotBBox(hA, L_CLASSES[valYHat], vBHat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> What would be the results if teh generated data had more small ellipses?\n",
    "* <font color='green'>(**@**)</font> Display the _accuracy_ and _IoU_ scores and _MSE_ and _CE_ loss over the epochs.   \n",
    "  It will require updating the Loss, Score classes and the training function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
