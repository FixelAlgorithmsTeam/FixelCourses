---
title: "Image Segmentation with U-Net"
subtitle: "Workshop"
author: "Royi Avital"
editor:
  render-on-save: false
execute:
  freeze: true
format: 
  revealjs:
    theme: [dark, Custom.scss]
    width: 1200
    height: 800
    footer: "[Technion - The School of Continuing Education](https://cont-edu.technion.ac.il)"
    logo: "https://i.imgur.com/8Eg8S2J.png"
    toc: true
    toc-title: Agenda
    toc-depth: 2
    margin: 0.05
    navigation-mode: vertical
    chalkboard: true
    html-math-method: mathjax
    include-in-header:
      - text: |
          <script>
          MathJax = {
            loader: {load: ['[tex]/color', '[tex]/html']},
            tex: {packages: {'[+]': ['autoload', 'color', html]}},
            options: {
              menuOptions: {
                settings: {
                  assistiveMml: false
                }
              }
            }
          };
          </script>

revealjs-plugins:
  - attribution
filters:
  - roughnotation
---


# Image Segmentation

![](https://i.imgur.com/3dMBM5o.png){#tight-fig fig-align="left" height="350px"}

{{< include MathJaXMacros.qmd >}}

<!-- It seems colors must be defined locally -->
$$
\definecolor{BoldRed}{rgb}{0.9, 0.2, 0.2}
$$

## Introduction

:::{.incremental}
 * Consider the _Image Segmentation_ task:
    * {{< iconify oui ml-classification-job >}} Classification (**What** is the object).
    * {{< iconify pixelarticons pixelarticons >}} Pixel Level (**Where** is the object).
:::

. . .

![](https://i.imgur.com/eBwcy4L.png){#tight-fig fig-align="center"}

## Image Segmentation Types {auto-animate="true"}

| Type  | Example  |
|--------|--------|
| Semantic  | ![](https://i.imgur.com/cnerHbN.png){#tight-fig fig-align="center" height="100px"}   |
| Instance   | ![](https://i.imgur.com/qbAoMk3.png){#tight-fig fig-align="center" height="100px"}   |
| Panoptic | ![](https://i.imgur.com/J99Rzpu.png){#tight-fig fig-align="center" height="100px"}   |

: Segmentation Types {.striped .success}

## Image Segmentation Types {.unlisted auto-animate="true"}

| Type  | Example  |
|--------|--------|
| Semantic  | Pixel $\to$ Class   |
| Instance   | Pixel $\to$ Object ID   |
| Panoptic | Pixel $\to$ Class + Object ID  |

: Segmentation Types {.striped .success}

. . . 

:::{.callout-important title="Interpretation"}

 - [Semantic]{style="color: magenta;"}: Pixel (**Each**) is labeled by its texture and other image related properties.
 - Instance: Pixel is labeled as part of a predefined set of objects. Each object is uniquely identified (Can be counted).
 - Panoptic: Pixel (**Each**) is labeled by its texture and object.

:::

## Image Segmentation Types {.unlisted auto-animate="true"}

| Type  | Example  |
|--------|--------|
| Semantic  | Pixel $\to$ Class   |
| Instance   | Pixel $\to$ Object ID   |
| Panoptic | Pixel $\to$ Class + Object ID  |

: Segmentation Types {.striped .success}

. . . 

:::{.callout-tip icon=false}

#### [{{< iconify fluent chat-bubbles-question-24-regular >}}]{style="color: #02b875;"} Question

How can one create an _Object Detector_ from _Image Segmentor_?

:::

## Semantic Segmentation {auto-animate="true"}

:::{.incremental}
 * {{< iconify pixelarticons pixelarticons >}} The model classifies **each** pixel in the image:
:::

. . .

![](https://i.imgur.com/LHBwINh.png){#tight-fig fig-align="center" height="450px"}

## Semantic Segmentation {.unlisted auto-animate="true"}

![](https://i.imgur.com/Rjpv18s.png){#tight-fig fig-align="center"}

## The Score {auto-animate="true"}

:::{.incremental}

 * Imbalanced (Background) Classification Scores:
    - Balanced Accuracy.
    - Recall, Precision.
    - Dice / F1.
    - Confusion Matrix.
 * Object Scores
    - IoU.
    - mAP.

:::

## The Score {.unlisted auto-animate="true"}

:::{style="font-size: 50%;"}

 * Imbalanced (Background) Classification Scores:
    - Balanced Accuracy.
    - Recall, Precision.
    - Dice / F1.
    - Confusion Matrix.
 * Object Scores
    - IoU.
    - mAP.

:::

. . .

:::{.callout-tip title="Resources"}

 - [Understanding Evaluation Metrics in Medical Image Segmentation](https://scribe.rip/d289a373a3f).
 - [Evaluating Image Segmentation Models](https://www.jeremyjordan.me/evaluating-image-segmentation-models).
 - [Image Segmentation — Choosing the Correct Metric](https://scribe.rip/aa21fd5751af).
 - [`miseval`: A Metric Library for Medical Image Segmentation EVALuation](https://github.com/frankkramer-lab/miseval).
 - Kaggle: [All the Segmentation Metrics](https://www.kaggle.com/code/yassinealouini/all-the-segmentation-metrics), [Understanding Dice Coefficient](https://www.kaggle.com/code/yerramvarun/understanding-dice-coefficient), [Visual Guide To Understanding Segmentation Metrics](https://www.kaggle.com/code/dschettler8845/visual-guide-to-understanding-segmentation-metrics).
:::


## The Loss Function {.unlisted}

:::{.incremental}
 * Cross Entropy Loss.
 * Cross Entropy Loss + Label Smoothing.
 * Balanced Cross Entropy / Focal Loss.
 * Gradient Friendly Region / Boundary Loss.

:::

. . .

:::{.callout-tip title="Resources"}

 - [Loss Functions for Image Segmentation](https://github.com/JunMa11/SegLossOdyssey).
 - [3 Common Loss Functions for Image Segmentation](https://dev.to/_aadidev/3-common-loss-functions-for-image-segmentation-545o).
 - [Instance segmentation loss functions](https://softwaremill.com/instance-segmentation-loss-functions).
 - [Focal Loss: An Efficient Way of Handling Class Imbalance](https://scribe.rip/4855ae1db4cb).
:::


# Data

## The Data Set

:::{.incremental style="font-size: 80%;"} 
 - The _RAW_ data set should be built with
    - Images: Real World / Synthetic / Specific / General.
    - Labels: Masks (Binary), Integer Maps (Multi Class).
 - There are several known datasets:
    - [COCO Dataset](https://cocodataset.org) - Common Objects in Context.  
    Originally by Microsoft. Labeled for many tasks (Segmentation, KeyPoints, etc...).
    - [PASCAL VOC2012 Segmentation](http://host.robots.ox.ac.uk/pascal/VOC).
    - [BDD100K: A Large Scale Diverse Driving Video Database](https://bair.berkeley.edu/blog/2018/05/30/bdd).
    - [Motion Based Segmentation and Recognition Dataset](https://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid).
    - [The Cityscapes Dataset](https://www.cityscapes-dataset.com/).
    - [Mapillary Vistas Dataset](https://www.mapillary.com/dataset/vistas).
    - [ApolloScape Scene Parsing](https://apolloscape.auto/scene.html).
 - The datasets are usually used for pre training of models and evaluation of architectures.
:::


# Models

## Image Segmentation Models {.unlisted auto-animate="true"}

:::{.incremental} 
 - The first models were based on large image classification models with different haeds.
 - Later models used the concept of _Fully Convolutional Net_ (FCN).
 - The next evolution step was Encoder / Decoder architectures.
 - Modern models are based on U-Net like shape.
:::

## Image Segmentation Models {.unlisted auto-animate="true"}

:::{style="font-size: 40%;"} 
 - The first models were based on large image classification models with different haeds.
 - Later models used the concept of _Fully Convolutional Net_ (FCN).
 - The next evolution step was Encoder / Decoder architectures.
 - Modern models are based on U-Net like shape.
:::

. . .

![](https://i.imgur.com/gQSl4Jm.png){#tight-fig fig-align="center" height="550px"}

## Image Segmentation Models {.unlisted auto-animate="true"}

:::{style="font-size: 100%;"} 
 - The first models were based on large image classification models with different haeds.
 - Later models used the concept of _Fully Convolutional Net_ (FCN).
 - The next evolution step was Encoder / Decoder architectures.
 - Modern models are based on U-Net like shape.
:::

. . .

:::{.callout-tip title="Resources"}

 - [Comparative Study of Image Segmentation Architectures Using Deep Learning](https://scribe.rip/3743875fd608).
 - [Image Segmentation: Architectures, Losses, Datasets and Frameworks](https://neptune.ai/blog/image-segmentation).
 - [Complete Guide to Semantic Segmentation](https://www.superannotate.com/blog/guide-to-semantic-segmentation).
:::

## UpSample Layer {.unlisted auto-animate="true"}

## U-Net {.unlisted auto-animate="true"}

![](https://i.imgur.com/fwEB4Cf.png){#tight-fig fig-align="center"}



# Training

## Pre Process {.unlisted}

:::{.incremental style="font-size: 80%;"} 
 - Gather a large enough data set of relevant images.  
   The size should match the difficulty of the task.  
   When using _Transfer Learning_ smaller dataset can be used.
 - Labeling  
   Apply labeling on the data if required.
 - Validate the Dataset  
   Validate all images are qualified and annotations are accurate.  
   For large dataset one might sample.
 - Processing of Images  
   Some processing of the images might be done offline.  
   For instance, removing color to make the model only structure sensitive.  
   Resizing can also be done in offline to reduce the overhead.  
 - Processing of Labels  
   Conversion of standards, filtering classes, etc...
:::

## Train the Model {.unlisted}

:::{.incremental style="font-size: 80%;"} 
 - Choose the Model  
   Select the model to be used according to the task.  
   Currently stick with `YOLOv8`.
 - Choose the Head (Task)  
   Modern YOLO models can do object detection, segmentation and key points detection.
 - Select the Hyper Parameters  
   Select the size of the model according to run time and training time constraints. In case of a doubt, start with the smallest.
 - Code the Scripts  
   Build the scripts for training.   
   Separate the scripts for data pre processing, training and evaluation.
:::

# Evaluation

## Score {.unlisted auto-animate="true"}

:::{#tight-div .incremental}
 * By definition the problem is _imbalanced_ (Background).
 * The most common score is the _Mean Average Precision_ (mAP).
:::

. . .

:::{#tight-math}
$$ \text{mAP} = \frac{1}{C} \sum_{c \in C} \frac{ \text{TP}_{\text{c}} }{ \text{TP}_{\text{c}} + \text{FP}_{\text{c}} } $$
:::

![](https://i.imgur.com/H3clPrk.png){#tight-fig fig-align="center" height="400px"}

## Score {.unlisted auto-animate="true"}

:::{#tight-div}
 * The most common score is the _Mean Average Precision_ (mAP).
:::

:::{#tight-math}
$$ \text{mAP} = \frac{1}{C} \sum_{c \in C} \frac{ \text{TP}_{\text{c}} }{ \text{TP}_{\text{c}} + \text{FP}_{\text{c}} } $$
:::

. . .

![](https://i.imgur.com/vPavBI6.png){#tight-fig fig-align="center" height="450px"}

## Real World Measures {.unlisted}

:::{.incremental}
 * While `mAP` is not ideal, it is common ("Best we have").
 * In some cases one might want to analyze different aspects:
    * Performance on specific type of objects.  
      For instance, small objects.
    * Analyze classification errors (Confusion Matrix).
    * Analyze regression errors: Size, Locations (Biases).
    * Analysis per class / weighted by classes.
:::

# Test Case - Cats & Dog Segmentation

## The Task {.unlisted}

:::{.incremental}
 * Given an Image, Classify each pixel into 3 classes:
    * Segment Dog pixels.
    * Segment Cat pixel.
    * Segment Background pixels.
:::

. . .

:::{.callout-caution icon=false}
#### [{{< iconify logos python >}}]{style="color: #02b875;"} Coding Task

 * Download the Workshop files from [Fixel Courses](https://github.com/FixelAlgorithmsTeam/FixelCourses).  
   The files are located in `AIProgram/2024_02/WorkShop003`.
 * Create a `conda` environment based on [`EnvImageSegmentation.yml`](https://github.com/FixelAlgorithmsTeam/FixelCourses/blob/master/AIProgram/2024_02/WorkShop002/EnvObjectDetection.yml).
:::

## The Data {.unlisted}

:::{.incremental}
 * Based on [The Oxford-IIIT Pet Dataset](https://www.robots.ox.ac.uk/~vgg/data/pets).
 * Each image is a _Full HD_ RGB image (`1920x1080x3`).
 * The images are extracted form a YouTube video (Frame).
 * In some images there might no objects to detect. 
:::

. . .

:::{.callout-caution icon=false}
#### [{{< iconify logos python >}}]{style="color: #02b875;"} Coding Task

 * Run the script `0002Dataset.py`.
 * Ensure the data in the folder `Data`.
 * The folder should contain 300 `png` files.
:::

## The Model {.unlisted}

:::{.incremental}
 * The Model: [`Ultralytics YOLOv8n`](https://github.com/ultralytics/ultralytics).
 * Set for _Object Detection_.
 * Image Input Size: `640x640`.
 * [Grid Size](https://github.com/ultralytics/ultralytics/issues/8863): `[80x80, 40x40, 20x20]`.  
   See [Defaults YOLOv8 Grid Size](https://stackoverflow.com/questions/75904407).
 * Number of Parameters: `3.2e6`.
:::

. . .

:::{.callout-note title="The Model Size"}

The choice of the smallest model (`n`) is due to the simplicity of the task and the constrained time for training.

:::

## Labeling {.unlisted auto-animate="true"}

:::{.incremental}
 * The labeling will done using the `LabelMe` JSON format.
 * The classes are: [`Ball`, `Referee`].  
   Mind the _capitalization_ of the letters.
 * Don't label if the object is partial due being at the edge of the image.
 * Do label if the object is partially occluded.
 * Use Rectangle Polygon (Bounding Box by `Ctrl + R`).
 * The box should be as tight as possible.  
   Use _Zoom In_.
:::

## Labeling {.unlisted auto-animate="true"}

:::{style="font-size: 60%;"}
 * The labeling will done using the `LabelMe` JSON format.
 * The classes are: [`Ball`, `Referee`].  
   Mind the _capitalization_ of the letters.
 * Don't label if the object is partial due being at the edge of the image.
 * Do label if the object is partially occluded.
 * Use Rectangle Polygon (Bounding Box).
 * The box should be as tight as possible.  
   Use _Zoom In_.
:::

. . .

:::{.callout-caution icon=false}
#### [{{< iconify logos python >}}]{style="color: #02b875;"} Coding Task

 * Download [LabelMe](https://github.com/labelmeai/labelme).  
   Use the pre compiled single file executables form `Releases`.
 * Put the [`.labelmerc`](https://github.com/FixelAlgorithmsTeam/FixelCourses/blob/master/AIProgram/2024_02/WorkShop002/.labelmerc) in `$USERFOLDER` / `$HOME`.
 * Run the program, load the folder and label your assigned files.
 * Upload the JSON files as instructed.
:::

## Pre Processing the Data {.unlisted}

:::{.incremental}
 * The image is relatively large (`1920x1080`).
 * The model was trained on `640x640` images with `[80x80, 40x40, 20x20]` grids.
 * Naive training means the data will be resized with factor `3`.
 * The ball size (_Bounding Box_) is about ~`12x12`.  
   After factorization it will be ~`4x4` pixels which is too low for valid performance.
 * Solution: Tiled Detection (See [SAHI: Slicing Aided Hyper Inference](https://github.com/obss/sahi)).
:::

## Training {.unlisted}

## Inference {.unlisted}



