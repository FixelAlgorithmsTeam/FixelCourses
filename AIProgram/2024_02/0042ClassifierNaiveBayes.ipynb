{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Fixel Algorithms](https://i.imgur.com/AqKHVZ0.png)](https://fixelalgorithms.gitlab.io/)\n",
    "\n",
    "# AI Program\n",
    "\n",
    "## Machine Learning - Supervised Learning - Classification - Naive Bayes Classifier\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 1.0.000 | 22/03/2024 | Royi Avital | First version                                                      |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/AIProgram/2024_02/0042ClassifierNaiveBayes.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T09:30:06.492269Z",
     "start_time": "2022-02-02T09:30:06.220934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, names\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Image Processing\n",
    "\n",
    "# Machine Learning\n",
    "\n",
    "# Miscellaneous\n",
    "import math\n",
    "import os\n",
    "from platform import python_version\n",
    "import random\n",
    "import timeit\n",
    "\n",
    "# Typing\n",
    "from typing import Callable, Dict, List, Optional, Set, Tuple, Union\n",
    "\n",
    "# Visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "from ipywidgets import Dropdown, FloatSlider, interact, IntSlider, Layout, SelectionSlider\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought.\n",
    "\n",
    "Code Notations:\n",
    "\n",
    "```python\n",
    "someVar    = 2; #<! Notation for a variable\n",
    "vVector    = np.random.rand(4) #<! Notation for 1D array\n",
    "mMatrix    = np.random.rand(4, 3) #<! Notation for 2D array\n",
    "tTensor    = np.random.rand(4, 3, 2, 3) #<! Notation for nD array (Tensor)\n",
    "tuTuple    = (1, 2, 3) #<! Notation for a tuple\n",
    "lList      = [1, 2, 3] #<! Notation for a list\n",
    "dDict      = {1: 3, 2: 2, 3: 1} #<! Notation for a dictionary\n",
    "oObj       = MyClass() #<! Notation for an object\n",
    "dfData     = pd.DataFrame() #<! Notation for a data frame\n",
    "dsData     = pd.Series() #<! Notation for a series\n",
    "hObj       = plt.Axes() #<! Notation for an object / handler / function handler\n",
    "```\n",
    "\n",
    "### Code Exercise\n",
    "\n",
    " - Single line fill\n",
    "\n",
    " ```python\n",
    " vallToFill = ???\n",
    " ```\n",
    "\n",
    " - Multi Line to Fill (At least one)\n",
    "\n",
    " ```python\n",
    " # You need to start writing\n",
    " ????\n",
    " ```\n",
    "\n",
    " - Section to Fill\n",
    "\n",
    "```python\n",
    "#===========================Fill This===========================#\n",
    "# 1. Explanation about what to do.\n",
    "# !! Remarks to follow / take under consideration.\n",
    "mX = ???\n",
    "\n",
    "???\n",
    "#===============================================================#\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# %matplotlib inline\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "# Matplotlib default color palette\n",
    "lMatPltLibclr = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "# sns.set_theme() #>! Apply SeaBorn theme\n",
    "\n",
    "runInGoogleColab = 'google.colab' in str(get_ipython())\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "DATA_FILE_ID        = r'12G6oUKCWzQnkDbv3TIcL9120YBdCwAmW'\n",
    "L_DATA_FILE_NAME    = ['IMDBReviewsText.txt', 'IMDBReviewsLabels.txt']\n",
    "D_CATEGORY          = {'positive': 1, 'negative': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courses Packages\n",
    "\n",
    "from DataManipulation import DownloadGDriveZip\n",
    "from DataVisualization import PlotConfusionMatrix, PlotLabelsHistogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Auxiliary Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier\n",
    "\n",
    "The _Naive Bayes Classifier_ is built on the assumption conditional independence between every pair of features given the value of the class variable.  \n",
    "\n",
    "Assume probabilistic model for the class given a set of features $\\boldsymbol{x}$:\n",
    "\n",
    "$$ P \\left( y \\mid {x}_{1}, \\dots, {x}_{d} \\right) = \\frac{P \\left( y \\right) P( {x}_{1}, \\dots, {x}_{d} \\mid y)} {P \\left( {x}_{1} , \\dots, {x}_{d} \\right)} $$\n",
    "\n",
    "Using the naive conditional independence assumption yields:\n",
    "\n",
    "$$ P \\left( y \\mid {x}_{1}, \\dots, {x}_{d} \\right) = \\frac{P \\left( y \\right) \\prod_{i = 1}^{d} P \\left( {x}_{i} \\mid y \\right)} {P \\left( {x}_{1}, \\dots, {x}_{d} \\right)} $$\n",
    "\n",
    "Given $P \\left( {x}_{1}, \\dots, {x}_{d} \\right)$ is a constant, one can optimize:\n",
    "\n",
    "$$ \\arg \\max_{y} P \\left( y \\right) \\prod_{i = 1}^{d} P \\left( {x}_{i} \\mid y \\right) $$\n",
    "\n",
    "The classifier model is set by the distribution set to the $i$ -th feature given the class: $P \\left( {x}_{i} \\mid y \\right)$.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> In lectures, for continuous features, a more general case is described where the features are _Jointly Gaussian_.  \n",
    "* <font color='brown'>(**#**)</font> The general case, where the joint distribution of the features (Which can be dependent) is unknown is usually modeled by a _Bayesian Net_.  \n",
    "One way to implement such net is by a Neural Network Net. See [Neural Networks vs. Bayesian Networks](https://datascience.stackexchange.com/questions/9818) and [Ehud Reiter's Blog - Bayesian vs Neural Networks](https://ehudreiter.com/2021/07/05/bayesian-vs-neural-networks).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis\n",
    "\n",
    "In this case highly polar reviews of movies will be analyzed for _Sentiment Analysis_.  \n",
    "The categories will be _negative_ or _positive_ reviews.   \n",
    "\n",
    "The RAW data is a review text.  \n",
    "The process of the classification will be:\n",
    "\n",
    "1. Download and Load the Data  \n",
    "The data will be downloaded and parsed into 2 lists: Reviews, Labels.\n",
    "2. Pre Process the Data  \n",
    "The text data will be cleaned, the labels will be transformed into numeric values.\n",
    "3. Feature Extraction  \n",
    "The text will be transformed into _histogram_ like of the words in the review.\n",
    "4. Classification  \n",
    "The classification will be done using Multinomial Naive Bayes model.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> [Multinomial Distribution](https://en.wikipedia.org/wiki/Multinomial_distribution) is a generalization of the [Binomial Distribution](https://en.wikipedia.org/wiki/Binomial_distribution). It suits the case of counting the number of occurrence of the words. \n",
    "* <font color='brown'>(**#**)</font> A math deep introduction is given in [Michael Collins - The Naive Bayes Model, Maximum Likelihood Estimation and the EM Algorithm](http://www.cs.columbia.edu/~mcollins/em.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# Data\n",
    "numSamplesRatio = 0.1\n",
    "\n",
    "# Features Engineering\n",
    "# Minimum Frequency of words\n",
    "minDf = 1\n",
    "# Maximum number of features (Histogram support)\n",
    "numFeaturesMax = 500_000\n",
    "minRatio = 2.5\n",
    "maxRatio = 5 #<! No Effect (Visualization)\n",
    "\n",
    "# Model\n",
    "α = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate / Load Data\n",
    "\n",
    "Loading [Andrew Maas' Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment) data set.  \n",
    "The original data was processed into 25,000 labeled reviews (See [Rockikz (`x4nth055`) - Sentiment Analysis Naive Bayes](https://github.com/x4nth055/sentiment_analysis_naive_bayes)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Data \n",
    "\n",
    "# Download the data from Google Drive\n",
    "DownloadGDriveZip(fileId = DATA_FILE_ID, lFileCont = L_DATA_FILE_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "\n",
    "# Read reviews - Python Context\n",
    "with open(L_DATA_FILE_NAME[0]) as hFile:\n",
    "    lReview = hFile.readlines() #<! Each review is a line\n",
    "\n",
    "# Read labels - Python Context\n",
    "with open(L_DATA_FILE_NAME[1]) as hFile:\n",
    "    lLabels = hFile.readlines() #<! Each review is a line\n",
    "\n",
    "\n",
    "print(f'Loaded {len(lReview)} reviews')\n",
    "print(f'Loaded {len(lLabels)} labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre Processing\n",
    "# 1. Remove leading and trailing spaces from text.\n",
    "# 2. Convert the labels: 'negative' -> 0, 'positive' -> 1 (Numpy Array).\n",
    "\n",
    "lReview = [reviewTxt.strip() for reviewTxt in lReview] #<! Remove leading and trailing whitespaces\n",
    "vY = np.array([D_CATEGORY[labelTxt.strip()] for labelTxt in lLabels]) #<! 'negative' -> 0, 'positive' -> 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Labels\n",
    "\n",
    "hA = PlotLabelsHistogram(vY)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / Test Split   \n",
    "\n",
    "Since the feature engineering will be mostly about counting and the distribution of the words, the split is done before the feature engineering.  \n",
    "This is done to prevent the train data pollution / contamination (Also known as [_Data Leakage_ in Machine Learning](https://en.wikipedia.org/wiki/Leakage_(machine_learning)))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numClass = len(np.unique(vY))\n",
    "lReviewTrain, lReviewTest, vYTrain, vYTest = train_test_split(lReview, vY, test_size = numSamplesRatio, shuffle = True, stratify = vY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "For simple based text tasks, most used features are based on the histogram of the words per object.  \n",
    "In general, the whole corpus of words in the data is described by a distribution.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The most simple\n",
    "\n",
    "In this case we'll use SciKit Learn [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) which learns the corpus of words.  \n",
    "Then, in its transformation phase, build the histogram per object.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Histogram Feature Vector\n",
    "\n",
    "# List of words to avoid counting\n",
    "lNames      = list(names.words())\n",
    "lNames      = [name.lower() for name in lNames]\n",
    "lStopWords  = list(stopwords.words())\n",
    "lStopWords  = [word.lower() for word in lStopWords]\n",
    "lIgnoreWords = list(ENGLISH_STOP_WORDS) + lNames + lStopWords\n",
    "\n",
    "# The Count Object \n",
    "oCntVec = CountVectorizer(strip_accents = 'ascii', stop_words = lIgnoreWords, min_df = minDf, max_features = numFeaturesMax)\n",
    "mXTrain = oCntVec.fit_transform(lReviewTrain) #<! Fit & Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features matrix is large and sparse.  \n",
    "The next move is to select important features.\n",
    "\n",
    "The concept, per feature, if it has some tendency to either class.  \n",
    "It makes sense since the assumption is each feature is independent of other given class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Class Ratio per Feature\n",
    "\n",
    "numFeatures = len(oCntVec.vocabulary_)\n",
    "lFeatNames  = list(oCntVec.get_feature_names_out())\n",
    "vClassRatio = np.zeros(numFeatures)\n",
    "\n",
    "# Number of occurrence per class\n",
    "vSumNeg = mXTrain.T @ (vYTrain == D_CATEGORY['negative'])\n",
    "vSumPos = mXTrain.T @ (vYTrain == D_CATEGORY['positive'])\n",
    "\n",
    "hF, hA = plt.subplots(figsize = (16, 6))\n",
    "hA.scatter(range(numFeatures), np.log1p(vSumNeg), color = lMatPltLibclr[0], label = 'Negative')\n",
    "hA.scatter(range(numFeatures), np.log1p(vSumPos), color = lMatPltLibclr[1], label = 'Positive')\n",
    "hA.set_xlabel('Feature Index')\n",
    "hA.set_ylabel('Log(Count)')\n",
    "hA.legend();\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratio per Feature\n",
    "\n",
    "# The ratio between the counts.\n",
    "# To make it symmetric, we'll take the maximum of both sides.\n",
    "vClsRatio = np.nanmax(np.column_stack((vSumNeg / (vSumPos + (1 / minRatio)), vSumPos / (vSumNeg + (1 / minRatio)))), axis = 1)\n",
    "# Limit the ratio to [0, 2]\n",
    "vClsRatio = np.clip(vClsRatio, a_min = 0, a_max = maxRatio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Ratio\n",
    "hF, hA = plt.subplots(figsize = (16, 6))\n",
    "hA.scatter(range(numFeatures), vClsRatio, color = lMatPltLibclr[0], label = 'Ratio')\n",
    "hA.axhline(y = minRatio, lw = 3, color = lMatPltLibclr[1], label = 'Minimum Ratio')\n",
    "hA.set_xlabel('Feature Index')\n",
    "hA.set_ylabel(f'Ratio [0, {maxRatio}]')\n",
    "hA.legend();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we build the words dictionary based on the selected words only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words Dictionary\n",
    "\n",
    "lDicWord = list(oCntVec.get_feature_names_out()[vClsRatio >= minRatio])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Features\n",
    "\n",
    "# The Count Object \n",
    "oCntVec = CountVectorizer(strip_accents = 'ascii', vocabulary = lDicWord)\n",
    "mXTrain = oCntVec.fit_transform(lReviewTrain) #<! Fit & Transform\n",
    "mXTest  = oCntVec.fit_transform(lReviewTest) #<! Fit & Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "\n",
    "print(f'The training features data shape: {mXTrain.shape}')\n",
    "print(f'The training labels data shape: {vYTrain.shape}')\n",
    "print(f'The test features data shape: {mXTest.shape}')\n",
    "print(f'The test labels data shape: {vYTest.shape}')\n",
    "print(f'The unique values of the labels: {np.unique(vY)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Model\n",
    "\n",
    "There are 2 common Naive Bayes Models:\n",
    "\n",
    "1. Gaussian Naive Bayes  \n",
    "   Models each **independent** feature by $P \\left( {x}_{i} \\mid y \\right) = \\frac{1}{\\sqrt{2 \\pi {\\sigma}_{y}^{2}}} \\exp \\left( -\\frac{{\\left( {x}_{i} - {\\mu}_{y} \\right)}^{2}}{2 {\\sigma}_{y}^{2}}\\right)$.  \n",
    "   The features are assumed to be continuous.  \n",
    "   It is the _go to_ choice for the case of continuous variables.  \n",
    "   It has some limitation for the case of bounded continuous variables.  \n",
    "   Implemented by [`GaussianNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html).\n",
    "2. Bernoulli Naive Bayes  \n",
    "   Models each **independent** feature by $P \\left( {x}_{i} \\mid y \\right) = P \\left( {x}_{i} \\mid y \\right) {x}_{i} + \\left( 1 - P \\left( {x}_{i} \\mid y \\right) \\right) \\left( 1 - {x}_{i} \\right)$.  \n",
    "   Models the probability of the _occurrence_ of the feature per class.  \n",
    "   Assumes each feature is binary. It explicitly punishes for non _occurrence_.  \n",
    "   Implemented by [`BernoulliNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html).\n",
    "3. Multinomial Naive Bayes  \n",
    "   Models the counting of the feature per class.  \n",
    "   Assumes each feature is non negative integer.  \n",
    "   Implemented by [`MultinomialNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html).\n",
    "\n",
    "\n",
    "* <font color='brown'>(**#**)</font> In lectures, the Gaussian Model does not assume independence of the features as above.  \n",
    "* <font color='brown'>(**#**)</font> Though the _Multinomial Naive Bayes_ assume counts, in the context of text features, it is known to work well with [_Term Frequency - Inverse Document Frequency_](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) (TF-IDF) features as well.  \n",
    "   See [How to Use TFIDF Vectors with Multinomial Naive Bayes](https://stackoverflow.com/questions/43237286).\n",
    "* <font color='brown'>(**#**)</font> The _Multinomial Naive Bayes_ sets the probability according to the counts, the more the better. The _Bernoulli Naive Bayes_ sets the probability based on occurrence, it _punishes_ if there is no occurrence. Hence _Bernoulli Naive Bayes_ has the benefit of explicitly modelling the absence of terms. It implies that a _Multinomial Naive Bayes_ classifier with frequency counts truncated to one is not equivalent of _Bernoulli Naive Bayes_ classifier. See [Difference Between Bernoulli and Multinomial Naive Bayes](https://datascience.stackexchange.com/questions/27624).  \n",
    "\n",
    "In the case above, to match the counts the _Multinomial Naive Bayes_ classifier will be used.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> Some advantages of NB Classifiers:\n",
    "  * Easy to train and predict with decent results in many cases. \n",
    "  * Usually, have a closed form solution for their parameters. \n",
    "  * Can handle new data efficiently (See the `partial_fit` method). \n",
    "  * Can handle mixed type of features inherently.\n",
    "* <font color='brown'>(**#**)</font> Some disadvantages of NB Classifiers: \n",
    "  * Can not be extended by _Ensembling_, _Boosting_, _Bagging_ as there is no variance to reduce.\n",
    "  * Multinomial model assumes the counts will be similar in train and test sets (Otherwise, requires smoothing).\n",
    "  * Performance degrades as the features becomes more dependent.\n",
    "  * Highly sensitive to imbalanced data.\n",
    "\n",
    "* <font color='red'>(**?**)</font> How mixed features, continuous, counts and binary, can be handled?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB Classfier Model\n",
    "oMultNomNBcls = MultinomialNB(alpha = α)\n",
    "oMultNomNBcls = oMultNomNBcls.fit(mXTrain, vYTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Train Data\n",
    "\n",
    "vHatY = oMultNomNBcls.predict(mXTrain)\n",
    "vYGt  = vYTrain\n",
    "\n",
    "valAcc = np.mean(vHatY == vYGt)\n",
    "valPrecision, valRecall, valF1, _ = precision_recall_fscore_support(vYGt, vHatY, pos_label = 1, average = 'binary')\n",
    "\n",
    "print(f'Accuracy  = {valAcc:0.3f}')\n",
    "print(f'Precision = {valPrecision:0.3f}')\n",
    "print(f'Recall    = {valRecall:0.3f}'   )\n",
    "print(f'F1 Score  = {valF1:0.3f}'       )\n",
    "\n",
    "dScore = {'Accuracy': valAcc}\n",
    "\n",
    "hF, hA = plt.subplots(figsize = (8, 8))\n",
    "PlotConfusionMatrix(vYGt, vHatY, normMethod = 'true', hA = hA, dScore = dScore, valFormat = '0.1%') #<! The accuracy should be >= than above!\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Test Data\n",
    "\n",
    "vHatY = oMultNomNBcls.predict(mXTest)\n",
    "vYGt  = vYTest\n",
    "\n",
    "valAcc = np.mean(vHatY == vYGt)\n",
    "valPrecision, valRecall, valF1, _ = precision_recall_fscore_support(vYGt, vHatY, pos_label = 1, average = 'binary')\n",
    "\n",
    "print(f'Accuracy  = {valAcc:0.3f}')\n",
    "print(f'Precision = {valPrecision:0.3f}')\n",
    "print(f'Recall    = {valRecall:0.3f}'   )\n",
    "print(f'F1 Score  = {valF1:0.3f}'       )\n",
    "\n",
    "dScore = {'Accuracy': valAcc}\n",
    "\n",
    "hF, hA = plt.subplots(figsize = (8, 8))\n",
    "PlotConfusionMatrix(vYGt, vHatY, normMethod = 'true', hA = hA, dScore = dScore, valFormat = '0.1%') #<! The accuracy should be >= than above!\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='green'>(**@**)</font> Redo the exercise using TF-IDF. See SciKit Learn's [`TfidfTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) and [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html).\n",
    "* <font color='green'>(**@**)</font> Redo the exercise using _Bernoulli Naive Bayes_ classifier. Adjust the features accordingly.\n",
    "* <font color='green'>(**@**)</font> Try to improve results for accuracy. You may use `NLTK` Lemmatizer (`WordNetLemmatizer`).\n",
    "* <font color='brown'>(**#**)</font> You may read about [The Difference Between a Bayesian Network and a Naive Bayes Classifier](https://stackoverflow.com/questions/12298150).  \n",
    "  It has a great example on the _XOR Problem_ and the problem with the assumption of feature independence.\n",
    "* <font color='brown'>(**#**)</font> You may read about [The Difference Between a Bayesian Network and a Naive Bayes Classifier](https://stats.stackexchange.com/questions/212240) (Different source).\n",
    "* <font color='brown'>(**#**)</font> You may read about [Bayesian Neural Networks: Implementing, Training, Inference With the JAX Framework](https://neptune.ai/blog/bayesian-neural-networks-with-jax)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> To work with mixed features:\n",
    "  1. Build a Multinomial NB classifier for the _count based features_.\n",
    "  2. Build a Bernoulli NB classifier using the _binary features_.\n",
    "  3. Build a Gaussian NB classifier using the _continuous features_.\n",
    "  4. Multiply the probabilities of each model to find the maximum probability based on all features (Using the **independence assumption).\n",
    "\n",
    "</br>\n",
    "\n",
    "* <font color='brown'>(**#**)</font> Categorical features can be transformed into counts using _dummy variables_.\n",
    "* <font color='brown'>(**#**)</font> Some continuous features should be engineered to better suit a Gaussian model (Like `log()`)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
