{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/qkg2E2D.png)\n",
    "\n",
    "# UnSupervised Learning Methods\n",
    "\n",
    "## Exercise 001 - Part II\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 0.1.000 | 12/03/2023 | Royi Avital | First version                                                      |\n",
    "|         |            |             |                                                                    |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/UnSupervisedLearningMethods/2023_03/0004EstimationNonParametric.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T09:30:06.492269Z",
     "start_time": "2022-02-02T09:30:06.220934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# Machine Learning\n",
    "\n",
    "# Miscellaneous\n",
    "import os\n",
    "import math\n",
    "from platform import python_version\n",
    "import random\n",
    "import urllib.request\n",
    "\n",
    "# Typing\n",
    "from typing import Callable, List, Tuple, Union\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython\n",
    "from IPython.display import Image, display\n",
    "from ipywidgets import Dropdown, FloatSlider, interact, IntSlider, Layout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "%matplotlib inline\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "# sns.set_theme() #>! Apply SeaBorn theme\n",
    "\n",
    "runInGoogleColab = 'google.colab' in str(get_ipython())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "DATA_FILE_URL   = r'https://drive.google.com/uc?export=download&confirm=9iBg&id=1ZmoK4qFlXs142kLBJeHSKig7DOKEnLIX'\n",
    "DATA_FILE_NAME  = r'ImgData.mat'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guidelines\n",
    "\n",
    " - Answer all questions within the Jupyter Notebook.\n",
    " - Open questions are in part I of the exercise.\n",
    " - Coding based questions are in the subsequent notebooks.\n",
    " - Use MarkDown + MathJaX + Code to answer.\n",
    " - Submission in groups (Single submission per group).\n",
    " - You may and _should_ use the forums for question.\n",
    " - Good Luck!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Descent Methods\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "Let $\\boldsymbol{Y} \\in \\mathbb{R}^{d \\times d}$ be a blurred version of the image $\\boldsymbol{X} \\in \\mathbb{R}^{d \\times d}$:\n",
    "\n",
    "$$ \\boldsymbol{Y} = \\boldsymbol{H} \\boldsymbol{X} \\boldsymbol{H}^{T} $$\n",
    "\n",
    "Where $\\boldsymbol{H} \\in\\mathbb{R}^{d \\times d}$ is a known separable blurring matrix.\n",
    "\n",
    "The goal is to find $\\boldsymbol{X}$ that minimizes:\n",
    "\n",
    "$$\\boldsymbol{X}^{\\star} = \\arg \\min_{\\boldsymbol{X}} f \\left( \\boldsymbol{X} \\right) = \\arg \\min_{\\boldsymbol{X}} {\\left\\| \\boldsymbol{H} \\boldsymbol{X} \\boldsymbol{H}^{T} - \\boldsymbol{Y} \\right\\|}_{F}^{2}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate / Load Data\n",
    "\n",
    "Load the data: $\\boldsymbol{Y}$ and $\\boldsymbol{H}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Data\n",
    "# This section downloads data from the given URL if needed.\n",
    "\n",
    "if not os.path.exists(DATA_FILE_NAME):\n",
    "    urllib.request.urlretrieve(DATA_FILE_URL, DATA_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dData = sp.io.loadmat(DATA_FILE_NAME)\n",
    "mY    = dData['Y']\n",
    "mH    = dData['H']\n",
    "\n",
    "print(f'The image dimensions are: {mY.shape}')\n",
    "print(f'The filter matrix dimensions are: {mH.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Data\n",
    "\n",
    "This sections illustrates the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hF, hAs = plt.subplots(nrows = 1, ncols = 2, figsize = (10, 4))\n",
    "hAs = hAs.flat\n",
    "\n",
    "hAs[0].imshow(mY, cmap = 'gray')\n",
    "hAs[0].set_title('$Y = H X H^T$')\n",
    "hAs[1].matshow(mH)\n",
    "hAs[1].set_title('$H$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Question\n",
    "\n",
    "Find a closed form expression for $\\boldsymbol{X}^{\\star}$ as a function of $\\boldsymbol{Y}$ and $\\boldsymbol{H}$.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Solution\n",
    "\n",
    "<font color='red'>??? Fill the answer here ???</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Task\n",
    "\n",
    "Compute $\\boldsymbol{X}^{\\star}$ and plot it.\n",
    "\n",
    "**Tip**: Avoid inverting the same matrix twice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================Fill This===========================#\n",
    "# 1. Compute the optimal `mX` using the derivation.\n",
    "# !! You may use `np.linalg.inv()` or better `np.linalg.solve()` or np.linalg.lstsq().\n",
    "mX = ???\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Estimated Image\n",
    "\n",
    "hF, hAs = plt.subplots(nrows = 1, ncols = 2, figsize = (10, 4))\n",
    "hAs = hAs.flat\n",
    "\n",
    "hAs[0].imshow(mY, cmap = 'gray')\n",
    "hAs[0].set_title('$Y = H X H^T$')\n",
    "hAs[1].imshow(mX)\n",
    "hAs[1].set_title('$X^{\\star}$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Solution\n",
    "\n",
    "In practice, for many real world problems, the direct solution can not be computed due to the inversion of a matrix.  \n",
    "In such cases, the model is given by a sparse matrix and the solution is calculated by using only _matrix vector operations_ (Also called _MatVec_, namely matrix vector multiplication).  \n",
    "\n",
    "In this section we'll implement such method based on the Gradient Descent.\n",
    "\n",
    "Given the function:\n",
    "\n",
    "$$ f \\left( \\boldsymbol{X} \\right) = {\\left\\| \\boldsymbol{H} \\boldsymbol{X} \\boldsymbol{H}^{T} - \\boldsymbol{Y} \\right\\|}_{F}^{2} $$\n",
    "\n",
    "### 4.3. Task\n",
    "\n",
    "Implement the following functions:\n",
    "\n",
    "```python\n",
    "#==================================================#\n",
    "def ObjF(mX):\n",
    "#==================================================#\n",
    "def GradF(mX):\n",
    "#==================================================#\n",
    "def ApplyGradientDescent(objF, gradF, mX0, μ, numIter, ε):\n",
    "#==================================================#\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================Fill This===========================#\n",
    "# 1. Implement the objective function.\n",
    "# !! You may use `np.linalg.norm()` or the element wise definition of the Frobenius norm (See `np.linalg.inner()`).\n",
    "def ObjF(mX: np.ndarray) -> float:\n",
    "    '''\n",
    "    f(X) = ||HXH^T - Y||_F^2 is the objective function\n",
    "    '''\n",
    "    pass\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================Fill This===========================#\n",
    "# 1. Implement the gradient of the objective function.\n",
    "def GradF(mX: np.ndarray) -> np.ndarray:\n",
    "    '''\n",
    "    Df(X) = ∇f(X) is the objective gradient\n",
    "    '''\n",
    "    pass\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================Fill This===========================#\n",
    "# 1. Implement the gradient descent optimizer function.\n",
    "def ApplyGradientDescent(objF: Callable, gradF: Callable, mX0: np.ndarray, μ: float, numIter: int, ε: float) -> Tuple[np.ndarray, List]:\n",
    "    '''\n",
    "    Apply gradient descent.\n",
    "    Args:\n",
    "        objF    - Objective function (Callable).\n",
    "        gradF   - Objective gradient function (Callable).\n",
    "        mX0     - Initial point (Array).\n",
    "        μ       - Step size / Learning rate (Float).\n",
    "        numIter - Maximum number of iterations (Integer).\n",
    "        ε       - Stopping criterion value (Float).\n",
    "    Output:\n",
    "        mX      - The converged mX (Array).\n",
    "        lF      - The value objective for each iteration (List).\n",
    "    Stopping criterion:\n",
    "        - Stop iterate when ||∇f(X)||_F < (numPixels * ε).\n",
    "        - Stop iterate after `numIter` iterations.\n",
    "    '''\n",
    "\n",
    "    lF = []\n",
    "    pass\n",
    "#===============================================================#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Task\n",
    "\n",
    " - Set a reasonable initial point for `mX0`.\n",
    " - Set the parameter `μ`.\n",
    " - Set the parameter `numIter`.\n",
    " - Set the parameter `ε`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================Fill This===========================#\n",
    "mX0     = ???\n",
    "μ       = ???\n",
    "numIter = ???\n",
    "ε       = ???\n",
    "#===============================================================#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Task\n",
    "\n",
    "In the following section we'll use the function `ApplyGradientDescent()` to estimate the image `mX`.  \n",
    "\n",
    " - Optimize the parameters above to get a comparable result as fast as you can.\n",
    " - Time the run time of the function and the number of iterations required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================Fill This===========================#\n",
    "# 1. Estimate `mXIter` using `ApplyGradientDescent()`.\n",
    "# 2. Time the run time and measure the number of iterations to converge.\n",
    "# 3. Optimize the parameters to get a comparable result to `mX` from above.\n",
    "\n",
    "mXIter = ???\n",
    "\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================Fill This===========================#\n",
    "# 1. Plot `mY`, `mX` and `mXIter`.\n",
    "# 2. Calculate the MSE between `mX` and `mXIter`.\n",
    "# 3. In the title of the plots show: The run time, the number of iterations and the MSE.\n",
    "?????\n",
    "#===============================================================#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acceleration Methods  \n",
    "\n",
    "The convergence speed of the 1st derivative method is sometimes very slow (Depends on the condition number of the model).  \n",
    "For small problems one might use a 2nd order methods, yet those are more sensitive and require more calculations per iteration.\n",
    "\n",
    "[Yurii Nesterov](https://en.wikipedia.org/wiki/Yurii_Nesterov), a Russian mathematician, an internationally recognized expert in convex optimization, was a pioneer in developing some approaches to [acceleration of 1st order methods](https://en.wikipedia.org/wiki/Gradient_descent#Fast_gradient_methods).  \n",
    "\n",
    "In this section we'll implement such method, yet based on the [_momentum_ approach](https://en.wikipedia.org/wiki/Gradient_descent#Momentum_or_heavy_ball_method).\n",
    "\n",
    "![](https://i.imgur.com/hFQv6Sa.png)\n",
    "\n",
    "Let's compare the update rule of both methods:\n",
    "\n",
    " - Vanilla Gradient Descent: $\\boldsymbol{x}^{\\left( k \\right)} = \\boldsymbol{x}^{\\left( k - 1 \\right)} - \\mu \\nabla f \\left( \\boldsymbol{x}^{\\left( k - 1 \\right)} \\right)$.\n",
    " - Momentum Accelerated Gradient Descent: $\\boldsymbol{x}^{\\left( k \\right)} = \\boldsymbol{x}^{\\left( k - 1 \\right)} - \\mu \\nabla f \\left( \\boldsymbol{x}^{\\left( k - 1 \\right)} \\right) + \\beta \\left( \\boldsymbol{x}^{\\left( k - 1 \\right)} - \\boldsymbol{x}^{\\left( k - 2 \\right)} \\right) $.  \n",
    "   Where $\\beta \\in \\left[ 0, 1 \\right)$ (Typically $\\beta \\in \\left[ 0.9, 0.99 \\right]$).\n",
    "\n",
    "The momentum method basically incorporate more data in order to optimize the direction of descent."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Question\n",
    "\n",
    "Show that $\\boldsymbol{x}^{\\left( k \\right)}$ can be expressed by:\n",
    "\n",
    "$$\\boldsymbol{x}^{\\left( k + 1 \\right)} = \\boldsymbol{x}^{\\left( k \\right)} - \\mu \\sum_{l = 0}^{k} \\beta^{k - l} \\nabla f \\left( \\boldsymbol{x}^{\\left( l \\right)} \\right)$$\n",
    "\n",
    "* <font color='brown'>(**#**)</font> Basically, the update step is a combination of all (Weighted) previous steps."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Solution\n",
    "\n",
    "<font color='red'>??? Fill the answer here ???</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6. Task\n",
    "\n",
    "Implement the following function:\n",
    "\n",
    "```python\n",
    "#====================================================#\n",
    "def ApplyGradientDescentMomentum(objF, gradF, mX0, μ, β, numIter, ε):\n",
    "#====================================================#\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================Fill This===========================#\n",
    "# 1. Implement the momentum accelerated gradient descent optimizer function.\n",
    "def ApplyGradientDescentMomentum(objF: Callable, gradF: Callable, mX0: np.ndarray, μ: float, β: float, numIter: int, ε: float) -> Tuple[np.ndarray, List]:\n",
    "    '''\n",
    "    Apply gradient descent.\n",
    "    Args:\n",
    "        objF    - Objective function (Callable)\n",
    "        gradF   - Objective gradient function (Callable)\n",
    "        mX0     - Initial point (Array)\n",
    "        μ       - Step size / Learning rate (Float)\n",
    "        numIter - Maximum number of iterations (Integer)\n",
    "        β       - Momentum decaying factor (Float)\n",
    "        ε       - Stopping criterion value (Float)\n",
    "    Output:\n",
    "        mX      - The converged mX (Array).\n",
    "        lF      - The value objective for each iteration (List)\n",
    "    Stopping criterion:\n",
    "        - Stop iterate when ||∇f(X)||_F < (numPixels * ε).\n",
    "        - Stop iterate after `numIter` iterations.\n",
    "    '''\n",
    "\n",
    "    lF = []\n",
    "    pass\n",
    "#===============================================================#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7. Task\n",
    "\n",
    "Repeat the above for the accelerated method:\n",
    "\n",
    " - Set the the same initial `mX0` as above.\n",
    " - Set the parameters for the `ApplyGradientDescentMomentum()` function.\n",
    " - Run the function, optimize parameters to converge as fast as you can while achieving similar result to `mX`.\n",
    " - Plot `mY`, `mX`, `mXIter`, `mXMomentum`. Write the MSE between `mXIter` and `mXMomentum` to `mX`, Write the number of iterations for each and the run time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================Fill This===========================#\n",
    "# 1. Estimate `mXIter` using `ApplyGradientDescentMomentum()`.\n",
    "# 2. Time the run time and measure the number of iterations to converge.\n",
    "# 3. Optimize the parameters to get a comparable result to `mX` from above.\n",
    "\n",
    "mXMomentum = ???\n",
    "\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================Fill This===========================#\n",
    "# 1. Plot `mY`, `mX`, `mXIter` and `mXMomentum`.\n",
    "# 2. Calculate the MSE between `mX` <-> `mXIter`, `mX` <-> `mXMomentum`.\n",
    "# 3. In the title of the plots show: The run time, the number of iterations and the MSE (For `mXIter` and `mXMomentum`).\n",
    "?????\n",
    "#===============================================================#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8. Task (Bonus 3%)\n",
    "\n",
    "Plot the objective value function as a function of the iteration for both iterative methods.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================Fill This===========================#\n",
    "# 1. Plot the objective value function as a function of the iteration index for both iterative methods.\n",
    "?????\n",
    "#===============================================================#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
