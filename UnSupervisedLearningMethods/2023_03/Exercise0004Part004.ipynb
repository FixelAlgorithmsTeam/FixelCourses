{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/qkg2E2D.png)\n",
    "\n",
    "# UnSupervised Learning Methods\n",
    "\n",
    "## Exercise 004 - Part IV\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 0.1.000 | 17/06/2023 | Royi Avital | First version                                                      |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/UnSupervisedLearningMethods/2023_03/Exercise0004Part004.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T09:30:06.492269Z",
     "start_time": "2022-02-02T09:30:06.220934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import Isomap, MDS, TSNE\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from umap import UMAP\n",
    "\n",
    "# Computer Vision\n",
    "\n",
    "# Miscellaneous\n",
    "import os\n",
    "import math\n",
    "from platform import python_version\n",
    "import random\n",
    "import time\n",
    "import urllib.request\n",
    "\n",
    "# Typing\n",
    "from typing import Callable, List, Tuple, Union\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython\n",
    "from IPython.display import Image, display\n",
    "from ipywidgets import Dropdown, FloatSlider, interact, IntSlider, Layout\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# %matplotlib inline\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "# sns.set_theme() #>! Apply SeaBorn theme\n",
    "\n",
    "runInGoogleColab = 'google.colab' in str(get_ipython())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "DATA_FILE_URL   = r'https://drive.google.com/uc?export=download&confirm=9iBg&id=1UXLdZgXwClgwZVszRq88UaaN2nvgMFiC'\n",
    "DATA_FILE_NAME  = r'BciData.npz'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary Functions\n",
    "\n",
    "def Plot2DScatter(mX: np.ndarray, hA: plt.Axes, vC: np.ndarray = None) -> None:\n",
    "    m = mX.min()\n",
    "    M = mX.max()\n",
    "    if vC is not None:\n",
    "        hA.scatter(*mX.T, s = 50,  c = vC, edgecolor = 'k', alpha = 1)\n",
    "    else:\n",
    "        hA.scatter(*mX.T, s = 50,  c = 'lime', edgecolor = 'k', alpha = 1)\n",
    "    hA.set_xlim([m, M])\n",
    "    hA.set_ylim([m, M])\n",
    "    hA.set_xlabel('$x_1$')\n",
    "    hA.set_ylabel('$x_2$')\n",
    "\n",
    "\n",
    "def PlotLabelsHistogram(vY: np.ndarray, hA = None, lClass = None, xLabelRot: int = None) -> plt.Axes:\n",
    "\n",
    "    if hA is None:\n",
    "        hF, hA = plt.subplots(figsize = (8, 6))\n",
    "    \n",
    "    vLabels, vCounts = np.unique(vY, return_counts = True)\n",
    "\n",
    "    hA.bar(vLabels, vCounts, width = 0.9, align = 'center')\n",
    "    hA.set_title('Histogram of Classes / Labels')\n",
    "    hA.set_xlabel('Class')\n",
    "    hA.set_ylabel('Number of Samples')\n",
    "    hA.set_xticks(vLabels)\n",
    "    if lClass is not None:\n",
    "        hA.set_xticklabels(lClass)\n",
    "    \n",
    "    if xLabelRot is not None:\n",
    "        for xLabel in hA.get_xticklabels():\n",
    "            xLabel.set_rotation(xLabelRot)\n",
    "\n",
    "    return hA\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guidelines\n",
    "\n",
    " - Fill the full names and ID's of the team members in the `Team Members` section.\n",
    " - Answer all questions / tasks within the Jupyter Notebook.\n",
    " - Use MarkDown + MathJaX + Code to answer.\n",
    " - Verify the rendering on VS Code.\n",
    " - Submission in groups (Single submission per group).\n",
    " - You may and _should_ use the forums for questions.\n",
    " - Good Luck!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> The `Import Packages` section above imports most needed tools to apply the work. Please use it.\n",
    "* <font color='brown'>(**#**)</font> You may replace the suggested functions to use with functions from other packages.\n",
    "* <font color='brown'>(**#**)</font> Whatever not said explicitly to implement maybe used by a 3rd party packages.\n",
    "* <font color='brown'>(**#**)</font> The total run time of this notebook must be **lower than 90 [Sec]**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Members\n",
    "\n",
    " - `<FULL>_<NAME>_<ID001>`.\n",
    " - `<FULL>_<NAME>_<ID002>`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate / Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Data\n",
    "# This section downloads data from the given URL if needed.\n",
    "\n",
    "if (DATA_FILE_NAME != 'None') and (not os.path.exists(DATA_FILE_NAME)):\n",
    "    urllib.request.urlretrieve(DATA_FILE_URL, DATA_FILE_NAME)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. T-SNE and UMAP\n",
    "\n",
    "In this section we'll use the _t-SNE_ and _UMAP_ algorithms to analyze EEG Signals in the context of _Brain Computer Interface_ (BCI).  \n",
    "\n",
    "### The BCI Data Set\n",
    "\n",
    "The Brain Computer Interfaces (BCI) Data from [BCI Competition IV](https://www.bbci.de/competition/iv/).  \n",
    "Specifically we'll use [data set 2a](https://www.bbci.de/competition/iv/#dataset2a) provided by the Institute for Knowledge Discovery (Laboratory of Brain Computer Interfaces), Graz University of Technology, (Clemens Brunner, Robert Leeb, Gernot Müller-Putz, Alois Schlögl, Gert Pfurtscheller).  \n",
    "This is a recording of EEG signals while the subject is doing a cued movement of the left hand, right hand, feet or tongue.\n",
    "\n",
    "The data is composed of:\n",
    "\n",
    " * 22 EEG channels (0.5-100Hz; notch filtered).\n",
    " * Sampling Rate of 250 [Hz] for 4 [Sec] for total of 1000 samples.\n",
    " * 4 classes: _left hand_, _right hand_, _feet_, _tongue_.\n",
    " * 9 subjects.\n",
    " * 287 measurements.\n",
    "\n",
    "</br>\n",
    "\n",
    "* <font color='brown'>(**#**)</font> You should install [UMAP](https://umap-learn.readthedocs.io/en/latest/index.html) on your system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Data\n",
    "\n",
    "dData = np.load(DATA_FILE_NAME)\n",
    "mX    = dData['mX1']\n",
    "vY    = dData['vY1']\n",
    "\n",
    "# Sample: An observation of a subject.\n",
    "# Measurement: Sample in time of the EEG signal.\n",
    "# Channel: EEG Channel.\n",
    "numSamples, numMeasurements, numChannels = mX.shape \n",
    "lLabel = ['Left Hand', 'Right Hand', 'Foot', 'Tongue'] #<! The labels\n",
    "\n",
    "print(f'The data shape: {mX.shape}')\n",
    "print(f'The number of samples: {mX.shape[0]}')\n",
    "print(f'The number of measurements per sample: {mX.shape[1]}')\n",
    "print(f'The number of EEG channels: {mX.shape[2]}')\n",
    "print(f'The classes labels: {np.unique(vY)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> In the above `sample` is used per object detected.\n",
    "* <font color='brown'>(**#**)</font> In the above `measurement` is used to describe the time sample (Like in Signal Processing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Data\n",
    "\n",
    "numRowsPlot = 3\n",
    "numColsPlot = 2\n",
    "\n",
    "numPlots = numRowsPlot * numColsPlot\n",
    "\n",
    "hF, hAs = plt.subplots(nrows = numRowsPlot, ncols = numColsPlot, figsize = (18, 10))\n",
    "hAs = hAs.flat\n",
    "\n",
    "vIdx = np.random.choice(numSamples, numPlots, replace = False)\n",
    "\n",
    "for sampleIdx, hA in zip(vIdx, hAs):\n",
    "    mXX = mX[sampleIdx, :, :]\n",
    "    hA.plot(mXX)\n",
    "    hA.set_title(f'EEG Signals of Sample {sampleIdx:04d} with Label {lLabel[vY[sampleIdx]]}')\n",
    "    hA.set_xlabel('Measurement Index')\n",
    "    hA.set_ylabel('Measurement Value')\n",
    "\n",
    "hF.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Data\n",
    "# Show a shifted and scaled channels at a single plot.\n",
    "# You may run it several times to observe different measurements.\n",
    "\n",
    "numSamples, numMeasurements, numChannels = mX.shape #<! Samples, Time Samples, Channels\n",
    "idx     = np.random.randint(numSamples)\n",
    "mXi     = mX[idx, :, :].copy()\n",
    "yi      = vY[idx]\n",
    "\n",
    "# Normalizing and Vertically Shifting\n",
    "mXi -= mXi.mean(0)\n",
    "mXi /= 20\n",
    "mXi += np.arange(numChannels)[None, :]\n",
    "vT   = np.linspace(0, 4, numMeasurements, endpoint = False)\n",
    "\n",
    "hF, hA = plt.subplots(figsize = (10, 6))\n",
    "hA.plot(vT, mXi)\n",
    "hA.set_title (f'Raw EEG Data\\nTrue Label = {lLabel[yi]}')\n",
    "hA.set_xlabel('Time [Sec]')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of Labels\n",
    "\n",
    "hA = PlotLabelsHistogram(vY, lClass = lLabel)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> The data is balanced."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1. Processing RAW Data\n",
    "\n",
    "In this section we'll compare 4 dimensionality reduction methods:\n",
    "\n",
    " - IsoMap.\n",
    " - MDS.\n",
    " - t-SNE.\n",
    " - UMAP.\n",
    "\n",
    "We'll reduce the data dimension to `d = 2` and use the reference labels to display result.\n",
    "\n",
    "The steps:\n",
    "\n",
    "1. Data Pre Processing  \n",
    "     * Reshape data into `mXRaw` such that each measurement is a row.  \n",
    "       Namely the data should have dimensions of `287 x 22_000`.  \n",
    "     * Apply dimensionality reduction using PCA to keep most of the data energy.\n",
    "2. Set Parameters  \n",
    "   Set parameters of each method to get results.  \n",
    "   Set the amount of energy preserved using the PCA pre processing.\n",
    "   **No need for grid search, just try and error of few values**.\n",
    "3. Apply the Transform  \n",
    "   Apply the transform on the data.  \n",
    "   The low dimensional data should be in 2D space.  \n",
    "4. Display Results  \n",
    "   Create a subplot per method showing the results and the used parameters.  \n",
    "\n",
    "* <font color='brown'>(**#**)</font> No need to self implement any of the methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================Fill This===========================#\n",
    "# 1. Pre Process data:\n",
    "#    * Reshape to (287 x 22_000).\n",
    "#    * Apply PCA to preserve `relEnergy` of the energy.\n",
    "# 2. Apply Dimensionality Reduction (d = 2) using IsoMap, MDS, t-SNE and UMAP.    \n",
    "#    Choose a reasonable parameters for each.\n",
    "# 3. Display the Low Dimensionality data.  \n",
    "#    Include the parameters in the title of each method.\n",
    "# !! The output should be a figure of 1x4 axes (2D Scatter and 3D Scatter per method).\n",
    "# !! You may use `Plot2DScatter()` for displaying the the data.\n",
    "\n",
    "d           = ???\n",
    "relEnergy   = ???\n",
    "\n",
    "\n",
    "?????\n",
    "\n",
    "#===============================================================#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.1 Question\n",
    "\n",
    "1. What's the purpose of the PCA pre process?\n",
    "2. Explain the results. Think in the context of being able to classify the body part in action."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.1 Solution\n",
    "\n",
    "<font color='red'>??? Fill the answer here ???</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2. Feature Engineering\n",
    "\n",
    "Form the above it is clear the RAW measurements are not a good representation of the data.  \n",
    "Since each sample is basically $\\boldsymbol{X}_{i}\\in\\mathbb{R}^{1000 \\times 22}$ we can work on the time axis of the data.  \n",
    "\n",
    "The strategy is as following:\n",
    "\n",
    " * Use the Covariance Matrix is a feature of the sample.\n",
    " * Use a metric optimized to covariance matrices.\n",
    "\n",
    "Do the following steps:\n",
    "\n",
    "1. Implements `GenCovMat()`  \n",
    "   For each sample $\\boldsymbol{X}_{i}\\in\\mathbb{R}^{1000 \\times 22}$, compute the covariance matrix $\\boldsymbol{C}_{i}\\in\\mathbb{R}^{22\\times22}$.  \n",
    "   You may use [`np.cov()`](https://numpy.org/doc/stable/reference/generated/numpy.cov.html).\n",
    "2. Implement `SpdMetric()`  \n",
    "   Given 2 SPD matrices in the form of vectors computes the geodesic distance between them.\n",
    "3. Apply the Feature Engineering  \n",
    "   The end of this process should be a matrix `mZ` with dimensions (287 x 22^2)\n",
    "4. Generate the Distance Matrix  \n",
    "   Using `SpdMetric()` generate the distance matrix of the data.  \n",
    "   The output should be a matrix `mD` with dimensions (287 x 287).\n",
    "\n",
    "#### The SPD Matrices Metric\n",
    "\n",
    "An optimized metric for the manifold of SPD Matrices, is given by the SPD Metric (`SpdMetric`):\n",
    "\n",
    "$$d\\left(\\boldsymbol{C}_{i},\\boldsymbol{C}_{j}\\right)=\\sqrt{\\sum_{i=1}^{d}\\log^{2}\\left(\\lambda_{i}\\left(\\boldsymbol{C}_{i}^{-1}\\boldsymbol{C}_{j}\\right)\\right)}$$\n",
    "\n",
    "Where ${\\lambda}_{i} \\left( \\cdot \\right)$ extract the $i$ -th eigen value of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenCovMat(mX: np.ndarray) -> np.ndarray:\n",
    "    '''\n",
    "    Calculates the covariance matrices of the input data.\n",
    "    Args:\n",
    "        mX - Input data with shape (N x T x C)\n",
    "    Output:\n",
    "        mC - N covariance matrices with shape (N x C x C)\n",
    "    '''\n",
    "\n",
    "    #===========================Fill This===========================#\n",
    "    # 1. Calculate the covariance matrices array.\n",
    "    # !! Do not use any loop!\n",
    "\n",
    "    pass\n",
    "    #===============================================================#\n",
    "\n",
    "    return mC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SpdMetric(vP: np.ndarray, vQ: np.ndarray) -> float:\n",
    "    '''\n",
    "    Calculates the AIRM geodesic distance between two SPD matrices.\n",
    "    Args:\n",
    "        mP    - An SPD matrix in the form of a vector with shape (d^2, )\n",
    "        mQ    - An SPD matrix in the form of a vector with shape (d^2, )\n",
    "    Output:\n",
    "        geoDist - The geodesic distance, dist ≥ 0\n",
    "    '''\n",
    "\n",
    "    #===========================Fill This===========================#\n",
    "    # 1. Convert the vectors into a matrix.\n",
    "    # 2. Calculate the geodesic distance of SPD matrices.\n",
    "    # !! Do not use any loop!\n",
    "    # !! You may find `scipy.linalg.eigvalsh()` useful.\n",
    "\n",
    "    pass\n",
    "    #===============================================================#\n",
    "    \n",
    "    return geoDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Data\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Generate the covariance matrices.\n",
    "# 2. Reshape data into the samples matrix.\n",
    "# 3. Calculate the distance matrix.\n",
    "# !! Don't use any loop!\n",
    "# !! You may use `pairwise_distances()` in order to avoid loops.\n",
    "\n",
    "pass\n",
    "#===============================================================#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Low Dimensional Data\n",
    "# This shows the result using IsoMap with SpdMetric.\n",
    "\n",
    "oIsoMapDr = Isomap(n_neighbors = 3, n_components = d, metric = 'precomputed')\n",
    "\n",
    "hF = plt.figure(figsize = (12, 6))\n",
    "    \n",
    "mZi = oIsoMapDr.fit_transform(mD)\n",
    "hA = hF.add_subplot()\n",
    "Plot2DScatter(mZi, hA, vY)\n",
    "hA.set_title(f'IsoMap (SPD Metric) with d = {d}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.2.1. Question\n",
    "\n",
    "In the above the _IsoMap_ is used.  \n",
    "If one would like to use SciKit Learn's _Spectral Embedding_ (See [`SpectralEmbedding`](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.SpectralEmbedding.html)) one need to supply _Affinity Matrix_ (See `affinity` parameter).  \n",
    "Describe how would you generate such matrix based on `SpdMetric()`.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> SciKit Learn's _Spectral Embedding_ is equivalent to _Laplacian Eigenmaps_ on lecture notes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.2.1. Solution\n",
    "\n",
    "<font color='red'>??? Fill the answer here ???</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3. Dimensionality Reduction and Classification on Engineered Data\n",
    "\n",
    "In this section we'll focus on `t-SNE` and `UMAP`.  \n",
    "\n",
    "1. Apply Dimensionality Reduction  \n",
    "   Apply `t-SNE` and / or `UMAP` on the engineered data.  \n",
    "   Make sure to use the appropriate options to utilize the metric.  \n",
    "   Apply this on the whole data.\n",
    "2. Apply Classification  \n",
    "   Apply a classifier (From SciKit Learn without Neural Based) on the low dimensional data.\n",
    "3. Measure Performance of the Classifier \n",
    "   Apply leave one out cross validation with score based on accuracy.  \n",
    "   You should use [`cross_val_predict()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html).  \n",
    "   This will generate a vector of 287 predictions.\n",
    "4. Display Results  \n",
    "   Display the confusion matrix and the total accuracy.\n",
    "\n",
    "**This is a competition**: \n",
    " \n",
    " * The group with the 1st highest score will get 4 bonus points.\n",
    " * The group with the 2nd highest score will get 2 bonus points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================Fill This===========================#\n",
    "# 1. Use t-SNE and/or UMAP for Dimensionality Reduction.\n",
    "# 2. On the output of (1) apply a classifier from SciKit Learn.\n",
    "# 3. To measure the performance use `cross_val_predict()` with accuracy as score.\n",
    "# 4. Display the confusion matrix of the result of the `cross_val_predict()`.\n",
    "# !! Use the `SpdMetric()` for UMAP / t-SNE!\n",
    "# !! This is a competition, optimize the hyper parameters of each step for best accuracy.\n",
    "# !! No use of Neural Net based classifiers!\n",
    "# !! If you use t-SNE, make sure to make it non random (See `init` and `random_state`).\n",
    "# !! Pay attention to the run time limitation of the whole notebook.\n",
    "\n",
    "\n",
    "#===============================================================#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.3.1. Question\n",
    "\n",
    "In the above we had the whole data for the _dimensionality reduction_ step.  \n",
    "Assume we use a method without _out of sample extension_.  \n",
    "In the case above, how would you handle a new data point? Explain the pipeline."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.3.1. Solution\n",
    "\n",
    "<font color='red'>??? Fill the answer here ???</font>\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
