{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Fixel Algorithms](https://fixelalgorithms.co/images/CCExt.png)](https://fixelalgorithms.gitlab.io/)\n",
    "\n",
    "# Explainability - SHAP\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 0.1.000 | 29/10/2022 | Royi Avital | First version                                                      |\n",
    "|         |            |             |                                                                    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/IntroductionMachineLearningSystemEngineers/ExplainabilityShap.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T17:52:07.921383Z",
     "start_time": "2022-02-02T17:52:07.649130Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor #<! Similar ot XGBoost\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "\n",
    "# Misc\n",
    "import datetime\n",
    "import math\n",
    "import os\n",
    "from platform import python_version\n",
    "import random\n",
    "import warnings\n",
    "import yaml\n",
    "\n",
    "# Typing\n",
    "from typing import Tuple\n",
    "\n",
    "# Visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from bokeh.plotting import figure, show\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython\n",
    "from IPython.display import Image, display\n",
    "from ipywidgets import Dropdown, FloatSlider, interact, IntSlider, Layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "%matplotlib inline\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "# sns.set_theme() #>! Apply SeaBorn theme\n",
    "sns.set_palette(\"tab10\")\n",
    "\n",
    "runInGoogleColab = 'google.colab' in str(get_ipython())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if runInGoogleColab:\n",
    "    !pip install git+https://github.com/8080labs/ppscore.git\n",
    "    !pip install --upgrade shap\n",
    "    !pip install --upgrade xgboost\n",
    "    !pip install --upgrade lightgbm\n",
    "\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "import ppscore as pps #<! See https://github.com/8080labs/ppscore -> pip install git+https://github.com/8080labs/ppscore.git\n",
    "import shap\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "FIG_SIZE_DEF    = (8, 8)\n",
    "ELM_SIZE_DEF    = 50\n",
    "CLASS_COLOR     = ('b', 'r')\n",
    "EDGE_COLOR      = 'k'\n",
    "MARKER_SIZE_DEF = 10\n",
    "LINE_WIDTH_DEF  = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixel Algorithms Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "numSplits = 5\n",
    "numShapSamples = 100\n",
    "\n",
    "# Data\n",
    "csvFilePath = r'../DataSets/winequality-red.csv'\n",
    "csvFileUrl  = r'https://raw.githubusercontent.com/FixelAlgorithmsTeam/FixelCourses/master/DataSets/winequality-red.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary Functions\n",
    "\n",
    "# From https://stackoverflow.com/questions/36728287\n",
    "def PolynomialFeaturesLabels(input_feature_names, power, include_bias: bool = True):\n",
    "    '''Basically this is a cover for the sklearn preprocessing function. \n",
    "    The problem with that function is if you give it a labeled dataframe, it ouputs an unlabeled dataframe with potentially\n",
    "    a whole bunch of unlabeled columns. \n",
    "\n",
    "    Inputs:\n",
    "    input_df = Your labeled pandas dataframe (list of x's not raised to any power) \n",
    "    power = what order polynomial you want variables up to. (use the same power as you want entered into pp.PolynomialFeatures(power) directly)\n",
    "\n",
    "    Ouput:\n",
    "    Output: This function relies on the powers_ matrix which is one of the preprocessing function's outputs to create logical labels and \n",
    "    outputs a labeled pandas dataframe   \n",
    "    '''\n",
    "    poly = PolynomialFeatures(power)\n",
    "    poly.fit(np.random.rand(1, len(input_feature_names)))\n",
    "    powers_nparray = poly.powers_\n",
    "\n",
    "    target_feature_names = []\n",
    "    if include_bias:\n",
    "        target_feature_names.append(\"Constant Term\")\n",
    "    for feature_distillation in powers_nparray[1:]:\n",
    "        intermediary_label = \"\"\n",
    "        final_label = \"\"\n",
    "        for i in range(len(input_feature_names)):\n",
    "            if feature_distillation[i] == 0:\n",
    "                continue\n",
    "            else:\n",
    "                variable = input_feature_names[i]\n",
    "                power = feature_distillation[i]\n",
    "                intermediary_label = \"%s^%d\" % (variable,power)\n",
    "                if final_label == \"\":         #If the final label isn't yet specified\n",
    "                    final_label = intermediary_label\n",
    "                else:\n",
    "                    final_label = final_label + \" x \" + intermediary_label\n",
    "        target_feature_names.append(final_label)\n",
    "    return target_feature_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case I - Linear Regression\n",
    "\n",
    "$$ y = 7 + {x}_{1} - {2x}_{2} + {3x}_{3} - {3x}_{4} + {5.5x}_{5} + \\epsilon $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T17:53:07.834772Z",
     "start_time": "2022-02-02T17:53:07.448832Z"
    }
   },
   "outputs": [],
   "source": [
    "numSamples  = 1000\n",
    "modelOrder  = 5\n",
    "noiseStd    = 0.1\n",
    "mX = 2 * (np.random.rand(numSamples, modelOrder) - 0.5) #<! Zero mean data\n",
    "\n",
    "vY = 7 + 1 * mX[:, 0] - 2 * mX[:, 1] + 3 * mX[:, 2] - 3 * mX[:, 3] + 5.5 * mX[:, 4] + (noiseStd * np.random.randn(numSamples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regressor\n",
    "oLS = LinearRegression().fit(mX, vY)\n",
    "modelScore = oLS.score(mX, vY)\n",
    "\n",
    "print(f'Model Score (Training): {modelScore}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP Analysis\n",
    "\n",
    "Since the model is linear and the SHAP method build an additive model we assume the result will be similar to the linear coefficients.\n",
    "\n",
    "The SHAP usually is calculated on sub sample of the data or a clustered version of teh data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buidling SHAP model without explicitly saying the model is linear\n",
    "# oSHAP = shap.KernelExplainer(oLS.predict, mX) #<! All data, slowest, yet most accurate\n",
    "# oSHAP = shap.KernelExplainer(oLS.predict, shap.kmeans(mX, 50)) #<! Clustering for smaller representation\n",
    "oSHAP = shap.KernelExplainer(oLS.predict, shap.sample(mX, 100)) #<! Sub Sampling for a random choice from data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape Values\n",
    "\n",
    "We'll analyze the SHAP values for the sample (Local Interpretability):\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x}^{\\star} = \\begin{bmatrix}1\\\\\n",
    "1\\\\\n",
    "5\\\\\n",
    "1\\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the Sample\n",
    "vX = np.array([1, 1, 5, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The prediction of the model for this sample:\n",
    "print(f'The model prediction for `vX`: {oLS.predict(vX[:, np.newaxis].T)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Shapley values for vX:\n",
    "vShapleValues = oSHAP.shap_values(vX)\n",
    "\n",
    "# Display values\n",
    "for ii in range(len(vX) + 1):\n",
    "    if ii == 0:\n",
    "        φ = oSHAP.expected_value\n",
    "    else:\n",
    "        φ = vShapleValues[ii - 1]\n",
    "    print(f'φ_{ii} = {φ: 5.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the values compared to the linear coefficiants.  \n",
    "Think of the SHAP values as something that gives you the shift from the expected value, either to increase or deacrese the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(oSHAP.expected_value, vShapleValues, feature_names = ['x_1', 'x_2', 'x_3', 'x_4', 'x_5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> One the fundemental properties of Shapley values is that they always sum up to the difference between the game outcome when all players are present and the game outcome when no players are present. For machine learning models this means that SHAP values of all the input features will always sum up to the difference between baseline (expected) model output and the current model output for the prediction being explained. The easiest way to see this is through a waterfall plot that starts our background prior expectation for a home price $\\mathbb{E} \\left[ f \\left( \\boldsymbol{x} \\right) \\right]$, and then adds features one at a time until we reach the current model output $f \\left( x \\right)$:\n",
    "\n",
    "**Remark**: Read the waterfall plot from bottom up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The wate\n",
    "shap.waterfall_plot(shap.Explanation(vShapleValues, oSHAP.expected_value, data = vX, feature_names = ['x_1', 'x_2', 'x_3', 'x_4', 'x_5']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case II - Ensemble Tree Model (Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate / Load Data \n",
    "\n",
    "if os.path.isfile(csvFilePath):\n",
    "    dfData = pd.read_csv(csvFilePath)\n",
    "else:\n",
    "    dfData = pd.read_csv(csvFileUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsY = dfData['quality']\n",
    "dfX = dfData[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']]\n",
    "dfX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfX.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfX.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hF, hA = plt.subplots(figsize = FIG_SIZE_DEF)\n",
    "sns.histplot(x = dsY, stat = 'count', discrete = True, ax = hA)\n",
    "hA.set_title('Wine Quality Histogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pipeline steps\n",
    "pBoostTress = Pipeline(steps = [('Scaler', StandardScaler()), ('PolyFeats', PolynomialFeatures(include_bias = False)), ('EnsTreesRegressor', GradientBoostingRegressor())])\n",
    "# pBoostTress = Pipeline(steps = [('Scaler', StandardScaler()), ('PolyFeats', PolynomialFeatures(include_bias = False)), ('EnsTreesRegressor', XGBRegressor())])\n",
    "# pBoostTress = Pipeline(steps = [('Scaler', StandardScaler()), ('PolyFeats', PolynomialFeatures(include_bias = False)), ('EnsTreesRegressor', LGBMRegressor())]) #<! Fastest\n",
    "# pBoostTress = Pipeline(steps = [('Scaler', StandardScaler()), ('PolyFeats', PolynomialFeatures(include_bias = False)), ('EnsTreesRegressor', RandomForestRegressor())])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid of parameters\n",
    "# Pay attention that the computational complexity is exponential!\n",
    "\n",
    "# For GradientBoostingRegressor\n",
    "dParamsGrid = {\n",
    "    'PolyFeats__degree': ['passthrough', 2, 3],\n",
    "    'EnsTreesRegressor__loss': ['squared_error', 'absolute_error'],\n",
    "    'EnsTreesRegressor__n_estimators': [50, 100, 150],\n",
    "    'EnsTreesRegressor__max_depth': [3, 5, 7],\n",
    "}\n",
    "\n",
    "# For XGBRegressor / LGBMRegressor\n",
    "# dParamsGrid = {\n",
    "#     'PolyFeats__degree': ['passthrough', 2, 3],\n",
    "#     'EnsTreesRegressor__n_estimators': [50, 100, 150],\n",
    "#     'EnsTreesRegressor__max_depth': [3, 5, 7],\n",
    "# }\n",
    "\n",
    "# For RandomForestRegressor\n",
    "# dParamsGrid = {\n",
    "#     'PolyFeats__degree': [passthrough, 2, 3],\n",
    "#     'EnsTreesRegressor__criterion': ['squared_error', 'absolute_error'],\n",
    "#     'EnsTreesRegressor__n_estimators': [50, 100],\n",
    "#     'EnsTreesRegressor__max_depth': [3, 5],\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data for Optimization of Parameters\n",
    "\n",
    "iDataBatch = StratifiedKFold(n_splits = numSplits, shuffle = True, random_state = seedNum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Grid Search (In pracice for many variables there are much better approachs: Random, Bayesian, etc...)\n",
    "oGridSearch = GridSearchCV(pBoostTress, dParamsGrid, n_jobs = -1, cv = iDataBatch.split(dfX, dsY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otpimization of Hyper Parameters\n",
    "oGridSearch.fit(dfX, dsY)\n",
    "print(f'Best parameter (CV Score = {oGridSearch.best_score_}')\n",
    "print(oGridSearch.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimalEst = oGridSearch.best_estimator_ #<! Basically the whole pipeline\n",
    "optimalEst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Prediction Results\n",
    "vYEst = optimalEst.predict(dfX)\n",
    "\n",
    "plt.plot(dsY, vYEst, '.r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Data by the Pipeline\n",
    "# See https://stackoverflow.com/questions/62180278\n",
    "dfXProcessed = optimalEst[:-1].transform(dfX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Estimator\n",
    "ensTreeEst = optimalEst[-1]\n",
    "ensTreeEst.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain Results by SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oSHAP = shap.KernelExplainer(optimalEst.predict, shap.sample(dfX, numShapSamples)) #<! Slow, match any model\n",
    "oSHAP = shap.TreeExplainer(ensTreeEst) #<! Optimized for trees, vey fast!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vShapleValues = oSHAP.shap_values(dfXProcessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polyDeg = oGridSearch.best_params_['PolyFeats__degree']\n",
    "lFeaturesName = PolynomialFeaturesLabels(dfX.columns.to_list(), polyDeg, include_bias = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variable Importance Plot — Global Interpretability**\n",
    "\n",
    "A variable importance plot lists the most significant variables in descending order. The top variables contribute more to the model than the bottom ones and thus have high predictive power. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(vShapleValues, dfXProcessed, plot_type = \"bar\", feature_names = lFeaturesName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variable Importance Plot — Global Interpretability**\n",
    "\n",
    " * Feature importance: Variables are ranked in descending order.\n",
    " * Impact: The horizontal location shows whether the effect of that value is associated with a higher or lower prediction.\n",
    " * Original value: Color shows whether that variable is high (in red) or low (in blue) for that observation.\n",
    " * Correlation: A high level of the “alcohol” content has a high and positive impact on the quality rating. The “high” comes from the red color, and the “positive” impact is shown on the X-axis. Similarly, we will say the “volatile acidity” is negatively correlated with the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(vShapleValues, dfXProcessed, feature_names = lFeaturesName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SHAP Dependence Plot — Global Interpretability**\n",
    "\n",
    "A partial dependence plot shows the marginal effect of one or two features on the predicted outcome of a machine learning model.  \n",
    "It tells whether the relationship between the target and a feature is linear, monotonic or more complex.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(lFeaturesName[0], vShapleValues, dfXProcessed, feature_names = lFeaturesName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Individual SHAP Value Plot — Local Interpretability**\n",
    "\n",
    "The explainability for any individual observation is the most critical step to convince your audience to adopt your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleIdx = 5\n",
    "\n",
    "\n",
    "shap.force_plot(oSHAP.expected_value, vShapleValues[sampleIdx], feature_names = lFeaturesName)\n",
    "\n",
    "# Why does the alcohol drives this to the left? Think about the mean value...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of SHAP\n",
    "\n",
    "It is helpful to remember the following points:\n",
    " * Each feature has a shap value contributing to the prediction.\n",
    " * The final prediction = the average prediction + the shap values of all features.\n",
    " * The shap value of a feature can be positive or negative.\n",
    " * If a feature is positively correlated to the target, a value higher than its own average will contribute positively to the prediction.\n",
    " * If a feature is negatively correlated to the target, a value higher than its own average will contribute negatively to the prediction.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case III - Ensemble Tree Model (Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pipeline steps\n",
    "# pBoostTress = Pipeline(steps = [('Scaler', StandardScaler()), ('PolyFeats', PolynomialFeatures(include_bias = False)), ('EnsTreesClassifier', GradientBoostingClassifier())])\n",
    "# pBoostTress = Pipeline(steps = [('Scaler', StandardScaler()), ('PolyFeats', PolynomialFeatures(include_bias = False)), ('EnsTreesClassifier', XGBClassifier())])\n",
    "pBoostTress = Pipeline(steps = [('Scaler', StandardScaler()), ('PolyFeats', PolynomialFeatures(include_bias = False)), ('EnsTreesClassifier', LGBMClassifier())]) #<! Fastest\n",
    "# pBoostTress = Pipeline(steps = [('Scaler', StandardScaler()), ('PolyFeats', PolynomialFeatures(include_bias = False)), ('EnsTreesClassifier', RandomForestClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid of parameters\n",
    "# Pay attention that the computational complexity is exponential!\n",
    "\n",
    "# For GradientBoostingRegressor\n",
    "# dParamsGrid = {\n",
    "#     'PolyFeats__degree': ['passthrough', 2, 3],\n",
    "#     'EnsTreesClassifier__loss': ['log_loss', 'deviance'],\n",
    "#     'EnsTreesClassifier__n_estimators': [50, 100, 150],\n",
    "#     'EnsTreesClassifier__max_depth': [3, 5, 7],\n",
    "# }\n",
    "\n",
    "# For LGBMClassifier\n",
    "dParamsGrid = {\n",
    "    'PolyFeats__degree': ['passthrough', 2, 3],\n",
    "    'EnsTreesClassifier__n_estimators': [50, 100, 150],\n",
    "    'EnsTreesClassifier__max_depth': [3, 5, 7],\n",
    "}\n",
    "\n",
    "# For RandomForestRegressor\n",
    "# dParamsGrid = {\n",
    "#     'PolyFeats__degree': [passthrough, 2, 3],\n",
    "#     'EnsTreesClassifier__criterion': ['squared_error', 'absolute_error'],\n",
    "#     'EnsTreesClassifier__n_estimators': [50, 100],\n",
    "#     'EnsTreesClassifier__max_depth': [3, 5],\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Grid Search (In pracice for many variables there are much better approachs: Random, Bayesian, etc...)\n",
    "oGridSearch = GridSearchCV(pBoostTress, dParamsGrid, n_jobs = -1, cv = iDataBatch.split(dfX, dsY), scoring = 'r2')\n",
    "# oGridSearch = GridSearchCV(pBoostTress, dParamsGrid, n_jobs = -1, cv = iDataBatch.split(dfX, dsY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otpimization of Hyper Parameters\n",
    "oGridSearch.fit(dfX, dsY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Best parameter (CV Score = {oGridSearch.best_score_}')\n",
    "print(oGridSearch.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimalEst = oGridSearch.best_estimator_ #<! Basically the whole pipeline\n",
    "optimalEst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Prediction Results\n",
    "vYEst = optimalEst.predict(dfX)\n",
    "\n",
    "confMatrix = confusion_matrix(dsY, vYEst, labels = optimalEst[-1].classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix = confMatrix, display_labels = optimalEst[-1].classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dsY, vYEst, '.r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Data by the Pipeline\n",
    "# See https://stackoverflow.com/questions/62180278\n",
    "dfXProcessed = optimalEst[:-1].transform(dfX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Estimator\n",
    "ensTreeEst = optimalEst[-1]\n",
    "ensTreeEst.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oSHAP = shap.KernelExplainer(optimalEst.predict, shap.sample(dfX, numShapSamples)) #<! Slow, match any model\n",
    "oSHAP = shap.TreeExplainer(ensTreeEst) #<! Optimized for trees, vey fast! (Currently SHAP doesn't support this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vShapleValues = oSHAP.shap_values(dfXProcessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polyDeg = oGridSearch.best_params_['PolyFeats__degree']\n",
    "lFeaturesName = PolynomialFeaturesLabels(dfX.columns.to_list(), polyDeg, include_bias = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(vShapleValues, dfXProcessed, plot_type = \"bar\", feature_names = lFeaturesName, class_names = optimalEst[-1].classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**?**: How can we improve the regressor in this case?  \n",
    "**!**: Think about the output values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "b091ae1a5f61fa4269f1f2c4a075dfd3ba6d6b741f8802b3932e01e064097caa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
