{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Fixel Algorithms](https://fixelalgorithms.co/images/CCExt.png)](https://fixelalgorithms.gitlab.io)\n",
    "\n",
    "# AI for System Engineers and Project Managers\n",
    "\n",
    "## Deep Learning - NLP - Text Classification\n",
    "\n",
    "Applying _Text Classification_ (Sentiment Analysis) using a Hugging Face model.\n",
    "\n",
    "<!-- https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_multiclass_classification.ipynb -->\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 1.0.000 | 08/03/2025 | Royi Avital | First version                                                      |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/AIProgram/2024_02/0037FeaturesTransform.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T09:30:06.492269Z",
     "start_time": "2022-02-02T09:30:06.220934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Deep Learning\n",
    "import clip\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import LRScheduler\n",
    "import torch.nn as nn\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torchmetrics.classification import MulticlassF1Score, MulticlassAccuracy\n",
    "\n",
    "import transformers\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "\n",
    "# Image Processing\n",
    "\n",
    "# Miscellaneous\n",
    "from enum import auto, Enum, unique\n",
    "import os\n",
    "import onedrivedownloader #<! https://github.com/loribonna/onedrivedownloader\n",
    "from platform import python_version\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Typing\n",
    "from typing import Callable, Dict, List, Optional, Self, Set, Tuple, Union\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought.\n",
    "\n",
    "Code Notations:\n",
    "\n",
    "```python\n",
    "someVar    = 2; #<! Notation for a variable\n",
    "vVector    = np.random.rand(4) #<! Notation for 1D array\n",
    "mMatrix    = np.random.rand(4, 3) #<! Notation for 2D array\n",
    "tTensor    = np.random.rand(4, 3, 2, 3) #<! Notation for nD array (Tensor)\n",
    "tuTuple    = (1, 2, 3) #<! Notation for a tuple\n",
    "lList      = [1, 2, 3] #<! Notation for a list\n",
    "dDict      = {1: 3, 2: 2, 3: 1} #<! Notation for a dictionary\n",
    "oObj       = MyClass() #<! Notation for an object\n",
    "dfData     = pd.DataFrame() #<! Notation for a data frame\n",
    "dsData     = pd.Series() #<! Notation for a series\n",
    "hObj       = plt.Axes() #<! Notation for an object / handler / function handler\n",
    "```\n",
    "\n",
    "### Code Exercise\n",
    "\n",
    " - Single line fill\n",
    "\n",
    " ```python\n",
    " valToFill = ???\n",
    " ```\n",
    "\n",
    " - Multi Line to Fill (At least one)\n",
    "\n",
    "```python\n",
    "# You need to start writing\n",
    "?????\n",
    "```\n",
    "\n",
    " - Section to Fill\n",
    "\n",
    "```python\n",
    "#===========================Fill This===========================#\n",
    "# 1. Explanation about what to do.\n",
    "# !! Remarks to follow / take under consideration.\n",
    "mX = ???\n",
    "\n",
    "?????\n",
    "#===============================================================#\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# %matplotlib inline\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "# Matplotlib default color palette\n",
    "lMatPltLibclr = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "# sns.set_theme() #>! Apply SeaBorn theme\n",
    "\n",
    "runInGoogleColab = 'google.colab' in str(get_ipython())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "FIG_SIZE_DEF    = (8, 8)\n",
    "ELM_SIZE_DEF    = 50\n",
    "CLASS_COLOR     = ('b', 'r')\n",
    "EDGE_COLOR      = 'k'\n",
    "MARKER_SIZE_DEF = 10\n",
    "LINE_WIDTH_DEF  = 2\n",
    "\n",
    "PROJECT_NAME      = 'FixelCourses'\n",
    "DATA_FOLDER_PATH  = 'DataSets'\n",
    "MODEL_FOLDER_PATH = 'Models'\n",
    "\n",
    "BASE_FOLDER      = os.getcwd()[:len(os.getcwd()) - (os.getcwd()[::-1].lower().find(PROJECT_NAME.lower()[::-1]))]\n",
    "\n",
    "L_IMG_EXT = ['.png', '.jpeg', '.jpg']\n",
    "\n",
    "L_DATA_CLS = ['Business', 'Entertainment', 'Health', 'Science']\n",
    "D_DATA_CLS = {'Business': 0, 'Entertainment': 1, 'Health': 2, 'Science': 3}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courses Packages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Auxiliary Functions\n",
    "\n",
    "class TextClsDataset(Dataset):\n",
    "    def __init__( self, dfData: pd.DataFrame, oTokenizer: Callable, dTokenizeParams: Dict, *, textCol: str = 'TITLE', tgtCol: str = 'CATEGORY' ) -> None:\n",
    "        \"\"\"\n",
    "        Constructor for the Dataset class\n",
    "        \"\"\"\n",
    "\n",
    "        lId = []\n",
    "        lMask = []\n",
    "\n",
    "        lText = dfData[textCol].tolist()\n",
    "        for textStr in lText:\n",
    "            lTextStr = \" \".join(textStr.split())\n",
    "            dToken   = oTokenizer(lTextStr, None, **dTokenizeParams)\n",
    "            lId.append(dToken['input_ids'])\n",
    "            lMask.append(dToken['attention_mask'])\n",
    "\n",
    "        self._numSamples = len(dfData)\n",
    "        self._lId        = lId\n",
    "        self._lMask      = lMask\n",
    "        self._lTgt       = dfData[tgtCol].tolist()\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "\n",
    "        return torch.tensor(self._lId[idx], dtype = torch.long), torch.tensor(self._lMask[idx], dtype = torch.long), torch.tensor(self._lTgt[idx], dtype = torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return self._numSamples\n",
    "\n",
    "class DistillBERT( torch.nn.Module ):\n",
    "    def __init__(self, oModel: DistilBertModel, numCls: int) -> None:\n",
    "        super(DistillBERT, self).__init__()\n",
    "        self.oModel         = oModel\n",
    "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
    "        self.dropout        = torch.nn.Dropout(0.3)\n",
    "        self.classifier     = torch.nn.Linear(768, numCls)\n",
    "\n",
    "    def forward(self, tInId: torch.Tensor, tAtnMask: torch.Tensor) -> torch.Tensor:\n",
    "        output_1     = self.oModel(input_ids = tInId, attention_mask = tAtnMask)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler       = hidden_state[:, 0]\n",
    "        pooler       = self.pre_classifier(pooler)\n",
    "        pooler       = torch.nn.ReLU()(pooler)\n",
    "        pooler       = self.dropout(pooler)\n",
    "        output       = self.classifier(pooler)\n",
    "\n",
    "        return output\n",
    "\n",
    "def PlotLabelsHistogram( vY: np.ndarray, hA: Optional[plt.Axes] = None, lClass: Optional[List] = None, xLabelRot: Optional[int] = None ) -> plt.Axes:\n",
    "\n",
    "    if hA is None:\n",
    "        hF, hA = plt.subplots(figsize = (8, 6))\n",
    "\n",
    "    vLabels, vCounts = np.unique(vY, return_counts = True)\n",
    "\n",
    "    hA.bar(vLabels, vCounts, width = 0.9, align = 'center')\n",
    "    hA.set_title('Histogram of Classes / Labels')\n",
    "    hA.set_xlabel('Class')\n",
    "    hA.set_xticks(vLabels, [f'{labelVal}' for labelVal in vLabels])\n",
    "    hA.set_ylabel('Count')\n",
    "    if lClass is not None:\n",
    "        hA.set_xticklabels(lClass)\n",
    "\n",
    "    if xLabelRot is not None:\n",
    "        for xLabel in hA.get_xticklabels():\n",
    "            xLabel.set_rotation(xLabelRot)\n",
    "\n",
    "    return hA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@unique\n",
    "class NNMode(Enum):\n",
    "    TRAIN     = auto()\n",
    "    INFERENCE = auto()\n",
    "\n",
    "def RunEpoch( oModel: nn.Module, dlData: DataLoader, hL: Callable, hS: Callable, oOpt: Optimizer = None, opMode: NNMode = NNMode.TRAIN ) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Runs a single Epoch (Train / Test) of a model.\n",
    "    Input:\n",
    "        oModel      - PyTorch `nn.Module` object.\n",
    "        dlData      - PyTorch `Dataloader` object.\n",
    "        hL          - Callable for the Loss function.\n",
    "        hS          - Callable for the Score function.\n",
    "        oOpt        - PyTorch `Optimizer` object.\n",
    "        opMode      - An `NNMode` to set the mode of operation.\n",
    "    Output:\n",
    "        valLoss     - Scalar of the loss.\n",
    "        valScore    - Scalar of the score.\n",
    "    Remarks:\n",
    "      - The `oDataSet` object returns a Tuple of (mX, vY) per batch.\n",
    "      - The `hL` function should accept the `vY` (Reference target) and `mZ` (Output of the NN).\n",
    "        It should return a Tuple of `valLoss` (Scalar of the loss) and `mDz` (Gradient by the loss).\n",
    "      - The `hS` function should accept the `vY` (Reference target) and `mZ` (Output of the NN).\n",
    "        It should return a scalar `valScore` of the score.\n",
    "      - The optimizer is required for training mode.\n",
    "    \"\"\"\n",
    "\n",
    "    epochLoss   = 0.0\n",
    "    epochScore  = 0.0\n",
    "    numSamples  = 0\n",
    "    numBatches = len(dlData)\n",
    "\n",
    "    runDevice = next(oModel.parameters()).device #<! CPU \\ GPU\n",
    "\n",
    "    if opMode == NNMode.TRAIN:\n",
    "        oModel.train(True) #<! Equivalent of `oModel.train()`\n",
    "    elif opMode == NNMode.INFERENCE:\n",
    "        oModel.eval() #<! Equivalent of `oModel.train(False)`\n",
    "    else:\n",
    "        raise ValueError(f'The `opMode` value {opMode} is not supported!')\n",
    "\n",
    "    for ii, (mX, mA, vY) in enumerate(dlData):\n",
    "        # Move Data to Model's device\n",
    "        mX = mX.to(runDevice) #<! Lazy\n",
    "        mA = mA.to(runDevice) #<! Lazy\n",
    "        vY = vY.to(runDevice) #<! Lazy\n",
    "\n",
    "        batchSize = mX.shape[0]\n",
    "\n",
    "        if opMode == NNMode.TRAIN:\n",
    "            # Forward\n",
    "            mZ      = oModel(mX, mA) #<! Model output\n",
    "            valLoss = hL(mZ, vY)     #<! Loss\n",
    "\n",
    "            # Backward\n",
    "            oOpt.zero_grad()    #<! Set gradients to zeros\n",
    "            valLoss.backward()  #<! Backward\n",
    "            oOpt.step()         #<! Update parameters\n",
    "            # oModel.eval()       #<! Inference mode for layers\n",
    "        else: #<! Value of `opMode` was already validated\n",
    "            with torch.no_grad():\n",
    "                # No computational graph\n",
    "                mZ      = oModel(mX, mA) #<! Model output\n",
    "                valLoss = hL(mZ, vY)     #<! Loss\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Score\n",
    "            valScore = hS(mZ, vY)\n",
    "            # Normalize so each sample has the same weight\n",
    "            epochLoss  += batchSize * valLoss.item()\n",
    "            epochScore += batchSize * valScore.item()\n",
    "            numSamples += batchSize\n",
    "\n",
    "        print(f'\\r{\"Train\" if opMode == NNMode.TRAIN else \"Val\"} - Iteration: {(ii + 1):3d} / {numBatches}, loss: {valLoss:.6f}', end = '')\n",
    "\n",
    "    print('', end = '\\r')\n",
    "\n",
    "    return epochLoss / numSamples, epochScore / numSamples\n",
    "\n",
    "def TrainModel( oModel: nn.Module, dlTrain: DataLoader, dlVal: DataLoader, oOpt: Optimizer, numEpoch: int, hL: Callable, hS: Callable, *, oSch: Optional[LRScheduler] = None, oTBWriter: Optional[SummaryWriter] = None) -> Tuple[nn.Module, List, List, List, List]:\n",
    "    \"\"\"\n",
    "    Trains a model given test and validation data loaders.\n",
    "    Input:\n",
    "        oModel      - PyTorch `nn.Module` object.\n",
    "        dlTrain     - PyTorch `Dataloader` object (Training).\n",
    "        dlVal       - PyTorch `Dataloader` object (Validation).\n",
    "        oOpt        - PyTorch `Optimizer` object.\n",
    "        numEpoch    - Number of epochs to run.\n",
    "        hL          - Callable for the Loss function.\n",
    "        hS          - Callable for the Score function.\n",
    "        oSch        - PyTorch `Scheduler` (`LRScheduler`) object.\n",
    "        oTBWriter   - PyTorch `SummaryWriter` object (TensorBoard).\n",
    "    Output:\n",
    "        lTrainLoss     - Scalar of the loss.\n",
    "        lTrainScore    - Scalar of the score.\n",
    "        lValLoss    - Scalar of the score.\n",
    "        lValScore    - Scalar of the score.\n",
    "        lLearnRate    - Scalar of the score.\n",
    "    Remarks:\n",
    "      - The `oDataSet` object returns a Tuple of (mX, vY) per batch.\n",
    "      - The `hL` function should accept the `vY` (Reference target) and `mZ` (Output of the NN).\n",
    "        It should return a Tuple of `valLoss` (Scalar of the loss) and `mDz` (Gradient by the loss).\n",
    "      - The `hS` function should accept the `vY` (Reference target) and `mZ` (Output of the NN).\n",
    "        It should return a scalar `valScore` of the score.\n",
    "      - The optimizer is required for training mode.\n",
    "    \"\"\"\n",
    "\n",
    "    lTrainLoss  = []\n",
    "    lTrainScore = []\n",
    "    lValLoss    = []\n",
    "    lValScore   = []\n",
    "    lLearnRate  = []\n",
    "\n",
    "    # Support R2\n",
    "    bestScore = -1e9 #<! Assuming higher is better\n",
    "\n",
    "    learnRate = oOpt.param_groups[0]['lr']\n",
    "\n",
    "    for ii in range(numEpoch):\n",
    "        startTime           = time.time()\n",
    "        trainLoss, trainScr = RunEpoch(oModel, dlTrain, hL, hS, oOpt, opMode = NNMode.TRAIN) #<! Train\n",
    "        valLoss,   valScr   = RunEpoch(oModel, dlVal, hL, hS, oOpt, opMode = NNMode.INFERENCE) #<! Score Validation\n",
    "        if oSch is not None:\n",
    "            # Adjusting the scheduler on Epoch level\n",
    "            learnRate = oSch.get_last_lr()[0]\n",
    "            oSch.step()\n",
    "        epochTime           = time.time() - startTime\n",
    "\n",
    "        # Aggregate Results\n",
    "        lTrainLoss.append(trainLoss)\n",
    "        lTrainScore.append(trainScr)\n",
    "        lValLoss.append(valLoss)\n",
    "        lValScore.append(valScr)\n",
    "        lLearnRate.append(learnRate)\n",
    "\n",
    "        if oTBWriter is not None:\n",
    "            oTBWriter.add_scalars('Loss (Epoch)', {'Train': trainLoss, 'Validation': valLoss}, ii)\n",
    "            oTBWriter.add_scalars('Score (Epoch)', {'Train': trainScr, 'Validation': valScr}, ii)\n",
    "            oTBWriter.add_scalar('Learning Rate', learnRate, ii)\n",
    "\n",
    "        # Display (Babysitting)\n",
    "        print('Epoch '              f'{(ii + 1):4d} / ' f'{numEpoch}', end = '')\n",
    "        print(' | Train Loss: '     f'{trainLoss          :6.3f}', end = '')\n",
    "        print(' | Val Loss: '       f'{valLoss            :6.3f}', end = '')\n",
    "        print(' | Train Score: '    f'{trainScr           :6.3f}', end = '')\n",
    "        print(' | Val Score: '      f'{valScr             :6.3f}', end = '')\n",
    "        print(' | Epoch Time: '     f'{epochTime          :5.2f}', end = '')\n",
    "\n",
    "        # Save best model (\"Early Stopping\")\n",
    "        if valScr > bestScore:\n",
    "            bestScore = valScr\n",
    "            try:\n",
    "                dCheckPoint = {'Model': oModel.state_dict(), 'Optimizer': oOpt.state_dict()}\n",
    "                if oSch is not None:\n",
    "                    dCheckPoint['Scheduler'] = oSch.state_dict()\n",
    "                torch.save(dCheckPoint, 'BestModel.pt')\n",
    "                print(' | <-- Checkpoint!', end = '')\n",
    "            except:\n",
    "                print(' | <-- Failed!', end = '')\n",
    "        print(' |')\n",
    "\n",
    "    # Load best model (\"Early Stopping\")\n",
    "    # dCheckPoint = torch.load('BestModel.pt')\n",
    "    # oModel.load_state_dict(dCheckPoint['Model'])\n",
    "\n",
    "    return oModel, lTrainLoss, lTrainScore, lValLoss, lValScore, lLearnRate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification\n",
    "\n",
    "Text classification predict the class given a text input.  \n",
    "The text is pre processed by a tokenizer.\n",
    "\n",
    "### Bert Model\n",
    "\n",
    "The _Bert_ (Bidirectional Encoder Representations from Transformers) model was one of the first models to incorporate the Transformers building block.  \n",
    "Its inception, at 2018, is notable for its dramatic improvement over previous state of the art models, and as an early example of a large language model.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/BERT_embeddings_01.png/799px-BERT_embeddings_01.png)\n",
    "\n",
    "The model is based on _Encoder Only_ transformer architecture.\n",
    "\n",
    "At a high level, BERT consists of 4 modules:\n",
    "\n",
    " - Tokenizer: This module converts a piece of English text into a sequence of integers (Tokens).\n",
    " - Embedding: This module converts the sequence of tokens into an array of real valued vectors representing the tokens.\n",
    " - Encoder: A stack of Transformer blocks with self attention.\n",
    " - Task Head: This module converts the final representation vectors into one hot encoded tokens again by producing a predicted probability distribution over the token types.  \n",
    "   It can be viewed as a simple decoder, decoding the latent representation into token types.\n",
    "\n",
    "Applications:\n",
    "\n",
    " - Text Classification.\n",
    " - Text Embedding.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> [A Primer in BERTology: What We Know About How BERT Works](https://arxiv.org/abs/2002.12327)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# Data\n",
    "datasetName = 'NewsAggregator'\n",
    "datasetUrl  = 'https://technionmail-my.sharepoint.com/:u:/g/personal/royia_technion_ac_il/ERVmszLTXR5IiSLYQt3qBYYBvFEE6nIcfZkjhms03KUREA?e=E3JKKR'\n",
    "datasetFile = 'NewsAggregator.csv'\n",
    "\n",
    "# Pre Processing\n",
    "\n",
    "# Training\n",
    "numSamplesTrain = 370_000\n",
    "numSamplesVal   = 52_419\n",
    "numSamplesTrain = 1024\n",
    "numSamplesVal   = 64\n",
    "batchSize       = 32\n",
    "numEpochs       = 30\n",
    "\n",
    "# Model\n",
    "modelName = 'DistilBERTModel'\n",
    "numCls = 4\n",
    "maxLen = 512\n",
    "dTokenParams = {\n",
    "    'add_special_tokens'    : True,\n",
    "    'max_length'            : maxLen,\n",
    "    'padding'               : 'max_length',\n",
    "    'return_token_type_ids' : True,\n",
    "    'truncation'            : True,\n",
    "}\n",
    "\n",
    "# Data Visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate / Load Data\n",
    "\n",
    "Data is based on the [UC Irvine Machine Learning Repository - News Aggregator](https://www.kaggle.com/datasets/adityajn105/flickr8k).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Data is Available\n",
    "\n",
    "dataSetPath = os.path.join(BASE_FOLDER, DATA_FOLDER_PATH, datasetName)\n",
    "\n",
    "if not os.path.isdir(dataSetPath):\n",
    "    # Download, unzip and remove ZIP file\n",
    "    onedrivedownloader.download(datasetUrl, os.path.join(BASE_FOLDER, DATA_FOLDER_PATH, datasetName + '.zip'), unzip = True, clean = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Set\n",
    "\n",
    "# dfRawData = pd.read_csv(os.path.join(dataSetPath, datasetFile), sep = '\\t', names = ['ID','TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP'])\n",
    "dfRawData = pd.read_csv('https://github.com/darcien/ML/raw/refs/heads/master/text-classification-attempt/data/newsCorpora.csv', sep = '\\t', names = ['ID','TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP'])\n",
    "dfRawData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Should the `URL` data be kept for training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for Training\n",
    "\n",
    "dfData     = dfRawData[['TITLE', 'CATEGORY']].copy()\n",
    "dfData['CATEGORY'] = dfData['CATEGORY'].map({'b': 'Business', 'e': 'Entertainment', 'm': 'Health', 't': 'Science'})\n",
    "numSamples = len(dfData)\n",
    "\n",
    "print(f'The number of training samples: {numSamples}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for Training\n",
    "dfData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Data\n",
    "\n",
    "dfData['CATEGORY'] = dfData['CATEGORY'].map(D_DATA_CLS)\n",
    "\n",
    "hA = PlotLabelsHistogram(dfData['CATEGORY'].values, lClass = L_DATA_CLS, xLabelRot = 45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Is the data balanced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapped Data\n",
    "dfData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "\n",
    "Loading the CLIP model by OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU Availability\n",
    "\n",
    "runDevice   = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') #<! The 1st CUDA device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "\n",
    "oTokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased', cache_dir = os.path.join(BASE_FOLDER, MODEL_FOLDER_PATH, modelName))\n",
    "oModel     = DistilBertModel.from_pretrained('distilbert-base-uncased', cache_dir = os.path.join(BASE_FOLDER, MODEL_FOLDER_PATH, modelName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DistillBERT Model\n",
    "\n",
    "oModelBERT = DistillBERT(oModel, numCls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Validation Split\n",
    "dfTrain, dfVal = train_test_split(dfData, train_size = numSamplesTrain, test_size = numSamplesVal, stratify = dfData['CATEGORY'], random_state = seedNum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for Training\n",
    "\n",
    "dsTrain = TextClsDataset(dfTrain, oTokenizer, dTokenParams)\n",
    "dsVal   = TextClsDataset(dfVal, oTokenizer, dTokenParams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loaders\n",
    "dlTrain = torch.utils.data.DataLoader(dsTrain, shuffle = True, batch_size = batchSize, drop_last = True)\n",
    "dlVal   = torch.utils.data.DataLoader(dsVal, shuffle = True, batch_size = 2 * batchSize, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate on the Loader\n",
    "# The first batch.\n",
    "tX, tA, tY = next(iter(dlTrain)) #<! PyTorch Tensors\n",
    "\n",
    "print(f'The batch features dimensions: {tX.shape}')\n",
    "print(f'The batch masks dimensions: {tA.shape}')\n",
    "print(f'The batch labels dimensions: {tY.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Functions\n",
    "hL = nn.CrossEntropyLoss()\n",
    "# hS = MulticlassF1Score(num_classes = numCls, average = 'macro')\n",
    "hS = MulticlassAccuracy(num_classes = numCls, average = 'macro')\n",
    "\n",
    "hL = hL.to(runDevice)\n",
    "hS = hS.to(runDevice)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "\n",
    "oModelBERT = oModelBERT.to(runDevice)\n",
    "oOpt = torch.optim.AdamW(oModelBERT.parameters(), lr = 1e-5, betas = (0.9, 0.99), weight_decay = 1e-5) #<! Define optimizer\n",
    "oSch = torch.optim.lr_scheduler.OneCycleLR(oOpt, max_lr = 1e-4, total_steps = numEpochs)\n",
    "oModelBERT, lTrainLoss, lTrainScore, lValLoss, lValScore, lLearnRate = TrainModel(oModelBERT, dlTrain, dlVal, oOpt, numEpochs, hL, hS, oSch = oSch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "# Does not work, Find the bug :-)\n",
    "\n",
    "# numBatchesTrain = len(dlTrain)\n",
    "# numBatchesVal   = len(dlVal)\n",
    "\n",
    "# for epochIdx in range(numEpochs):\n",
    "#     startTime = time.time()\n",
    "\n",
    "#     epochLoss       = 0\n",
    "#     epochScore      = 0\n",
    "#     numSamplesEpoch = 0\n",
    "\n",
    "#     oModelBERT.train()\n",
    "\n",
    "#     # Training\n",
    "#     for ii, (tX, tA, tY) in enumerate(dlTrain):\n",
    "#         itrBatchSize = tX.shape[0]\n",
    "\n",
    "\n",
    "#         tX, tA, tY = tX.to(runDevice), tA.to(runDevice), tY.to(runDevice)\n",
    "#         tYHat = oModelBERT(tX, tA)\n",
    "\n",
    "#         valLoss = hL(tYHat, tY)\n",
    "#         oOpt.zero_grad()\n",
    "#         valLoss.backward() #<! Backward Propagation\n",
    "#         oOpt.step()\n",
    "\n",
    "#         epochLoss += itrBatchSize * valLoss.item()\n",
    "\n",
    "#         with torch.inference_mode():\n",
    "#             valScore    = hS(tYHat, tY)\n",
    "#             epochScore += itrBatchSize * valScore.item()\n",
    "\n",
    "#             numSamplesEpoch += itrBatchSize\n",
    "\n",
    "#         print(f'\\rTraining - Iteration: {(ii + 1):3d} / {numBatchesTrain}, loss: {valLoss:.6f}', end = '')\n",
    "\n",
    "#     print('', end = '\\r')\n",
    "\n",
    "#     trainLoss  = epochLoss / numSamplesEpoch\n",
    "#     trainScore = epochScore / numSamplesEpoch\n",
    "\n",
    "#     epochLoss       = 0\n",
    "#     epochScore      = 0\n",
    "#     numSamplesEpoch = 0\n",
    "\n",
    "#     oModelBERT.eval()\n",
    "\n",
    "#     # Validation\n",
    "#     for ii, (tX, tA, tY) in enumerate(dlVal):\n",
    "#         itrBatchSize = tX.shape[0]\n",
    "\n",
    "#         tX, tA, tY = tX.to(runDevice), tA.to(runDevice), tY.to(runDevice)\n",
    "\n",
    "#         with torch.inference_mode():\n",
    "#             tYHat    = oModelBERT(tX, tA)\n",
    "#             valLoss  = hL(tYHat, tY)\n",
    "#             valScore = hS(tYHat, tY)\n",
    "\n",
    "#         epochLoss       += itrBatchSize * valLoss.item()\n",
    "#         epochScore      += itrBatchSize * valScore.item()\n",
    "#         numSamplesEpoch += itrBatchSize\n",
    "\n",
    "#         print(f'\\rValidation - Iteration: {(ii + 1):3d} / {numBatchesVal}, loss: {valLoss:.6f}', end = '')\n",
    "\n",
    "#     print('', end = '\\r')\n",
    "\n",
    "#     valLoss  = epochLoss / numSamplesEpoch\n",
    "#     valScore = epochScore / numSamplesEpoch\n",
    "\n",
    "#     epochTime = time.time() - startTime\n",
    "#     print('Epoch '              f'{(epochIdx + 1):4d} / ' f'{numEpochs}', end = '')\n",
    "#     print(' | Train Loss: '     f'{trainLoss          :6.3f}', end = '')\n",
    "#     print(' | Val Loss: '       f'{valLoss            :6.3f}', end = '')\n",
    "#     print(' | Train Score: '    f'{trainScore         :6.3f}', end = '')\n",
    "#     print(' | Val Score: '      f'{valScore           :6.3f}', end = '')\n",
    "#     print(' | Epoch Time: '     f'{epochTime          :5.2f}', end = '')\n",
    "#     print(' |')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training Phase\n",
    "\n",
    "hF, vHa = plt.subplots(nrows = 1, ncols = 3, figsize = (12, 5))\n",
    "vHa = np.ravel(vHa)\n",
    "\n",
    "hA = vHa[0]\n",
    "hA.plot(lTrainLoss, lw = 2, label = 'Train')\n",
    "hA.plot(lValLoss, lw = 2, label = 'Validation')\n",
    "hA.set_title(f'Classification Loss')\n",
    "hA.set_xlabel('Epoch')\n",
    "hA.set_ylabel('Loss')\n",
    "hA.legend()\n",
    "\n",
    "hA = vHa[1]\n",
    "hA.plot(lTrainScore, lw = 2, label = 'Train')\n",
    "hA.plot(lValScore, lw = 2, label = 'Validation')\n",
    "hA.set_title('Classification Score')\n",
    "hA.set_xlabel('Epoch')\n",
    "hA.set_ylabel('Score')\n",
    "hA.legend()\n",
    "\n",
    "hA = vHa[2]\n",
    "hA.plot(lLearnRate, lw = 2)\n",
    "hA.set_title('Learn Rate Scheduler')\n",
    "hA.set_xlabel('Epoch')\n",
    "hA.set_ylabel('Learn Rate');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference Mode\n",
    "\n",
    "oModelBERT = oModelBERT.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on Training and Validation Data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
