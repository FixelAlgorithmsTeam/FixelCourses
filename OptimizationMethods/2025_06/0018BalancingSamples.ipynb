{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Fixel Algorithms](https://fixelalgorithms.co/images/CCExt.png)](https://fixelalgorithms.gitlab.io)\n",
    "\n",
    "# Optimization Methods\n",
    "\n",
    "## Convex Optimization - Constrained Optimization - Balancing Classes for Machine Learning Training \n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 1.2.001 | 08/08/2025 | Royi Avital | Removing all zeros rows, Added the \"Engineer Solution\"             |\n",
    "| 1.2.000 | 24/07/2025 | Royi Avital | Added the Mean Invariance formulation                              |\n",
    "| 1.1.000 | 23/07/2025 | Royi Avital | Using the KL Divergence formulation                                |\n",
    "| 1.0.000 | 17/07/2025 | Royi Avital | First version                                                      |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/AIProgram/2024_02/0002PointLine.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T17:52:07.921383Z",
     "start_time": "2022-02-02T17:52:07.649130Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Optimization\n",
    "import cvxpy as cp\n",
    "\n",
    "# Miscellaneous\n",
    "import math\n",
    "from platform import python_version\n",
    "import random\n",
    "import fractions\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython\n",
    "\n",
    "# Typing\n",
    "from typing import List, Optional, Tuple, Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought.\n",
    "\n",
    "Code Notations:\n",
    "\n",
    "```python\n",
    "someVar    = 2; #<! Notation for a variable\n",
    "vVector    = np.random.rand(4) #<! Notation for 1D array\n",
    "mMatrix    = np.random.rand(4, 3) #<! Notation for 2D array\n",
    "tTensor    = np.random.rand(4, 3, 2, 3) #<! Notation for nD array (Tensor)\n",
    "tuTuple    = (1, 2, 3) #<! Notation for a tuple\n",
    "lList      = [1, 2, 3] #<! Notation for a list\n",
    "dDict      = {1: 3, 2: 2, 3: 1} #<! Notation for a dictionary\n",
    "oObj       = MyClass() #<! Notation for an object\n",
    "dfData     = pd.DataFrame() #<! Notation for a data frame\n",
    "dsData     = pd.Series() #<! Notation for a series\n",
    "hObj       = plt.Axes() #<! Notation for an object / handler / function handler\n",
    "```\n",
    "\n",
    "### Code Exercise\n",
    "\n",
    " - Single line fill\n",
    "\n",
    "```python\n",
    "valToFill = ???\n",
    "```\n",
    "\n",
    " - Multi Line to Fill (At least one)\n",
    "\n",
    " ```python\n",
    " # You need to start writing\n",
    " ?????\n",
    " ```\n",
    "\n",
    " - Section to Fill\n",
    "\n",
    "```python\n",
    "#===========================Fill This===========================#\n",
    "# 1. Explanation about what to do.\n",
    "# !! Remarks to follow / take under consideration.\n",
    "mX = ???\n",
    "\n",
    "?????\n",
    "#===============================================================#\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "%matplotlib inline\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "# Matplotlib default color palette\n",
    "lMatPltLibclr = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "# sns.set_theme() #>! Apply SeaBorn theme\n",
    "# sns.set_palette(\"tab10\")\n",
    "\n",
    "runInGoogleColab = 'google.colab' in str(get_ipython())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "FIG_SIZE_DEF    = (8, 8)\n",
    "ELM_SIZE_DEF    = 50\n",
    "CLASS_COLOR     = ('b', 'r')\n",
    "EDGE_COLOR      = 'k'\n",
    "MARKER_SIZE_DEF = 10\n",
    "LINE_WIDTH_DEF  = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Course Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary Functions\n",
    "\n",
    "def DisplayClassBalance(vClassBalance: np.ndarray, *, figSize: Tuple[float, float] = (6.0, 6.0), hA: Optional[plt.Axes] = None) -> plt.Axes:\n",
    "\n",
    "    if hA is None:\n",
    "        hF, hA = plt.subplots(figsize = figSize)\n",
    "    \n",
    "    numClasses = len(vClassBalance)\n",
    "    \n",
    "    hA.bar(np.arange(numClasses), vClassBalance, width = 0.85)\n",
    "    hA.set_xlabel('Class Index')\n",
    "    hA.set_ylabel('Number of Samples')\n",
    "    hA.set_title('Class Balance')\n",
    "\n",
    "    return hA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Balancing in Machine Learning\n",
    "\n",
    "### Balanced and Imbalanced Data Set\n",
    "\n",
    "The _Classification_ task in _Machine Learning_ by default assumes balanced data.  \n",
    "Namely, the number of samples per _Class_ is similar among classes.\n",
    "\n",
    "<img src=\"https://i.imgur.com/PytqYsy.png\" width=\"750\"/>\n",
    "<!-- ![](https://i.imgur.com/PytqYsy.png) -->\n",
    "<!-- ![](https://i.postimg.cc/3RwJGjhv/68e7d3ca-1f1c-4ea4-a56d-ac0b0116306f.png) -->\n",
    "\n",
    "* <font color='brown'>(**#**)</font> While balanced data is the ideal, there are methods to handle imbalanced data in Machine Learning Classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing the Data Set by Weighted Oversampling  \n",
    "\n",
    "Let $\\boldsymbol{a}_{i} \\in \\mathbb{R}^{n}$ be the vector of class appearances in the data sample $i$.  \n",
    "The sample (For example an image in _Object Detection_ task) contains different numbers of examples per class.  \n",
    "This case, for $m$ samples and $n$ classes can be represented by a matrix $\\boldsymbol{A} \\in \\mathbb{R}^{m \\times n}$:\n",
    "\n",
    " * Each row is the number of class examples per sample.\n",
    " * Each column is the number of examples of a certain class per sample.\n",
    "\n",
    "Assuming one may _oversample_ each sample (Row), with a factor ${x}_{i}$ then the factoring can be formulated as:\n",
    "\n",
    "$$ \\boldsymbol{y} = \\boldsymbol{A}^{T} \\boldsymbol{x}, \\; \\boldsymbol{x} \\in \\mathbb{N}_{+}^{m} $$\n",
    "\n",
    "Namely, the value ${x}_{i} \\in \\mathbb{N}_{+}$ is the number of copies of the sample $i$ in the balanced data set.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> A better way the an actually oversample / replicate the sample is to increase its weight or increase the class weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept Formulation of the Problem\n",
    "\n",
    "Conceptually the problem is given by:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\arg \\min_{\\boldsymbol{x}} \\quad & R \\left( \\boldsymbol{y} \\right) \\\\\n",
    "\\text{subject to} \\quad & \\begin{aligned} \n",
    "\\boldsymbol{A}^{T} \\boldsymbol{x} & = \\boldsymbol{y} \\\\\n",
    "\\boldsymbol{x} & \\geq \\boldsymbol{1}\n",
    "\\end{aligned}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Where $R \\left( \\cdot \\right)$ is a function which penalizes high variance vectors / promotes near constant vectors.\n",
    "\n",
    "This notebook explores different formulations to solve the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measures of Vector Unevenness / Curvature / Non Uniformity\n",
    "\n",
    "In order force nearly constant vector one need to define $R \\left( \\cdot \\right)$ to measure the _Unevenness_ (_Curvature_ / _Roughness_) of the vector.  \n",
    "This section introduces several ideas (Limited to _Convex_ functions):\n",
    "\n",
    " - Variance - $\\frac{1}{n} \\sum_{i = 1}^{n} {\\left( {y}_{i} - \\frac{1}{n} \\sum_{j = 1}^{n} {y}_{j} \\right)}^{2}$  \n",
    "   Can be also formulated as $\\frac{1}{2 {n}^{2}} \\sum_{i, j = 1}^{n} {\\left( {y}_{i} - {y}_{j} \\right)}^{2}$.\n",
    " - Mean Absolute Deviation - $\\frac{1}{n} \\sum_{i = 1}^{n} \\left| {y}_{i} - \\frac{1}{n} \\sum_{j = 1}^{n} {y}_{j} \\right|$  \n",
    "   Can be formulated as a _Linear PRogramming_ problem.\n",
    " - Maximum Absolute Deviation - ${\\left\\| \\boldsymbol{y} - \\bar{y} \\boldsymbol{1} \\right\\|}_{\\infty}$  \n",
    "   Where $\\bar{y} = \\frac{1}{n} \\sum_{j = 1}^{n} {y}_{j}$.  \n",
    "   Can be formulated as a _Linear PRogramming_ problem.\n",
    " - Huber based Deviation - $\\frac{1}{n} \\sum_{i = 1}^{n} {\\delta}^{2} \\left( \\sqrt{ 1 + \\frac{ {\\left( {y}_{i} - \\bar{y} \\right)}^{2} }{{\\delta}^{2}} } - 1 \\right)$  \n",
    "   Outliers robust Variance (Combines _Variance_ and _Mean Absolute Deviation_).\n",
    " - Invariance to Filtration by Mean Filter - ${\\left\\| \\boldsymbol{C} \\boldsymbol{y} - \\boldsymbol{y} \\right\\|}_{2}^{2}$  \n",
    "   Where ${C}_{i, j} = \\frac{1}{n}$.  \n",
    "   The norm can be replaced with the ${L}_{1}$ or ${L}_{\\infty}$ norm.\n",
    " - Relative Entropy to Uniform Distribution - $\\operatorname{D}_{KL} \\left( \\boldsymbol{y}, \\boldsymbol{u} \\right) = \\sum_{i = 1}^{n} {y}_{i} \\log \\left( \\frac{ {y}_{i} }{ 1 / n } \\right) = \\log \\left( n \\right) + \\sum_{i = 1}^{n} {y}_{i} \\log \\left( {y}_{i} \\right)$  \n",
    "   Requires a discrete distribution. Namely only when we can enforce $\\boldsymbol{y} \\in \\mathcal{\\Delta}^{n}$.  \n",
    "   In order to use with larger numbers, one must \"Guess\" the uniform vector.  \n",
    "   See [Better Intuition for Information Theory](https://www.blackhc.net/blog/2019/better-intuition-for-information-theory), [Less Wrong - Six Intuitions for KL Divergence](https://www.lesswrong.com/posts/no5jDTut5Byjqb4j5).\n",
    "\n",
    "* <font color='brown'>(**#**)</font> Some formulations will require additional penalty on the sum of $\\boldsymbol{y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# Data\n",
    "dataFileUrl        = r'https://raw.githubusercontent.com/FixelAlgorithmsTeam/FixelCourses/refs/heads/master/DataSets/ClassBalancing.csv'\n",
    "numSamplesDiscrete = 10_000\n",
    "\n",
    "# Model\n",
    "upperbound = 1_000\n",
    "fracRes    = 100_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data\n",
    "\n",
    "This section loads a real world case (Credit to Michael Kaster) as a sample case for the algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T17:53:07.834772Z",
     "start_time": "2022-02-02T17:53:07.448832Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load Data of Real World Case\n",
    "\n",
    "mA = np.loadtxt(dataFileUrl, dtype = np.float64, delimiter = ',')\n",
    "\n",
    "# Remove All Zeros rows\n",
    "vM = np.any(mA, axis = 1) #<! Mask of rows with at least one non zero element\n",
    "mA = mA[vM, :]            #<! Filter rows\n",
    "\n",
    "print(f'The Data Shape            : {mA.shape}')\n",
    "print(f'The Data Number of Samples: {mA.shape[0]}')\n",
    "print(f'The Data Number of Classes: {mA.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the Class Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Class Balance\n",
    "\n",
    "numSamples = mA.shape[0]\n",
    "numClasses = mA.shape[1]\n",
    "\n",
    "hA = DisplayClassBalance(np.sum(mA, axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulation 001 - Discrete\n",
    "\n",
    "Since the values of ${x}_{i}$ stands for counting one could force the problem to be a _Discrete Optimization_ problem.  \n",
    "In order to make it feasible, one can limit itself to cases where the objective, defined by $R \\left( \\cdot \\right)$, can be formulated using _Integer Linear Programming_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 001\n",
    "\n",
    "Using the _Maximum Absolute Deviation_ the problem is given by:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\arg \\min_{\\boldsymbol{x} \\in \\mathbb{N}_{+}^{m}, \\mu} \\quad & {\\left\\| \\boldsymbol{A}^{T} \\boldsymbol{x} - \\mu \\boldsymbol{1} \\right\\|}_{\\infty} \\\\\n",
    "\\text{subject to} \\quad & \\begin{aligned} \n",
    "\\boldsymbol{x} & \\geq \\boldsymbol{1} \\\\\n",
    "\\boldsymbol{x} & \\leq u \\boldsymbol{1} \\\\\n",
    "\\frac{1}{m} \\boldsymbol{1}^{T} \\boldsymbol{A}^{T} \\boldsymbol{x} & = \\mu\n",
    "\\end{aligned}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This formulation also adds an upper bound to the number of copies ($u$).\n",
    "\n",
    "Formulate the problem as a _Integer Linear Programming_ (Linear objective, Linear constraints)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 001\n",
    "\n",
    "<font color='red'>??? Fill the answer here ???</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum Absolute Deviation - Original Formulation\n",
    "# Using the straight forward formulation, the problem can be solved using a convex optimization solver.\n",
    "\n",
    "def SolveDiscreteInfNormOrg( mA: np.ndarray, upBound: float ) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Solve the discrete infinity norm optimization problem.\n",
    "    \n",
    "    Input:\n",
    "     mA     - Input matrix where each row represents a sample and each column represents a class.\n",
    "    upBound - Upper bound for the number of copies of the rows.\n",
    "    \n",
    "    Output:\n",
    "     vX     - Solution vector representing the number of copies of each class.\n",
    "    \n",
    "    Remarks:\n",
    "    - The problem size (number of elements in `mA`) should be relatively small.\n",
    "    \"\"\"\n",
    "    \n",
    "    numSamples = np.size(mA, 0)\n",
    "    numClasses = np.size(mA, 1)\n",
    "    \n",
    "    # Variables\n",
    "    vX = cp.Variable(numSamples, integer = True) #<! Integer variable for class counts\n",
    "    μ  = cp.Variable(1) #<! Mean of the class counts\n",
    "\n",
    "    # Problem\n",
    "    cpObjFun = cp.Minimize(cp.norm(mA.T @ vX - μ, p = 'inf')) #<! Objective function\n",
    "    cpConst  = [vX >= 1, vX <= upBound, cp.mean(mA.T @ vX) == μ] #<! Constraints\n",
    "    oCvxPrb  = cp.Problem(cpObjFun, cpConst) #<! Create the convex problem instance\n",
    "    \n",
    "    # Solution\n",
    "    oCvxPrb.solve(solver = cp.HIGHS)  #<! Solve the problem\n",
    "    \n",
    "    assert (oCvxPrb.status == 'optimal'), 'The problem is not solved.'\n",
    "    \n",
    "    return vX.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the Point to Minimize Sum of Squared Euclidean Distances\n",
    "\n",
    "def SolveDiscreteInfNorm( mA: np.ndarray, upBound: float ) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Solve the discrete infinity norm optimization problem.\n",
    "    \n",
    "    Input:\n",
    "     mA     - Input matrix where each row represents a sample and each column represents a class.\n",
    "    upBound - Upper bound for the number of copies of the rows.\n",
    "    \n",
    "    Output:\n",
    "     vX     - Solution vector representing the number of copies of each class.\n",
    "    \n",
    "    Remarks:\n",
    "    - The problem size (number of elements in `mA`) should be relatively small.\n",
    "    \"\"\"\n",
    "    \n",
    "    numSamples = np.size(mA, 0)\n",
    "    numClasses = np.size(mA, 1)\n",
    "\n",
    "    #===========================Fill This===========================#\n",
    "    # 1. Set the optimization variables.\n",
    "    # 2. Define the objective.\n",
    "    # 3. Define the constraints.\n",
    "    # !! The solution should match the original formulation.\n",
    "    \n",
    "    # Variables\n",
    "    vX = ??? #<! Integer variable for class counts\n",
    "    μ  = ??? #<! Mean of the class counts\n",
    "    t  = ??? #<! Boundary variable for the maximum absolute deviation\n",
    "\n",
    "    # Problem\n",
    "    cpObjFun = ??? #<! Objective function\n",
    "    cpConst  = ??? #<! Constraints\n",
    "    oCvxPrb  = ??? #<! Create the convex problem instance\n",
    "    #===============================================================#\n",
    "    \n",
    "    oCvxPrb.solve(solver = cp.HIGHS)  #<! Solve the problem\n",
    "    \n",
    "    assert (oCvxPrb.status == 'optimal'), 'The problem is not solved.'\n",
    "    \n",
    "    return vX.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective Function Value\n",
    "\n",
    "def ObjFunMaxAbsDev( vX: np.ndarray, mA: np.ndarray ) -> float:\n",
    "\n",
    "    vY = mA.T @ vX\n",
    "    μ = np.mean(vY)\n",
    "    \n",
    "    return np.max(np.abs(vY - μ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification of the Solution\n",
    "\n",
    "vXRef = SolveDiscreteInfNormOrg(mA, upBound = upperbound)\n",
    "vX    = SolveDiscreteInfNorm(mA, upBound = upperbound)\n",
    "\n",
    "assert np.allclose(vX, vXRef), 'The solutions do not match.'\n",
    "print('The solutions match.')\n",
    "\n",
    "print(f'Objective Function Value: {ObjFunMaxAbsDev(vX, mA):0.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Class Balance\n",
    "\n",
    "hF, vHa = plt.subplots(nrows = 1, ncols = 2, figsize = (12, 4))\n",
    "hA = DisplayClassBalance(np.sum(mA, axis = 0), hA = vHa[0])\n",
    "hA.set_title(f'Original Class Balance, Objective: {ObjFunMaxAbsDev(np.ones(np.size(mA, 0)), mA):0.2f}')\n",
    "hA = DisplayClassBalance(mA.T @ vXRef, hA = vHa[1])\n",
    "hA.set_title(f'Balanced Class Balance, Objective: {ObjFunMaxAbsDev(vXRef, mA):0.2f}')\n",
    "vYlim = hA.get_ylim()\n",
    "vHa[0].set_ylim(vYlim);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulation 002 - Continuous\n",
    "\n",
    "One can promote flat / uniform continuous vector and use rounding to achieve discrete solution.  \n",
    "While not guaranteed to yield optimal discrete solution, it might yield good enough solution while being faster and scale better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 002\n",
    "\n",
    "The _Mean Filter Invariance_ property: $\\boldsymbol{C} \\boldsymbol{y} = \\boldsymbol{y}, \\; \\boldsymbol{y} \\in \\mathbb{R}^{n}, \\; \\boldsymbol{C} = \\frac{1}{n} \\boldsymbol{y} \\boldsymbol{y}^{\\top}$.  \n",
    "Only constant vectors holds it, hence it can be used to promote constant vectors.\n",
    "\n",
    "Formulate the problem with the objective defined by **minimization** of the _Mean Filter Invariance_: $\\arg \\min_{\\boldsymbol{y}} \\frac{1}{2} {\\left\\| \\boldsymbol{C} \\boldsymbol{y} - \\boldsymbol{y} \\right\\|}_{2}^{2}$.  \n",
    "\n",
    "Remarks:\n",
    " - One may use other Norms to promote the property.\n",
    " - The optimization is not defined up to a constant.  \n",
    "   Hence add penalty to ensure the lowest value which obeys it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 002\n",
    "\n",
    "<font color='red'>??? Fill the answer here ???</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KL Divergence Formulation\n",
    "\n",
    "def SolveContMeanInv( mA: np.ndarray, upBound: float ) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Solve the continuous Mean Invariance optimization problem.\n",
    "    Input:\n",
    "     mA     - Input matrix where each row represents a sample and each column represents a class\n",
    "    upBound - Upper bound for the number of copies of the rows.\n",
    "    Output:\n",
    "     vX     - Solution vector representing the number of copies of each class.\n",
    "    Remarks:\n",
    "     - A\n",
    "    \"\"\"\n",
    "    \n",
    "    numSamples = np.size(mA, 0)\n",
    "    numClasses = np.size(mA, 1)\n",
    "\n",
    "    #===========================Fill This===========================#\n",
    "    # 1. Set the optimization variables.\n",
    "    # 2. Define the objective.\n",
    "    # 3. Define the constraints.\n",
    "    \n",
    "    # Variables\n",
    "    vX = cp.Variable(numSamples) #<! Objective variable for class counts\n",
    "    vY = cp.Variable(numClasses) #<! Auxiliary variable\n",
    "\n",
    "    mC = np.ones((numClasses, numClasses)) / numClasses #<! Mean filter matrix\n",
    "    maxY = np.max(np.sum(mA, axis = 0)) * numClasses #<! Auxiliary variable for the mean filter\n",
    "\n",
    "    # Problem\n",
    "    cpObjFun = cp.Minimize(cp.sum_squares(mC @ vY - vY)) #<! Objective function\n",
    "    cpConst  = [mA.T @ vX == vY, vX >= 1, vX <= upBound, cp.sum(vY) <= maxY] #<! Constraints\n",
    "\n",
    "    oCvxPrb  = cp.Problem(cpObjFun, cpConst) #<! Create the convex problem instance\n",
    "    #===============================================================#\n",
    "    \n",
    "    oCvxPrb.solve(solver = cp.CLARABEL, verbose = False)  #<! Solve the problem\n",
    "    \n",
    "    assert (oCvxPrb.status == 'optimal'), 'The problem is not solved.'\n",
    "    \n",
    "    return vX.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Can the calculation of the optimization be optimized? Think of a matrix $\\boldsymbol{D}$ such that the objective is given by $\\boldsymbol{D} \\boldsymbol{y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the Continuous Problem\n",
    "\n",
    "vX  = SolveContMeanInv(mA, upBound = upperbound)\n",
    "vXR = np.round(vX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective Function\n",
    "\n",
    "def ObjFunMeanInv( vX: np.ndarray, mA: np.ndarray ) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the Mean Invariance of the class distribution.\n",
    "    \n",
    "    Input:\n",
    "     vX     - Solution vector representing the number of copies of each class.\n",
    "     mA     - Input matrix where each row represents a sample and each column represents a class.\n",
    "    \n",
    "    Output:\n",
    "     valE - The entropy value of the class distribution.\n",
    "\n",
    "    Remarks:\n",
    "    - A\n",
    "    \"\"\"\n",
    "\n",
    "    mC  = np.ones((numClasses, numClasses)) / numClasses #<! Mean filter matrix\n",
    "    mC -= np.eye(numClasses)  #<! Optimization of calculation\n",
    "    \n",
    "    vY = mA.T @ vX\n",
    "    valE = np.sum(np.square(mC @ vY)) \n",
    "    \n",
    "    return valE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Class Balance\n",
    "\n",
    "hF, vHa = plt.subplots(nrows = 1, ncols = 2, figsize = (12, 4))\n",
    "hA = DisplayClassBalance(np.sum(mA, axis = 0), hA = vHa[0])\n",
    "hA.set_title(f'Original Class Balance, Objective: {ObjFunMeanInv(np.ones(np.size(mA, 0)), mA):0.2f}')\n",
    "hA = DisplayClassBalance(mA.T @ vXR, hA = vHa[1])\n",
    "hA.set_title(f'Balanced Class Balance, Objective: {ObjFunMeanInv(vXR, mA):0.2f}')\n",
    "vYlim = hA.get_ylim()\n",
    "vHa[0].set_ylim(vYlim);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulation 003 - Continuous\n",
    "\n",
    "Most continuous formulations are based on a Norm based loss.  \n",
    "This section demonstration of a dissimilarity function based on probability concepts.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The KL Divergence is neither a distance function nor symmetric function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 003\n",
    "\n",
    "Formulate the problem with the objective defined by **minimization** of the _Kullback Leibler Divergence_ (KL Divergence): $\\operatorname{D}_{KL} \\left( \\boldsymbol{y}, \\boldsymbol{z} \\right) = \\sum_{i} {y}_{i} \\log \\left( \\frac{{y}_{i}}{{z}_{i}} \\right) - {y}_{i} + {z}_{i}$.\n",
    "\n",
    "Remarks:\n",
    " - Pay attention that for a fixed $\\boldsymbol{z}$ the KL Divergence is a _strictly_ convex function.\n",
    " - For a fixed $\\boldsymbol{z}$ the optimization over $\\boldsymbol{y}$ is equivalent ot maximization of the Entropy: $\\sum_{i} {y}_{i} \\left( \\log \\left( \\frac{{y}_{i}}{{z}_{i}} \\right) - 1 \\right)$.  \n",
    "   Assuming the natural logarithm is used, the above has a critical point for $\\frac{{y}_{i}}{{z}_{i}} = e$.\n",
    " - The vector $\\boldsymbol{z}$ ideally was the lowest value achievable for a uniform counts of the classes.  \n",
    "   In practice it is not known. The optimal value would be the class with maximum value as the constraint $\\boldsymbol{x} \\geq \\boldsymbol{1}$ means it can not be lower.\n",
    "\n",
    "\n",
    "<font color='red'>(**!**)</font><font color='red'>(**!**)</font><font color='red'>(**!**)</font> Currently [formulation requires the (Commercial) MOSEK solver](https://github.com/oxfordcontrol/Clarabel.rs/issues/197). Skip this formulation (Learn from it and implement another choice). <font color='red'>(**!**)</font><font color='red'>(**!**)</font><font color='red'>(**!**)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 003\n",
    "\n",
    "<font color='red'>??? Fill the answer here ???</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KL Divergence Formulation\n",
    "\n",
    "def SolveContKLDiv( mA: np.ndarray, upBound: float ) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Solve the continuous Kullback Leibler Divergence (KL Divergence) optimization problem.\n",
    "    Input:\n",
    "     mA     - Input matrix where each row represents a sample and each column represents a class\n",
    "    upBound - Upper bound for the number of copies of the rows.\n",
    "    Output:\n",
    "     vX     - Solution vector representing the number of copies of each class.\n",
    "    Remarks:\n",
    "     - The problem uses the Kullback Leibler Divergence to measure the difference between the counts of balanced class and a uniform counts (Max).\n",
    "     - Pay attention to the formulation of the KL Divergence in `CVXPY` and `SciPy` to support non distributional data.\n",
    "     - The solution of the optimization problem should be rounded to generate a discrete solution.\n",
    "    \"\"\"\n",
    "    \n",
    "    numSamples = np.size(mA, 0)\n",
    "    numClasses = np.size(mA, 1)\n",
    "\n",
    "    #===========================Fill This===========================#\n",
    "    # 1. Set the optimization variables.\n",
    "    # 2. Define the objective.\n",
    "    # 3. Define the constraints.\n",
    "    # !! You may find `cp.kl_div()` useful.\n",
    "    \n",
    "    # Variables\n",
    "    vX = ??? #<! Objective variable for class counts\n",
    "    vY = ??? #<! Auxiliary variable\n",
    "\n",
    "    # Problem\n",
    "    cpObjFun = ??? #<! Objective function\n",
    "    cpConst  = ??? #<! Constraints\n",
    "    oCvxPrb  = ??? #<! Create the convex problem instance\n",
    "    #===============================================================#\n",
    "    \n",
    "    oCvxPrb.solve(solver = cp.MOSEK, verbose = False)  #<! Solve the problem\n",
    "    \n",
    "    assert (oCvxPrb.status == 'optimal'), 'The problem is not solved.'\n",
    "    \n",
    "    return vX.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the Continuous Problem\n",
    "\n",
    "if 'MOSEK' in cp.installed_solvers():\n",
    "    vX  = SolveContKLDiv(mA, upBound = upperbound)\n",
    "else:\n",
    "    vX = np.ones(np.size(mA, 0)) #<! Fallback solution if MOSEK is not available\n",
    "vXR = np.round(vX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective Function\n",
    "\n",
    "def ObjFunEntropy( vX: np.ndarray, mA: np.ndarray ) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the Kullback Leibler Divergence (KL Divergence) of the class distribution.\n",
    "    \n",
    "    Input:\n",
    "     vX     - Solution vector representing the number of copies of each class.\n",
    "     mA     - Input matrix where each row represents a sample and each column represents a class.\n",
    "    \n",
    "    Output:\n",
    "     valE - The entropy value of the class distribution.\n",
    "\n",
    "    Remarks:\n",
    "    - The entropy is calculated using the Kullback Leibler Divergence (KL Divergence) with respect to a uniform vector.  \n",
    "      In this case the uniform vector is the maximum sum of the columns of `mA` multiplied by the number of classes.  \n",
    "      As the optimal result is  a uniform vector yet the lowest value can be achieved is the maximum class.\n",
    "    \"\"\"\n",
    "\n",
    "    vYY = np.max(np.sum(mA, axis = 0)) * np.ones(numClasses) #<! Auxiliary variable for the mean filter\n",
    "    \n",
    "    vY = mA.T @ vX\n",
    "    valE = np.sum(sp.special.kl_div(vY, vYY))  #<! Adding a small constant to avoid log(0)\n",
    "    \n",
    "    return valE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Class Balance\n",
    "\n",
    "hF, vHa = plt.subplots(nrows = 1, ncols = 2, figsize = (12, 4))\n",
    "hA = DisplayClassBalance(np.sum(mA, axis = 0), hA = vHa[0])\n",
    "hA.set_title(f'Original Class Balance, Objective: {ObjFunEntropy(np.ones(np.size(mA, 0)), mA):0.2f}')\n",
    "hA = DisplayClassBalance(mA.T @ vXR, hA = vHa[1])\n",
    "hA.set_title(f'Balanced Class Balance, Objective: {ObjFunEntropy(vXR, mA):0.2f}')\n",
    "vYlim = hA.get_ylim()\n",
    "vHa[0].set_ylim(vYlim);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='green'>(**@**)</font> Implement another Continuous Formulation of the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulation 004 - Discrete / Continuous\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\arg \\min_{\\boldsymbol{x}, \\boldsymbol{e}, c} \\quad & \\sum_{i = 1}^{n} \\left| {e}_{i} \\right| + c \\\\\n",
    "\\text{subject to} \\quad & \\begin{aligned} \n",
    "\\boldsymbol{A}^{T} \\boldsymbol{x} + \\boldsymbol{e} & = c \\boldsymbol{1} \\\\\n",
    "c & \\geq \\min \\left( {A}^{T} \\boldsymbol{1} \\right) \\\\\n",
    "\\boldsymbol{x} & \\geq \\boldsymbol{1} \\\\\n",
    "\\boldsymbol{x} & \\leq u \\boldsymbol{1} \\\\\n",
    "\\end{aligned}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "* <font color='red'>(**?**)</font> Explain how the formulation work.  \n",
    "   Explain the roles of $c$ and $\\boldsymbol{e}$. In practice, will the constraint on $c$ be active?\n",
    "* <font color='blue'>(**!**)</font> Implement the formulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulation 005 - Continuous (The Engineer Solution)\n",
    "\n",
    "One could solve the the problem with sum to 1 then using the [Least Common Multiple](https://en.wikipedia.org/wiki/Least_common_multiple) (LCM) to scale the result.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\arg \\min_{\\boldsymbol{x}} \\quad & \\frac{1}{2} {\\left\\| \\boldsymbol{A}^{T} \\boldsymbol{x} - \\boldsymbol{1} \\right\\|}_{2}^{2} \\\\\n",
    "\\text{subject to} \\quad & \\begin{aligned} \n",
    "\\boldsymbol{x} & \\geq \\frac{1}{r} \\boldsymbol{1} \\\\\n",
    "\\boldsymbol{x} & \\leq \\frac{u}{r} \\boldsymbol{1} \\\\\n",
    "\\end{aligned}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Where $r$ is the fraction resolution (Forcing an LCM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the Problem using Non Negative Least Squares\n",
    "\n",
    "oRes = sp.optimize.lsq_linear(mA.T, np.ones(mA.shape[1]), bounds = (1 / fracRes, upperbound / fracRes))\n",
    "vXF  = oRes.x\n",
    "vX = np.round(fracRes * vXF) #<! Round by the fraction resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> Using Accelerated Projected Gradient Descent one can solve it faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Class Balance\n",
    "\n",
    "hF, vHa = plt.subplots(nrows = 1, ncols = 2, figsize = (12, 4))\n",
    "hA = DisplayClassBalance(np.sum(mA, axis = 0), hA = vHa[0])\n",
    "hA.set_title(f'Original Class Balance')\n",
    "hA = DisplayClassBalance(mA.T @ vX, hA = vHa[1])\n",
    "hA.set_title(f'Balanced Class Balance')\n",
    "vYlim = hA.get_ylim()\n",
    "vHa[0].set_ylim(vYlim);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalency of Variance and Pair Wise Sum Square\n",
    "# Show empirical equivalency of the _Variance_ and _Pair Wise Sum Square_.\n",
    "\n",
    "def PairWiseSumSquare( vX: np.ndarray ) -> float:\n",
    "    \"\"\"\n",
    "    Computes the sum of pairwise squared differences for a given vector `vX`.\n",
    "    \"\"\"\n",
    "    numSamples = len(vX)\n",
    "    return np.sum(np.square(vX[:, None] - vX[None, :])) / (2 * numSamples * numSamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Equivalency\n",
    "numSamples = 1000\n",
    "vX = np.random.randn(numSamples)\n",
    "\n",
    "print(f'Variance: {np.mean(np.square(vX - np.mean(vX))):0.5f}')\n",
    "print(f'Sum of Pair Wise Square: {PairWiseSumSquare(vX):0.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximizing Entropy Promotes Constant Vectors\n",
    "\n",
    "vX   = cp.Variable(10)\n",
    "\n",
    "cpObjFun = cp.Maximize( cp.sum(cp.entr(vX)) ) #<! Objective Function\n",
    "cpConst  = [vX >= 1.2] #<! Constraint per each sample\n",
    "oCvxPrb  = cp.Problem(cpObjFun, cpConst)   \n",
    "\n",
    "oCvxPrb.solve(solver = cp.CLARABEL) #<! Solve the problem\n",
    "\n",
    "assert (oCvxPrb.status == 'optimal'), 'The problem is not solved.'\n",
    "print('Problem is solved.')\n",
    "\n",
    "vX.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Entropy in 2D\n",
    "\n",
    "vG = np.linspace(0.5, 3, 1000)\n",
    "\n",
    "vX = np.tile(vG, len(vG))\n",
    "vY = np.repeat(vG, len(vG))\n",
    "\n",
    "mXX = np.r_[np.reshape(vX, (1, -1)), np.reshape(vY, (1, -1))]\n",
    "mXX.shape\n",
    "\n",
    "vE = sp.stats.entropy(mXX, axis = 0)\n",
    "mE = np.reshape(vE, (len(vG), len(vG)))\n",
    "\n",
    "hF, hA = plt.subplots(figsize = (8, 6))\n",
    "# mE = np.log(1 + mE)  #<! Apply logarithm to the entropy values for better visualization\n",
    "mE = np.power(mE, 4)\n",
    "hA.imshow(mE, extent = (vG[0], vG[-1], vG[0], vG[-1]), origin = 'lower')\n",
    "# Add contour lines with different colors\n",
    "hA.contour(vG, vG, mE, levels = 100, cmap = 'jet', linewidths = 0.5, linestyles = 'solid')\n",
    "hA.set_xlabel('X')\n",
    "hA.set_ylabel('Y')\n",
    "hA.set_title('Entropy Surface');\n",
    "\n",
    "hF, hA = plt.subplots(figsize = (8, 6), subplot_kw = {'projection': '3d'})\n",
    "hA.view_init(elev = 25, azim = 225, roll = 0)\n",
    "hA.plot_surface(np.reshape(vX, (len(vG), len(vG))), np.reshape(vY, (len(vG), len(vG))), mE, cmap = 'viridis', edgecolor = 'none')\n",
    "hA.set_xlabel('X')\n",
    "hA.set_ylabel('Y')\n",
    "hA.set_zlabel('Entropy')\n",
    "hA.set_title('Entropy Surface');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
