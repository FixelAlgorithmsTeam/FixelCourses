{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Fixel Algorithms](https://fixelalgorithms.co/images/CCExt.png)](https://fixelalgorithms.gitlab.io)\n",
    "\n",
    "# Optimization Methods\n",
    "\n",
    "## Essential Linear Algebra - Numerical Differentiation\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 1.0.000 | 12/09/2024 | Royi Avital | First version                                                      |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/AIProgram/2024_02/0001LpNorm.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T17:52:07.921383Z",
     "start_time": "2022-02-02T17:52:07.649130Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Miscellaneous\n",
    "import os\n",
    "from platform import python_version\n",
    "import random\n",
    "import warnings\n",
    "import yaml\n",
    "\n",
    "# Visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython\n",
    "from IPython.display import Image, display\n",
    "from ipywidgets import Dropdown, FloatSlider, interact, IntSlider, Layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought.\n",
    "\n",
    "Code Notations:\n",
    "\n",
    "```python\n",
    "someVar    = 2; #<! Notation for a variable\n",
    "vVector    = np.random.rand(4) #<! Notation for 1D array\n",
    "mMatrix    = np.random.rand(4, 3) #<! Notation for 2D array\n",
    "tTensor    = np.random.rand(4, 3, 2, 3) #<! Notation for nD array (Tensor)\n",
    "tuTuple    = (1, 2, 3) #<! Notation for a tuple\n",
    "lList      = [1, 2, 3] #<! Notation for a list\n",
    "dDict      = {1: 3, 2: 2, 3: 1} #<! Notation for a dictionary\n",
    "oObj       = MyClass() #<! Notation for an object\n",
    "dfData     = pd.DataFrame() #<! Notation for a data frame\n",
    "dsData     = pd.Series() #<! Notation for a series\n",
    "hObj       = plt.Axes() #<! Notation for an object / handler / function handler\n",
    "```\n",
    "\n",
    "### Code Exercise\n",
    "\n",
    " - Single line fill\n",
    "\n",
    " ```python\n",
    " vallToFill = ???\n",
    " ```\n",
    "\n",
    " - Multi Line to Fill (At least one)\n",
    "\n",
    " ```python\n",
    " # You need to start writing\n",
    " ????\n",
    " ```\n",
    "\n",
    " - Section to Fill\n",
    "\n",
    "```python\n",
    "#===========================Fill This===========================#\n",
    "# 1. Explanation about what to do.\n",
    "# !! Remarks to follow / take under consideration.\n",
    "mX = ???\n",
    "\n",
    "???\n",
    "#===============================================================#\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# %matplotlib inline\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "# Matplotlib default color palette\n",
    "lMatPltLibclr = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "sns.set_theme() #>! Apply SeaBorn theme\n",
    "# sns.set_palette(\"tab10\")\n",
    "\n",
    "runInGoogleColab = 'google.colab' in str(get_ipython())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "FIG_SIZE_DEF    = (8, 8)\n",
    "ELM_SIZE_DEF    = 50\n",
    "CLASS_COLOR     = ('b', 'r')\n",
    "EDGE_COLOR      = 'k'\n",
    "MARKER_SIZE_DEF = 10\n",
    "LINE_WIDTH_DEF  = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Course Packages\n",
    "\n",
    "from NumericDiff import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# Grid\n",
    "lowerBound = -1.5\n",
    "upperBound = 1.5\n",
    "numGridPts = 500\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Differentiation\n",
    "\n",
    "This notebooks explores the use of [_Numerical Differentiation_](https://en.wikipedia.org/wiki/Numerical_differentiation) to caclulate the gradient of a function.\n",
    "\n",
    "The gradient of a multivariate scalar function, $f : \\mathbb{R}^{n} \\to \\mathbb{R}$, is given by:\n",
    "\n",
    "$$ {{\\nabla}_{x} f \\left( \\boldsymbol{x} \\right)}_{i} = \\lim_{t \\to 0} \\frac{ f \\left( \\boldsymbol{x} + t \\boldsymbol{e}_{i} \\right) - f \\left( \\boldsymbol{x} \\right) }{t} $$\n",
    "\n",
    "Where $\\boldsymbol{e}_{i} = \\left[ 0, 0, \\ldots, 0, \\underbrace{1}_{\\text{i -th index}}, 0, \\ldots, 0 \\right]$. \n",
    "\n",
    "This can be approximated by [_Finite Difference_](https://en.wikipedia.org/wiki/Finite_difference) with specific [_Finite Difference Coefficient_](https://en.wikipedia.org/wiki/Finite_difference_coefficient).  \n",
    "There 3 common approaches:\n",
    "\n",
    " - Forward: ${{\\nabla}_{x} f \\left( \\boldsymbol{x} \\right)}_{i} \\approx \\frac{ f \\left( \\boldsymbol{x} + h \\boldsymbol{e}_{i} \\right) - f \\left( \\boldsymbol{x} \\right) }{h}$.\n",
    " - Backward: ${{\\nabla}_{x} f \\left( \\boldsymbol{x} \\right)}_{i} \\approx \\frac{ f \\left( \\boldsymbol{x} \\right) - f \\left( \\boldsymbol{x} - h \\boldsymbol{e}_{i} \\right) }{h}$.\n",
    " - Central: ${{\\nabla}_{x} f \\left( \\boldsymbol{x} \\right)}_{i} \\approx \\frac{ f \\left( \\boldsymbol{x} + h \\boldsymbol{e}_{i} \\right) - f \\left( \\boldsymbol{x} - h \\boldsymbol{e}_{i} \\right) }{2 h}$.\n",
    "\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The notebook use the `xxx.py` file for the actual calculation.\n",
    "* <font color='red'>(**?**)</font> What are the advantages / disadvantages of the _forward_ / _backward_ vs. the _central_ mode?  \n",
    "  Think of the edge values and the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Gradient of a Composition of a Linear Function and Element Wise Function\n",
    "\n",
    "Compute the directional derivative $\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]$ and the gradient $\\nabla f \\left( \\boldsymbol{x} \\right)$ of:\n",
    "\n",
    "$$ f \\left( \\boldsymbol{x} \\right) = {a}^{T} g \\left( \\boldsymbol{x} \\right) $$\n",
    "\n",
    "Where $g \\left( \\cdot \\right)$ is an element wise function $g \\left( \\boldsymbol{x} \\right) = \\begin{bmatrix} g \\left( {x}_{1} \\right) \\\\ g \\left( {x}_{2} \\right) \\\\ \\vdots \\\\ g \\left( {x}_{d} \\right) \\end{bmatrix} \\in \\mathbb{R}^{d}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> For a _Matrix_ input We'll be using $\\left[ \\cdot \\right]$ as a notation for element wise functions.  \n",
    "  This will differentiate between $\\exp \\left[ \\boldsymbol{X} \\right] = \\sum_{n = 1}^{\\infty} \\frac{\\boldsymbol{A}^{n}}{n!}$ and $\\exp \\left[ \\boldsymbol{X} \\right] = \\exp \\left( {X}_{i, j}\\right) \\forall i, j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The directional derivative of $g \\left( \\cdot \\right)$ is given by:\n",
    "\n",
    "$$ \\nabla g \\left( \\boldsymbol{x} \\right) = \\lim_{t \\to 0} \\frac{g \\left( \\boldsymbol{x} + t \\boldsymbol{h} \\right) - g \\left( \\boldsymbol{x} \\right)}{t} = \\lim_{t \\to 0} \\frac{1}{t} \\left( \\begin{bmatrix} g \\left( {x}_{1} + t {h}_{1} \\right) \\\\ g \\left( {x}_{2} + t {h}_{2} \\right) \\\\ \\vdots \\\\ g \\left( {x}_{d} + t {h}_{d} \\right) \\end{bmatrix} - \\begin{bmatrix} g \\left( {x}_{1} \\right) \\\\ g \\left( {x}_{2} \\right) \\\\ \\vdots \\\\ g \\left( {x}_{d} \\right) \\end{bmatrix} \\right) = \\begin{bmatrix} g' \\left( {x}_{1} \\right) {h}_{1} \\\\ g' \\left( {x}_{2} \\right) {h}_{2} \\\\ \\vdots \\\\ g' \\left( {x}_{d} \\right) {h}_{d} \\end{bmatrix} = g' \\left( \\boldsymbol{x} \\right) \\circ \\boldsymbol{h} $$\n",
    "\n",
    "* <font color='brown'>(**#**)</font> Pay attention that $g \\left( \\cdot \\right)$ is not a scalar function but a vector function.\n",
    "\n",
    "By definition $ f \\left( \\boldsymbol{x} \\right) = \\left \\langle \\boldsymbol{x}, g \\left( \\boldsymbol{x} \\right) \\right \\rangle$ hence:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right] & = \\left \\langle \\boldsymbol{a}, \\nabla g \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right] \\right \\rangle && \\text{Linear operator} \\\\\n",
    "& = \\left \\langle \\boldsymbol{a}, g' \\left( \\boldsymbol{x} \\right) \\circ \\boldsymbol{h} \\right \\rangle && \\text{} \\\\\n",
    "& = \\left \\langle \\boldsymbol{a}, \\operatorname{Diag} \\left( g' \\left( \\boldsymbol{x} \\right) \\right) \\boldsymbol{h} \\right \\rangle && \\text{Property of Hadamard product: $\\boldsymbol{a} \\circ \\boldsymbol{b} = \\operatorname{Diag} \\left( \\boldsymbol{a} \\right) \\boldsymbol{b}$} \\\\\n",
    "& = \\left \\langle \\operatorname{Diag} \\left( g' \\left( \\boldsymbol{x} \\right) \\right) \\boldsymbol{a}, \\boldsymbol{h} \\right \\rangle && \\text{Adjoint of diagonal matrix} \\\\\n",
    "& \\Rightarrow \\nabla f \\left( \\boldsymbol{x} \\right) = \\operatorname{Diag} \\left( g' \\left( \\boldsymbol{x} \\right) \\right) \\boldsymbol{a}\n",
    "&& \\blacksquare\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> The function $\\operatorname{diag} \\left( \\cdot \\right) : \\mathbb{R}^{d \\times d} \\to \\mathbb{R}^{d} $ returns the diagonal of a matrix, that is, $\\boldsymbol{b} = \\operatorname{diag} \\left( \\boldsymbol{X} \\right) \\implies \\boldsymbol{b} \\left[ i \\right] = \\left( \\boldsymbol{X} \\left[ i, i\\right] \\right)$.\n",
    "* <font color='brown'>(**#**)</font> The function $\\operatorname{Diag} \\left( \\cdot \\right) : \\mathbb{R}^{d} \\to \\mathbb{R}^{d \\times d} $ returns a diagonal matrix from a vector, that is, $B = \\operatorname{Diag} \\left( \\boldsymbol{x} \\right) \\implies \\boldsymbol{B} \\left[ i, j \\right] = \\begin{cases}\n",
    "{x}_{i} & \\text{ if } i = j \\\\ \n",
    "0 & \\text{ if } i \\neq j \n",
    "\\end{cases}$.\n",
    "* <font color='brown'>(**#**)</font> Pay attention that $\\left \\langle \\boldsymbol{a}, \\operatorname{diag} \\left( X \\right) \\right \\rangle = \\left \\langle \\operatorname{Diag} \\left( \\boldsymbol{a} \\right), X \\right \\rangle$.\n",
    "* <font color='brown'>(**#**)</font> Pay attention that $\\operatorname{Diag} \\left( \\boldsymbol{x} \\right) \\boldsymbol{1} = \\boldsymbol{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Size Sensitivity Analysis\n",
    "\n",
    "In this section we'll analyze the sensitivity of the numerical differentiation to the step size, $h$.\n",
    "\n",
    "We'll use the function:\n",
    "\n",
    "$$ f \\left( \\boldsymbol{X} \\right) = \\left \\langle \\boldsymbol{A}, \\sin \\left[ \\boldsymbol{X} \\right] \\right \\rangle $$\n",
    "\n",
    "Where:\n",
    "\n",
    " - $\\boldsymbol{X} \\in \\mathbb{R}^{d \\times d}$.\n",
    " - The function $\\sin \\left[ \\cdot \\right]$ is the element wise $\\sin$ function: $\\boldsymbol{M} = \\sin \\left[ \\boldsymbol{X} \\right] \\implies \\boldsymbol{M} \\left[ i, j \\right] = \\sin \\left( \\boldsymbol{X} \\left[ i, j\\right] \\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla f \\left( X \\right) \\left[ \\boldsymbol{H} \\right] & = \\left \\langle A, \\left( \\cos \\left[ X \\right] \\right) \\circ H \\right \\rangle && \\text{Since $\\frac{d \\sin \\left( x \\right)}{dx} = \\cos \\left( x \\right)$} \\\\\n",
    "& = \\left \\langle \\cos \\left[ \\boldsymbol{X} \\right] \\circ \\boldsymbol{A}, H \\right \\rangle && \\text{Adjoint} \\\\\n",
    "& \\Rightarrow \\nabla f \\left( X \\right) = \\cos \\left[ \\boldsymbol{X} \\right] \\circ A\n",
    "&& \\blacksquare\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters of the Problem\n",
    "\n",
    "numSteps = 1000\n",
    "\n",
    "numRows = 5\n",
    "numCols = 1; #<! Like a vector\n",
    "\n",
    "vStepSize = np.logspace(-4, -11, numSteps)\n",
    "\n",
    "lMethods    = [DiffMode.BACKWARD, DiffMode.CENTRAL, DiffMode.FORWARD]\n",
    "lMethodName = ['Forward', 'Backward', 'Central']\n",
    "\n",
    "# Data \n",
    "mA = np.random.randn(numRows, numCols)\n",
    "mX = np.random.randn(numRows, numCols)\n",
    "\n",
    "# Function\n",
    "hF = lambda mX: np.sum(mA * np.sin(mX))\n",
    "\n",
    "# Analytic Gradient\n",
    "hGradF = lambda mX: np.cos(mX) * mA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensitivity Analysis\n",
    "\n",
    "numMethods = len(lMethods)\n",
    "\n",
    "vG = hGradF(mX)\n",
    "mE = np.zeros(shape = (numSteps, numMethods)) #<! Error\n",
    "\n",
    "for jj in range(numMethods):\n",
    "  for ii in range(numSteps):\n",
    "    mE[ii, jj] = 20 * np.log10(np.linalg.norm(vG - CalcFunGrad(mX, hF, diffMode = lMethods[jj], ε = vStepSize[ii]), np.inf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Results\n",
    "\n",
    "hF, hA = plt.subplots(figsize = (16, 8))\n",
    "\n",
    "for ii in range(numMethods):\n",
    "  hA.plot(vStepSize, mE[:, ii], lw = 2, label = f'{lMethodName[ii]}')\n",
    "\n",
    "hA.set_title('Numerical Differentiation Error - Max Absolute Error')\n",
    "hA.set_xlabel('Step Size')\n",
    "hA.set_ylabel('Error [dB]')\n",
    "\n",
    "hA.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Complex Step Trick\n",
    "\n",
    "In general, the finite differences step size si a function of the argument and the function itself.  \n",
    "There are many cases where the method becomes highly sensitive and with the finite floating point accuracy it might cause some errors.\n",
    "\n",
    "It turns out that for _real analytic functions_ (Think of a convergent Taylor Series) we can do a trick:\n",
    "\n",
    "$$ f \\left( x + ih \\right) = f \\left( x \\right) + f' \\left( x \\right) i h + \\frac{f'' \\left( x \\right)}{2} {\\left(ih \\right)}^{2} + \\mathcal{O}(h^3) \\implies \\mathrm{Im} \\,\\left( \\frac{ f \\left( x + ih \\right)}{h} \\right) = f' \\left( x \\right) + \\mathcal{O}(h^2). $$\n",
    "\n",
    "Which is much more stable regardless of the value of the step size.\n",
    "\n",
    "Yet, there are some cases to handle:\n",
    " - Use `abs()` which uses the definition `abs(x + i y) = sign(x) * (x + i y)`.\n",
    " - Use `min()` / `max()` which only use the real part for comparison.\n",
    " - Use `.'` instead of `'` to apply _transpose_ instead of _hermitian transpose_.\n",
    "\n",
    "Resources:\n",
    " - [Sebastien Boisgerault - Complex Step Differentiation](https://direns.mines-paristech.fr/Sites/Complex-analysis/Complex-Step%20Differentiation/).\n",
    " - [Nick Higham - What Is the Complex Step Approximation](https://nhigham.com/2020/10/06/what-is-the-complex-step-approximation/).\n",
    " - [Derek Elkins - Complex Step Differentiation](https://www.hedonisticlearning.com/posts/complex-step-differentiation.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "In order to verify the robustness of the problem we'll use:\n",
    "\n",
    "$$ f \\left( x \\right) = {e}^{x} $$\n",
    "\n",
    "At $x = 0$, which will allow us to use a perfect reference and the relative error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "numSteps = 1500\n",
    "\n",
    "vStepSize = np.logspace(-3, -15, numSteps)\n",
    "\n",
    "lMethods    = [DiffMode.BACKWARD, DiffMode.CENTRAL, DiffMode.FORWARD, DiffMode.COMPLEX]\n",
    "lMethodName = ['Forward', 'Backward', 'Central', 'Complex']\n",
    "\n",
    "# Data \n",
    "valX = 0.0\n",
    "\n",
    "# Function\n",
    "hF = lambda x: np.exp(x)\n",
    "\n",
    "# Analytic Gradient\n",
    "gradF = 1; #<! At x = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensitivity Analysis\n",
    "\n",
    "numMethods = len(lMethods)\n",
    "\n",
    "mE = np.zeros(shape = (numSteps, numMethods)) #<! Error\n",
    "\n",
    "for jj in range(numMethods):\n",
    "  for ii in range(numSteps):\n",
    "    mE[ii, jj] = 20 * np.log10(abs(gradF - CalcFunGrad(valX, hF, diffMode = lMethods[jj], ε = vStepSize[ii])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Results\n",
    "\n",
    "hF, hA = plt.subplots(figsize = (16, 8))\n",
    "\n",
    "for ii in range(numMethods):\n",
    "  hA.plot(vStepSize, mE[:, ii], lw = 2, label = f'{lMethodName[ii]}')\n",
    "\n",
    "hA.set_title('Numerical Differentiation Error - Relative Error')\n",
    "hA.set_xlabel('Step Size')\n",
    "hA.set_ylabel('Error [dB]')\n",
    "hA.set_xscale('log')\n",
    "hA.invert_xaxis()\n",
    "\n",
    "hA.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "4c555be6fa9458c8c75b4612c68315d9f1d74815b73d0e564fda29ad772cfcda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
