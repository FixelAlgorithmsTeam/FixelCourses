{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Fixel Algorithms](https://fixelalgorithms.co/images/CCExt.png)](https://fixelalgorithms.gitlab.io)\n",
    "\n",
    "# Optimization Methods\n",
    "\n",
    "## Convex Optimization - Algorithms & Solvers - Alternating Direction Method of Multipliers (ADMM)\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 1.0.000 | 04/10/2024 | Royi Avital | First version                                                      |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/AIProgram/2024_02/0012LinearFitL1.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T09:30:06.492269Z",
     "start_time": "2022-02-02T09:30:06.220934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "\n",
    "# Optimization\n",
    "import cvxpy as cp\n",
    "from sksparse.cholmod import cholesky\n",
    "\n",
    "# Miscellaneous\n",
    "import math\n",
    "from platform import python_version\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Typing\n",
    "from typing import Callable, List, Tuple, Union\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought.\n",
    "\n",
    "Code Notations:\n",
    "\n",
    "```python\n",
    "someVar    = 2; #<! Notation for a variable\n",
    "vVector    = np.random.rand(4) #<! Notation for 1D array\n",
    "mMatrix    = np.random.rand(4, 3) #<! Notation for 2D array\n",
    "tTensor    = np.random.rand(4, 3, 2, 3) #<! Notation for nD array (Tensor)\n",
    "tuTuple    = (1, 2, 3) #<! Notation for a tuple\n",
    "lList      = [1, 2, 3] #<! Notation for a list\n",
    "dDict      = {1: 3, 2: 2, 3: 1} #<! Notation for a dictionary\n",
    "oObj       = MyClass() #<! Notation for an object\n",
    "dfData     = pd.DataFrame() #<! Notation for a data frame\n",
    "dsData     = pd.Series() #<! Notation for a series\n",
    "hObj       = plt.Axes() #<! Notation for an object / handler / function handler\n",
    "```\n",
    "\n",
    "### Code Exercise\n",
    "\n",
    " - Single line fill\n",
    "\n",
    " ```python\n",
    " vallToFill = ???\n",
    " ```\n",
    "\n",
    " - Multi Line to Fill (At least one)\n",
    "\n",
    " ```python\n",
    " # You need to start writing\n",
    " ????\n",
    " ```\n",
    "\n",
    " - Section to Fill\n",
    "\n",
    "```python\n",
    "#===========================Fill This===========================#\n",
    "# 1. Explanation about what to do.\n",
    "# !! Remarks to follow / take under consideration.\n",
    "mX = ???\n",
    "\n",
    "???\n",
    "#===============================================================#\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# %matplotlib inline\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "# Matplotlib default color palette\n",
    "lMatPltLibclr = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "# sns.set_theme() #>! Apply SeaBorn theme\n",
    "# sns.set_palette(\"tab10\")\n",
    "\n",
    "runInGoogleColab = 'google.colab' in str(get_ipython())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "FIG_SIZE_DEF    = (8, 8)\n",
    "ELM_SIZE_DEF    = 50\n",
    "CLASS_COLOR     = ('b', 'r')\n",
    "EDGE_COLOR      = 'k'\n",
    "MARKER_SIZE_DEF = 10\n",
    "LINE_WIDTH_DEF  = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Course Packages\n",
    "\n",
    "from AuxFun import ProxGradientDescent\n",
    "from AuxFun import DisplayCompaisonSummary, DisplayRunSummary, MakeSignal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# Data\n",
    "numSamples  = 200\n",
    "noiseStd    = 0.25\n",
    "\n",
    "λ = 0.5\n",
    "\n",
    "# Solver\n",
    "μ               = 0.0025\n",
    "ρ               = 2\n",
    "numIterations   = 10_000\n",
    "\n",
    "# # Verification\n",
    "ε = 1e-6 #<! Error threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADMM for TV Denoising\n",
    "\n",
    "The _Total Variation Denoising_ is a sparse based module which promotes the model of a _piece wise constant_ signal.  \n",
    "Its general form is given as:\n",
    "\n",
    "$$ \\arg \\min_{\\boldsymbol{x}} \\frac{1}{2} {\\left\\| \\boldsymbol{A} \\boldsymbol{x} - \\boldsymbol{b} \\right\\|}_{2}^{2} + \\lambda {\\left\\| \\boldsymbol{D} \\boldsymbol{x} \\right\\|}_{1} $$\n",
    "\n",
    "Where $\\lambda {\\left\\| \\boldsymbol{D} \\boldsymbol{x} \\right\\|}_{1}$ is the regularization term with $\\lambda \\geq 0$ sets the regularization level.  \n",
    "The matrix $\\boldsymbol{D}$ represent the [Finite Difference operator](https://en.wikipedia.org/wiki/Finite_difference).  \n",
    "In this case the regularization term promotes sparsity over the 1st derivative which implies piece wise constant signal.  \n",
    "\n",
    "* <font color='brown'>(**#**)</font> There are variations of the [Finite Difference Coefficients](https://en.wikipedia.org/wiki/Finite_difference_coefficient).\n",
    "* <font color='brown'>(**#**)</font> The regularization is a non smooth function. It requires _non smooth solver_.\n",
    "\n",
    "This notebooks covers:\n",
    " - The solution of the above problem using the ADMM Method.  \n",
    " - Comparing the convergence of the ADMM method to the accelerated Sub Gradient.\n",
    "\n",
    "### The ADMM Method\n",
    "\n",
    "The Alternating Direction Method of Multipliers (ADMM) is a powerful optimization technique that solves the composite model.  \n",
    "The canonical form of the problem is:\n",
    "\n",
    "$$ \\arg \\min_{\\boldsymbol{x}} f \\left( \\boldsymbol{x} \\right) + \\lambda g \\left( \\boldsymbol{P} \\boldsymbol{x} \\right) $$  \n",
    "\n",
    "Its main motivation is when $\\lambda g \\left( \\boldsymbol{P} \\boldsymbol{x} \\right)$ does not have an efficient Prox Operator while $\\lambda g \\left( \\boldsymbol{x} \\right)$ does.  \n",
    "In order to solve the problem the ADMM method breaks the problem into smaller subproblems:\n",
    "\n",
    "$$ \\arg \\min_{\\boldsymbol{x}, \\boldsymbol{z}} f \\left( \\boldsymbol{x} \\right) + \\lambda g \\left( \\boldsymbol{z} \\right), \\; \\text{ subject to } \\; \\boldsymbol{P} \\boldsymbol{x} = \\boldsymbol{z} $$\n",
    "\n",
    "Each subproblem is simpler and easier to solve while ensuring convergence to a global solution:\n",
    "\n",
    " - $\\arg \\min_{\\boldsymbol{x}} f \\left( \\boldsymbol{x} \\right) + \\frac{\\rho}{2} {\\left\\| \\boldsymbol{P} \\boldsymbol{x} - \\boldsymbol{z}^{k - 1} + \\boldsymbol{w}^{k - 1} \\right\\|}_{2}^{2}$.\n",
    " - $\\arg \\min_{\\boldsymbol{z}} \\lambda g \\left( \\boldsymbol{z} \\right) + \\frac{\\rho}{2} {\\left\\| \\boldsymbol{P} \\boldsymbol{x}^{k} - \\boldsymbol{z} + \\boldsymbol{w}^{k - 1} \\right\\|}_{2}^{2} = \\operatorname{prox}_{\\frac{\\lambda}{\\rho} g} \\left( \\boldsymbol{P} \\boldsymbol{x}^{k} - \\boldsymbol{z}^{k} \\right)$.\n",
    " - $\\boldsymbol{w}^{k} = \\boldsymbol{w}^{k - 1} + \\left( \\boldsymbol{P} \\boldsymbol{x}^{k} - \\boldsymbol{z}^{k} \\right)$.\n",
    "\n",
    "ADMM is ideal for large scale problems or when the objective has separable structure.  \n",
    "Its convergence is similar to Accelerated Proximal Gradient Descent yet in practice it is faster and more robust.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The minimization updates is in [_Gauss Seidel Method_](https://en.wikipedia.org/wiki/Gauss%E2%80%93Seidel_method) style.  \n",
    "Namely each step uses the latest version of the parameters and not the ones from the previous iteration.\n",
    "* <font color='brown'>(**#**)</font> There are canonical forms of ADMM. For instance, in case $f$ is the Linear Least Squares problem.  \n",
    "  The efficiency of the method depends on utilizing those forms properly.\n",
    "* <font color='brown'>(**#**)</font> The ADMM is a very robust method. It has a single hyper parameter, $\\rho$.  \n",
    "  It will converge given any non negative value of $\\rho$, yet it will have effect on the speed. \n",
    "* <font color='brown'>(**#**)</font> The parameter $\\rho$ balances between feasibility (Higher value) to minimization (Lower value).  \n",
    "There are adaptive policies for setting ${\\rho}_{k}$. See [Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers](https://stanford.edu/~boyd/papers/pdf/admm_distr_stats.pdf) section `3.4.1`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is a Piece Wise Constant signal to match the model of the problem.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The function `MakeSignal`  is based on code by [Ivan W. Selesnick - Total Variation Denoising](https://eeweb.engineering.nyu.edu/iselesni/lecture_notes/TVDmm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate / Load the Data\n",
    "\n",
    "# Model Data\n",
    "vS = MakeSignal('Blocks', numSamples)\n",
    "vY = vS + (noiseStd * np.random.randn(numSamples))\n",
    "\n",
    "mD = sp.sparse.spdiags([-np.ones(numSamples), np.ones(numSamples)], [0, 1], numSamples - 1, numSamples) #<! Different than MATLAB in the length required\n",
    "\n",
    "mX = np.zeros(shape = (numIterations, numSamples)) #<! Initialization by zeros\n",
    "\n",
    "dSolverData = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Data \n",
    "\n",
    "hF, hA = plt.subplots(figsize = (10, 6))\n",
    "hA.plot(range(numSamples), vS, lw = 2, label = 'Signal Model')\n",
    "hA.plot(range(numSamples), vY, ls = 'None', marker = '*', ms = 5, label = 'Signal Samples')\n",
    "hA.set_title('Signal Model and Signal Samples (Noisy)')\n",
    "hA.set_xlabel('Sample Index')\n",
    "hA.set_ylabel('Sample Value')\n",
    "\n",
    "hA.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> How the gradient of the signal model would look like?\n",
    "* <font color='red'>(**?**)</font> How the gradient of the signal samples  would look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Variation Denoising\n",
    "\n",
    "This section defines the problem and solve it using the _ADMM_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective Function\n",
    "\n",
    "The objective function of the TV Denoising:\n",
    "\n",
    "$$ \\arg \\min_{\\boldsymbol{x}} \\underbrace{\\frac{1}{2} {\\left\\| \\boldsymbol{x} - \\boldsymbol{y} \\right\\|}_{2}^{2}}_{\\text{Fidelity}} + \\underbrace{\\lambda {\\left\\| \\boldsymbol{D} \\boldsymbol{x} \\right\\|}_{1}}_{\\text{Regularization}} $$\n",
    "\n",
    "The format of fidelity term and regularization is common in [_Inverse Problem_](https://en.wikipedia.org/wiki/Inverse_problem) which are one of the most challenging types of problems in Engineering.  \n",
    "The regularization is modelling a desired effect on the output. In our case promoting sparse derivative which implies a _piece wise constant_ signal.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> For simplicity the explicit sparse matrix is used above. In many cases it is better to use an operator.  \n",
    "In this case by applying a convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective Function\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Implement the objective function. \n",
    "#    Given a vector of `vX` it returns the objective.\n",
    "# 2. The implementation should be using a Lambda Function.\n",
    "# !! You may `np.square()` and / or `np.linalg.norm()`.\n",
    "\n",
    "hObjFun = lambda vX: 0.5 * np.square(np.linalg.norm(vX - vY)) + λ * np.linalg.norm(mD @ vX, ord = 1)\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> How would the least squares (With no regularization, $\\lambda = 0$) solution look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "This section solves the problem in 3 ways:\n",
    "\n",
    " - DCP Solver: As the problem is _convex_ and relatively small it can be solved by a DCP solver.\n",
    " - Accelerated Sub Gradient: Iterative method using the _sub gradient_ of the regularization term with acceleration.\n",
    " - ADMM: Iterative separable method which can utilize the Prox operator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DCP Solver\n",
    "\n",
    "Solving the problem using a DCP Solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DCP Solution\n",
    "# The Total Variation Denoising model.\n",
    "# Solved using `CVXPY`.\n",
    "\n",
    "startTime = time.time()\n",
    "\n",
    "solverString = 'CVXPY'\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Create the auxiliary variable `vX`.\n",
    "# 1. Define the objective function.\n",
    "# 3. Define the constraints.\n",
    "# 4. Solve the problem using `CVXPY`.\n",
    "# !! You may use list operations to define constraints.\n",
    "\n",
    "vX = cp.Variable(numSamples) #<! Objective Variable\n",
    "\n",
    "cpObjFun = cp.Minimize(0.5 * cp.sum_squares(vX - vY) + λ * cp.norm(mD @ vX, 1)) #<! Objective Function\n",
    "cpConst  = [] #<! Constraints\n",
    "oCvxPrb  = cp.Problem(cpObjFun, cpConst) #<! Problem\n",
    "#===============================================================#\n",
    "\n",
    "oCvxPrb.solve(solver = cp.SCS)\n",
    "\n",
    "assert (oCvxPrb.status == 'optimal'), 'The problem is not solved.'\n",
    "print('Problem is solved.')\n",
    "\n",
    "vX = vX.value\n",
    "\n",
    "runTime = time.time() - startTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing Results\n",
    "\n",
    "DisplayRunSummary(solverString, hObjFun, vX, runTime, oCvxPrb.status)\n",
    "\n",
    "dSolverData[solverString] = {'vX': vX, 'objVal': hObjFun(vX)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accelerated Sub Gradient Solver\n",
    "\n",
    "This section implements the Accelerated Sub Gradient Solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Function\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Implement the gradient function of (1/2) * || x - y ||_2^2 + λ * || D x ||_1. \n",
    "#    Given a vector `vX` it returns the gradient at `vX`.\n",
    "# 2. The implementation should be using a Lambda Function.\n",
    "# !! You may pre calculate terms for efficient code.\n",
    "\n",
    "hGradFun = lambda vX: (vX - vY) + (λ * mD.T @ np.sign(mD @ vX))\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prox Function\n",
    "\n",
    "hProxFun = lambda vY, λ: vY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sub Gradient Solution\n",
    "# The Total Variation Denoising model.\n",
    "# Solved using Accelerates Sub Gradient Method.\n",
    "\n",
    "hMuK = lambda kk: μ #<! Try using the 1/L \n",
    "# hMuK = lambda kk: 1 / math.sqrt(kk) #<! Classic Sub Gradient\n",
    "\n",
    "startTime = time.time()\n",
    "\n",
    "solverString = 'Accelerated Sub Gradient'\n",
    "\n",
    "oSubGrad = ProxGradientDescent(np.zeros(numSamples), hGradFun, μ, λ, hProxFun = hProxFun, useAccel = True)\n",
    "lX = oSubGrad.ApplyIterations(numIterations, logArg = True)\n",
    "\n",
    "runTime = time.time() - startTime\n",
    "\n",
    "mX = np.array(lX)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing Results\n",
    "\n",
    "vX = np.copy(mX[-1])\n",
    "\n",
    "DisplayRunSummary(solverString, hObjFun, vX, runTime)\n",
    "\n",
    "dSolverData[solverString] = {'vX': vX, 'objVal': hObjFun(vX), 'mX': np.copy(mX)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADMM Solver\n",
    "\n",
    "This section implements the ADMM.  \n",
    "\n",
    "> [!TIP]  \n",
    "> For $k \\in \\left\\{ 1, 2, \\ldots, K \\right\\}$:\n",
    ">    1. $\\boldsymbol{x}^{k} = \\arg \\min_{\\boldsymbol{x}} f \\left( \\boldsymbol{x} \\right) + \\frac{\\rho}{2} {\\left\\| \\boldsymbol{P} \\boldsymbol{x} - \\boldsymbol{z}^{k - 1} + \\boldsymbol{w}^{k - 1} \\right\\|}_{2}^{2}$\n",
    ">    2. $\\boldsymbol{z}^{k} = \\arg \\min_{\\boldsymbol{z}} \\lambda g \\left( \\boldsymbol{z} \\right) + \\frac{\\rho}{2} {\\left\\| \\boldsymbol{P} \\boldsymbol{x}^{k} - \\boldsymbol{z} + \\boldsymbol{w}^{k - 1} \\right\\|}_{2}^{2}$\n",
    ">    3. $\\boldsymbol{w}^{k} = \\boldsymbol{w}^{k - 1} + \\left( \\boldsymbol{P} \\boldsymbol{x}^{k} - \\boldsymbol{z}^{k} \\right)$\n",
    "\n",
    "Where $\\arg \\min_{\\boldsymbol{z}} \\lambda g \\left( \\boldsymbol{z} \\right) + \\frac{\\rho}{2} {\\left\\| \\boldsymbol{P} \\boldsymbol{x}^{k} - \\boldsymbol{z} + \\boldsymbol{w}^{k - 1} \\right\\|}_{2}^{2} = \\operatorname{prox}_{\\frac{\\lambda}{\\rho} g} \\left( \\boldsymbol{P} \\boldsymbol{x}^{k} - \\boldsymbol{z}^{k} \\right)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADMM Function\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Implement the ADMM solver. \n",
    "#    Given a matrix `mX` with shape `(numIter, dataDim)` it applies the method.\n",
    "# 2. An input parameter is `hMinFun` which is callable `hMinFun(vZ, vW, ρ)`.  \n",
    "#    It minimizes the problem with respect to `vX`.\n",
    "# 3. An input parameter is `hProxFun` which is callable `hProxFun(vY, λ)`.  \n",
    "#    It minimizes the problem with respect to `vZ`.\n",
    "# 4. The initial value is given by `mX[0, :]`.\n",
    "# !! Do not overwrite `mX[0, :]`.\n",
    "# !! Follow the doc string of the function.\n",
    "\n",
    "def ADMM( mX: np.ndarray, hMinFun: Callable[[np.ndarray, np.ndarray, float], np.ndarray], hProxFun: Callable[[np.ndarray, float], np.ndarray], mP: np.ndarray, /, *, ρ: float = 1.0, λ: float = 1.0 ) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Alternating Direction Method of Multipliers (ADMM) for Solving Composite Optimization Problems.\n",
    "\n",
    "    This class implements the ADMM algorithm to solve problems of the form:\n",
    "\n",
    "        F(x) = f(x) + λ * g(P * x)    ->    f(x) + λ * g(z), subject to P * x - z = 0\n",
    "\n",
    "    Where:\n",
    "    - `f(x)` is a function that is easily minimized (e.g., least squares).\n",
    "    - `g(z)` is a function that has an efficient proximal mapping.\n",
    "    - `P` is a linear operator (matrix) that transforms `x` into `z`.\n",
    "\n",
    "    ADMM splits the problem into subproblems that alternately minimize over `x` and `z`, with a dual update for the constraint violation.  \n",
    "    This method is well suited for large scale or distributed optimization problems where the objective is separable or involves a linear constraint.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    mX : np.ndarray\n",
    "        A 2D array of shape `(numIter, dataDim)`, where each row represents a point in the optimization process.\n",
    "        The initial point is provided in `mX[0]`, and subsequent points are updated in place.\n",
    "\n",
    "    hMinFun : Callable[[np.ndarray, np.ndarray, float], np.ndarray]\n",
    "        A function that minimizes the function `f(x)` and a quadratic penalty term `(ρ / 2) * || P * x - z + w ||_2^2`.\n",
    "        This function should take three arguments: the current values of `z`, the dual variable `w`, and the penalty parameter `ρ`, \n",
    "        and return the next `x` value that minimizes `f(x) + (ρ / 2) * || P * x - z + w ||_2^2`.\n",
    "\n",
    "    hProxFun : Callable[[np.ndarray, float], np.ndarray]\n",
    "        A function that computes the proximal mapping of `g(z)` with respect to the current value of `P * x + w`.\n",
    "        This function should take two inputs: the current value of `P * x + w`, and the scaled penalty parameter `λ / ρ`, \n",
    "        and return the next `z` value that minimizes `λ * g(z) + (ρ / 2) * || P * x - z + w ||_2^2`.\n",
    "\n",
    "    mP : np.ndarray\n",
    "        The linear operator (matrix) that transforms the variable `x` into `z`, i.e., the matrix `P` in the constraint `Px - z = 0`.\n",
    "\n",
    "    ρ : float, optional\n",
    "        Penalty parameter for the augmented Lagrangian. This parameter controls the tradeoff between the primal and dual residuals \n",
    "        in the optimization. Larger values of `ρ` put more emphasis on satisfying the constraint `P * x - z = 0`.\n",
    "        Range: (0, inf).\n",
    "        Default: `1.0`.\n",
    "\n",
    "    λ : float, optional\n",
    "        The regularization parameter that weights the term `g(z)` in the objective function. \n",
    "        Range: (0, inf).\n",
    "        Default: `1.0`.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    mX : np.ndarray\n",
    "        The updated sequence of points after performing the optimization. The final point can be found\n",
    "        in `mX[-1]`.\n",
    "    \n",
    "    Notes:\n",
    "    ------\n",
    "    - ADMM is an efficient and flexible method for solving optimization problems that involve separable objectives and \n",
    "      linear constraints.\n",
    "    - The algorithm alternates between minimizing `x` and `z`, which allows for parallelization or distributed \n",
    "      computation in some applications.\n",
    "    - Convergence is typically achieved when the primal and dual residuals (measures of constraint violation and dual \n",
    "      update) become sufficiently small.\n",
    "    \"\"\"\n",
    "\n",
    "    numIter = np.size(mX, 0)\n",
    "    \n",
    "    # Initialization\n",
    "    vZ = np.empty(np.size(mP, 0)) #<! Separable variable\n",
    "    vW = np.empty(np.size(mP, 0)) #<! Dual variable (Lagrangian Multiplier)\n",
    "    \n",
    "    # Steady state\n",
    "    for kk in range(1, numIter):\n",
    "        mX[kk] = hMinFun(vZ, vW, ρ) #<! Solves \\arg \\min_x f(x) + (ρ / 2) * || P * x - z + w ||_2^2\n",
    "        vZ[:]  = hProxFun(mP @ mX[kk] + vW, λ / ρ) #<! Solves \\arg \\min_z λ * g(z) + (ρ / 2) * || P * x - z + w ||_2^2\n",
    "        vW[:] += mP @ mX[kk] - vZ #<! Dual variable update\n",
    "    \n",
    "    return mX\n",
    "\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADMM Auxiliary Functions\n",
    "# Implement the `hMinFun()` and `hProxFun()`.\n",
    "\n",
    "# Minimization Function\n",
    "#===========================Fill This===========================#\n",
    "# 1. Implement the minimization function `hMinFun()`.\n",
    "#    The function minimizes \\arg \\min_x 0.5 * || x - y ||_2^2 + (ρ / 2) * || D * x - z + w ||_2^2. \n",
    "#    Given the vectors `vZ` and `vW` and the parameter `ρ` it returns the optimal value for `vX`.\n",
    "# 2. The implementation should be using a Lambda Function.\n",
    "# !! You may solve the LS problem or try to vanish the gradient.\n",
    "# !! You may pre calculate terms for efficient code.\n",
    "# !! You may assume `ρ` is a known constant to improve performance.\n",
    "\n",
    "mDD = sp.sparse.eye(numSamples) + ρ * (mD.T @ mD) #<! In practice, much better use operators\n",
    "# Cholesky from `Cholmod` requires `CSC` matrices\n",
    "mDC = cholesky(sp.sparse.csc_matrix(mDD)) #<! An alternative: https://gist.github.com/omitakahiro/c49e5168d04438c5b20c921b928f1f5d\n",
    "\n",
    "hMinFun = lambda vZ, vW, ρ: mDC(vY + ρ * mD.T @ (vZ - vW)) #<! Normal equations (Squared the condition number)\n",
    "#===============================================================#\n",
    "\n",
    "# Prox Function\n",
    "#===========================Fill This===========================#\n",
    "# 1. Implement the prox operator function of the `λ || z ||_1 + (ρ / 2) * || z - (w + D x) ||_2^2` function. \n",
    "#    Given a vector `vY` and `λ` it returns the proximal at `vY`.\n",
    "# 2. The implementation should be using a Lambda Function.\n",
    "# !! You may assume `λ` > 0.\n",
    "# !! In this case `vY = w + D x`. Assume all given is `vY`.\n",
    "\n",
    "hProxFun = lambda vY, λ: np.maximum(np.fabs(vY) - λ, 0) * np.sign(vY) #<! Prox L1\n",
    "#===============================================================#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADMM Solution\n",
    "# The Total Variation Denoising model.\n",
    "# Solved using ADMM Method.\n",
    "\n",
    "startTime = time.time()\n",
    "\n",
    "solverString = 'ADMM'\n",
    "\n",
    "mX = ADMM(mX, hMinFun, hProxFun, mD, ρ = ρ, λ = λ)\n",
    "\n",
    "runTime = time.time() - startTime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing Results\n",
    "\n",
    "vX = np.copy(mX[-1])\n",
    "\n",
    "DisplayRunSummary(solverString, hObjFun, vX, runTime)\n",
    "\n",
    "dSolverData[solverString] = {'vX': vX, 'objVal': hObjFun(vX), 'mX': np.copy(mX)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Results\n",
    "\n",
    "hF = DisplayCompaisonSummary(dSolverData, hObjFun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='blue'>(**!**)</font> Replace `hMuK` with the step size of the Sub Gradient Method.  \n",
    "This is the difference between \"practice\" and theory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Data \n",
    "\n",
    "hF, hA = plt.subplots(figsize = (10, 6))\n",
    "hA.plot(range(numSamples), vS, lw = 2, label = 'Signal Model')\n",
    "hA.plot(range(numSamples), vY, ls = 'None', marker = '*', ms = 5, label = 'Signal Samples')\n",
    "hA.plot(range(numSamples), mX[-1], lw = 2, label = 'Denoised Signal')\n",
    "hA.set_title('Signal Model, Signal Samples (Noisy) and Smoothed Signal')\n",
    "hA.set_xlabel('Sample Index')\n",
    "hA.set_ylabel('Sample Value')\n",
    "\n",
    "hA.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> What the result of $\\lambda \\to \\infty$ would look like?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
