{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Fixel Algorithms](https://fixelalgorithms.co/images/CCExt.png)](https://fixelalgorithms.gitlab.io)\n",
    "\n",
    "# Optimization Methods\n",
    "\n",
    "## Convex Optimization - Non Smooth Optimization - Proximal Gradient Method\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 1.1.000 | 31/07/2025 | Royi Avital | Added feature importance analysis for the Diabetes data set        |\n",
    "| 1.0.000 | 03/10/2024 | Royi Avital | First version                                                      |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/AIProgram/2024_02/0012LinearFitL1.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T09:30:06.492269Z",
     "start_time": "2022-02-02T09:30:06.220934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "# Miscellaneous\n",
    "from platform import python_version\n",
    "import random\n",
    "\n",
    "# Typing\n",
    "from typing import Callable, List, Tuple, Union\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought.\n",
    "\n",
    "Code Notations:\n",
    "\n",
    "```python\n",
    "someVar    = 2; #<! Notation for a variable\n",
    "vVector    = np.random.rand(4) #<! Notation for 1D array\n",
    "mMatrix    = np.random.rand(4, 3) #<! Notation for 2D array\n",
    "tTensor    = np.random.rand(4, 3, 2, 3) #<! Notation for nD array (Tensor)\n",
    "tuTuple    = (1, 2, 3) #<! Notation for a tuple\n",
    "lList      = [1, 2, 3] #<! Notation for a list\n",
    "dDict      = {1: 3, 2: 2, 3: 1} #<! Notation for a dictionary\n",
    "oObj       = MyClass() #<! Notation for an object\n",
    "dfData     = pd.DataFrame() #<! Notation for a data frame\n",
    "dsData     = pd.Series() #<! Notation for a series\n",
    "hObj       = plt.Axes() #<! Notation for an object / handler / function handler\n",
    "```\n",
    "\n",
    "### Code Exercise\n",
    "\n",
    " - Single line fill\n",
    "\n",
    "```python\n",
    "valToFill = ???\n",
    "```\n",
    "\n",
    " - Multi Line to Fill (At least one)\n",
    "\n",
    "```python\n",
    "# You need to start writing\n",
    "?????\n",
    "```\n",
    "\n",
    " - Section to Fill\n",
    "\n",
    "```python\n",
    "#===========================Fill This===========================#\n",
    "# 1. Explanation about what to do.\n",
    "# !! Remarks to follow / take under consideration.\n",
    "mX = ???\n",
    "\n",
    "?????\n",
    "#===============================================================#\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# %matplotlib inline\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "seedNum = 512 # Try: 23, 12, 20\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "# Matplotlib default color palette\n",
    "lMatPltLibclr = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "# sns.set_theme() #>! Apply SeaBorn theme\n",
    "# sns.set_palette(\"tab10\")\n",
    "\n",
    "runInGoogleColab = 'google.colab' in str(get_ipython())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "FIG_SIZE_DEF    = (8, 8)\n",
    "ELM_SIZE_DEF    = 50\n",
    "CLASS_COLOR     = ('b', 'r')\n",
    "EDGE_COLOR      = 'k'\n",
    "MARKER_SIZE_DEF = 10\n",
    "LINE_WIDTH_DEF  = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Course Packages\n",
    "\n",
    "from AuxFun import ProxGradientDescent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# Data\n",
    "numGridPts  = 25\n",
    "polyDeg     = 5 #<! Polynomial Degree\n",
    "numFeatures = 3\n",
    "noiseStd    = 0.085\n",
    "\n",
    "# Solution Path\n",
    "λ  = 0.0075 #<! Verification\n",
    "vλ = np.linspace(0, 8, 250)\n",
    "\n",
    "# Solver\n",
    "μ               = 0.0075\n",
    "numIterations   = 75_000\n",
    "\n",
    "# # Verification\n",
    "ε = 1e-6 #<! Error threshold"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares with ${L}_{0}$ **Pseudo** Norm Regularization\n",
    "\n",
    "The ${L}_{0}$ regularized Least Squares (${L}_{0}$ Regularized LS) is given by:\n",
    "\n",
    "$$ \\arg \\min_{\\boldsymbol{x}} \\frac{1}{2} {\\left\\| \\boldsymbol{A} \\boldsymbol{x} - \\boldsymbol{b} \\right\\|}_{2}^{2} + \\lambda {\\left\\| \\boldsymbol{x} \\right\\|}_{0} $$\n",
    "\n",
    "Where $\\lambda {\\left\\| \\boldsymbol{x} \\right\\|}_{0}$ is the regularization term with $\\lambda \\geq 0$ sets the regularization level.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The ${L}_{0}$ is **not a norm**.\n",
    "* <font color='brown'>(**#**)</font> The ${L}_{0}$ counts the non zero elements: ${\\left\\| \\boldsymbol{x} \\right\\|}_{0} = \\sum_{i} {I}_{ \\neq 0 } \\left( {x}_{i} \\right)$.\n",
    "* <font color='brown'>(**#**)</font> The _mode_ of a set of numbers (As a vector) can be defined as $\\arg \\min_{\\alpha} {\\left\\| \\boldsymbol{x} - \\alpha \\boldsymbol{1} \\right\\|}_{0}$.\n",
    "* <font color='brown'>(**#**)</font> The motivation for the ${L}_{0}$ norm can explained as:\n",
    "  - Sparsity of the weights solution.\n",
    "  - [Bernoulli Distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution) Prior for the existence of the weight.  \n",
    "    For exact derivation see [Sparsifying Parametric Models with L0 Regularization](https://arxiv.org/abs/2409.03489).\n",
    "* <font color='brown'>(**#**)</font> The function $g \\left( \\boldsymbol{x} \\right) = \\lambda {\\left\\| \\boldsymbol{x} \\right\\|}_{0}$ has an efficient _Prox Operator_.\n",
    "* <font color='brown'>(**#**)</font> There are some approximations of the ${L}_{0}$ norm. See [Convex Optimization with ${L}_{0}$ Pseudo Norm](https://math.stackexchange.com/questions/1862775).\n",
    "\n",
    "The motivation for regularization is:\n",
    "\n",
    " - Include prior knowledge into into the model.\n",
    " - Avoid overfitting.\n",
    " - Make underdetermined systems solvable.\n",
    "\n",
    "This notebooks covers the solution of the above problem using the Proximal Gradient Descent method. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a sparse model of the parameters to optimize by.  \n",
    "The feature space is a polynomial where the data is generated by a sub set of coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate / Load the Data\n",
    "\n",
    "vA = np.sort(np.random.rand(numGridPts)) #<! Grid (Random samples, Sorted in ascending manner)\n",
    "mA = np.power(vA[:, None], range(polyDeg + 1)) #<! Model Matrix\n",
    "\n",
    "vXRef           = np.zeros(polyDeg + 1)\n",
    "vFeatIdx        = np.random.choice(polyDeg + 1, numFeatures, replace = False) #!< Active features index\n",
    "vXRef[vFeatIdx] = np.random.randn(numFeatures); #<! Active features\n",
    "\n",
    "vN = noiseStd * np.random.randn(numGridPts) #<! Noise Samples\n",
    "vS = mA @ vXRef\n",
    "vY = vS + vN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "This section defines the problem and solve it using the _Proximal Gradient Method_ (PGM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective Function\n",
    "\n",
    "The objective function:\n",
    "\n",
    "$$ \\arg \\min_{\\boldsymbol{x}} \\frac{1}{2} {\\left\\| \\boldsymbol{A} \\boldsymbol{x} - \\boldsymbol{b} \\right\\|}_{2}^{2} + \\lambda {\\left\\| \\boldsymbol{x} \\right\\|}_{0} $$\n",
    "\n",
    "Since the model \"punishes\" for extreme points only, it tries to bound the values into a symmetric range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective Function\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Implement the objective function. \n",
    "#    Given a vector of `vX` and a scalar `λ` it returns the objective.\n",
    "# 2. The implementation should be using a Lambda Function.\n",
    "# !! You may `np.square()` and / or `np.linalg.norm()`.\n",
    "# !! Pay attention to the variable of the labels.\n",
    "\n",
    "hObjFun = ???\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proximal Operator\n",
    "\n",
    "The Proximal Operator of a function $g \\left( \\cdot \\right)$ is given by:\n",
    "\n",
    "$$ \\operatorname{prox}_{\\lambda g \\left( \\cdot \\right)} \\left( \\boldsymbol{y} \\right) = \\arg \\min_{\\boldsymbol{x}} \\frac{1}{2} {\\left\\| \\boldsymbol{x} - \\boldsymbol{y} \\right\\|}_{2}^{2} + \\lambda g \\left( \\boldsymbol{x} \\right) $$\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The Proximal Operator can be thought as a generalization of a projection operator.\n",
    "* <font color='brown'>(**#**)</font> The Proximal Operator can be used to generalize the _Gradient_.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 001\n",
    "\n",
    "The Prox of the ${L}_{0}$ norm:\n",
    "\n",
    "$$ \\operatorname{prox}_{\\lambda {\\left\\| \\cdot \\right\\|}_{1}} \\left( \\boldsymbol{y} \\right) = \\arg \\min_{\\boldsymbol{x}} \\frac{1}{2} {\\left\\| \\boldsymbol{x} - \\boldsymbol{y} \\right\\|}_{2}^{2} + \\lambda {\\left\\| \\boldsymbol{x} \\right\\|}_{0} $$\n",
    "\n",
    "Hints:\n",
    "\n",
    " - Pay attention the problem is separable.\n",
    " - Per element, first solve with the assumption the solution is ${x}_{i} = 0$ and then for ${x}_{i} = {y}_{i}$.  \n",
    "   Explain why those are the 2 options.\n",
    "\n",
    "* <font color='red'>(**?**)</font> Is the problem convex?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 001\n",
    "\n",
    "<font color='red'>??? Fill the answer here ???</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Proximal Function\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Implement the prox operator function of the `λ || ||_∞` function. \n",
    "#    Given a vector `vY` and `λ` it returns the proximal at `vY`.\n",
    "# 2. The implementation should be using a Lambda Function.\n",
    "# !! You may assume `λ` > 0.\n",
    "# !! You may find `np.where()` useful.\n",
    "\n",
    "hProxFun = ???\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Could the function be validated with DCP?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation \n",
    "# Using SciPy with a gradient free method global optimizer.\n",
    "\n",
    "# Model Data\n",
    "vYY = np.linspace(-0.5, 0.5, 41)\n",
    "hMinFun = lambda vX: 0.5 * np.square(np.linalg.norm(vX - vYY)) + λ * np.linalg.norm(vX, 0) #<! Objective function\n",
    "\n",
    "# Model Problem\n",
    "sOptRes = sp.optimize.direct(hMinFun, sp.optimize.Bounds(-3 * np.ones_like(vYY), 3 * np.ones_like(vYY)), maxfun = 10_000 * len(vYY), maxiter = 50_000)\n",
    "vXX = sOptRes.x\n",
    "\n",
    "assert (sOptRes.success), 'The problem is not solved.'\n",
    "print('Problem is solved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Operator\n",
    "\n",
    "hF, hA = plt.subplots(figsize = (10, 6))\n",
    "hA.plot(vYY, vYY, lw = 2, label = 'Input')\n",
    "hA.plot(vYY, hProxFun(vYY, λ), lw = 2, label = f'Hard Threshold Prox, Objective Value: {hMinFun(hProxFun(vYY, λ))}')\n",
    "hA.plot(vYY, vXX, lw = 2, label = f'Global Optimization SciPy, Objective Value: {hMinFun(vXX)}')\n",
    "hA.set_title(r'The Prox Operator of ${L}_{0}$' + f' Norm, λ = {λ: 0.2f}')\n",
    "hA.set_xlabel('Input Value')\n",
    "hA.set_ylabel('Output Value')\n",
    "\n",
    "hA.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> How come the Prox method achieves better objective value than the _Global Optimizer_?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proximal Gradient Method\n",
    "\n",
    "For the composition model of:\n",
    "\n",
    "$$ F \\left( \\boldsymbol{x} \\right) = f \\left( \\boldsymbol{x} \\right) + \\lambda g \\left( \\boldsymbol{x} \\right) $$\n",
    "\n",
    "Where $f \\left( \\boldsymbol{x} \\right)$ is smooth and convex and $g \\left( \\boldsymbol{x} \\right)$ is convex with a given prox operator.\n",
    "\n",
    "The method iteration is given by:\n",
    "\n",
    "$$ \\boldsymbol{x}^{k + 1} = \\operatorname{prox}_{ {\\mu}_{k} \\lambda g \\left( \\cdot \\right) } \\left( \\boldsymbol{x}^{k} - {\\nabla}_{f} \\left( \\boldsymbol{x}^{k} \\right) \\right) $$\n",
    "\n",
    "Where ${\\mu}_{k}$ is the step size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> For which $g$ the above becomes a _Projected Gradient_ descent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Function\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Implement the gradient function (1/2) * || A x - y ||_2^2. \n",
    "#    Given a vector `vX` it returns the gradient at `vX`.\n",
    "# 2. The implementation should be using a Lambda Function.\n",
    "# !! You may pre calculate terms for efficient code.\n",
    "\n",
    "hGradFun = ???\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proximal Gradient Method (PGM)\n",
    "\n",
    "oProxGrad = ProxGradientDescent(np.zeros(polyDeg + 1), hGradFun, μ, λ = λ, hProxFun = hProxFun, useAccel = True)\n",
    "lX = oProxGrad.ApplyIterations(numIterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * <font color='brown'>(**#**)</font> The size of $\\mu$ which guarantees convergence depends on the smoothness of $f$, The Lipschitz constant of its gradient.  \n",
    "   For cases where $f$ is the Linear Least Squares problem the constant is given by ${\\left\\| \\boldsymbol{A} \\right\\|}_{2}^{2}$, Namely the square of the largest singular value of $\\boldsymbol{A}$.\n",
    " * <font color='brown'>(**#**)</font> One could implement adaptive step size in a similar manner to Gradient Descent with a different decision rule.\n",
    " * <font color='blue'>(**!**)</font> Go through the implementation of `ProxGradientDescent`.\n",
    " * <font color='blue'>(**!**)</font> Edit the code to use $\\mu$ set by the Lipschitz constant of ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution Analysis\n",
    "\n",
    "objValRef   = hObjFun(vXRef, λ)\n",
    "vObjVal     = np.empty(numIterations)\n",
    "vArgErr     = np.empty(numIterations)\n",
    "\n",
    "for ii in range(numIterations):\n",
    "    vObjVal[ii] = hObjFun(lX[ii], λ)\n",
    "    vArgErr[ii] = np.linalg.norm(lX[ii] - vXRef)\n",
    "\n",
    "vObjVal = 20 * np.log10(np.maximum(np.abs(vObjVal - objValRef), np.sqrt(np.spacing(1.0))) / max(np.abs(objValRef), np.sqrt(np.spacing(1.0))))\n",
    "vArgErr = 20 * np.log10(np.maximum(np.abs(vArgErr), np.sqrt(np.spacing(1.0))) / max(np.linalg.norm(vXRef), np.sqrt(np.spacing(1.0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Results\n",
    "\n",
    "hF, hA = plt.subplots(figsize = (12, 6))\n",
    "hA.plot(range(numIterations), vObjVal, lw = 2, label = 'Objective Function')\n",
    "hA.plot(range(numIterations), vArgErr, lw = 2, label = 'Argument Error')\n",
    "hA.set_xlabel('Iteration Index')\n",
    "hA.set_ylabel('Relative Error [dB]')\n",
    "hA.set_title('Proximal Gradient Method Convergence')\n",
    "\n",
    "hA.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression\n",
    "\n",
    "This section shows the difference between the Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Least Squares Solution\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Calculate the LS solution of the problem.\n",
    "#    The polynomial model is `mA * vXLS ≈ vY`.\n",
    "# !! You find `sp.linalg.lstsq()` useful.\n",
    "\n",
    "vXLS ???\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Results\n",
    "\n",
    "hF, hA = plt.subplots(figsize = (10, 8))\n",
    "hA.plot(vA, vS, lw = 2, label = 'Model Data')\n",
    "hA.plot(vA, vY, ls = 'None', marker = '*', markersize = 5, label = 'Data Samples')\n",
    "hA.plot(vA, mA @ vXLS, lw = 2, label = 'Least Squares')\n",
    "hA.plot(vA, mA @ lX[-1], lw = 2, label = 'L0 Regularized Least Squares')\n",
    "hA.set_xlim((0, 1))\n",
    "hA.set_title('Polynomial Model - Estimation from Data Samples')\n",
    "hA.set_xlabel('x')\n",
    "hA.set_ylabel('y')\n",
    "\n",
    "hA.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> The regularization limits the _number of active_ coefficients.\n",
    "* <font color='red'>(**?**)</font> Is the result better? Should it be better?\n",
    "* <font color='red'>(**?**)</font> Is is robust to the outlier at the top left?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "\n",
    "This section shows the ability to select features using the _Sparsity_ property of the solution.  \n",
    "The dataset is the [Diabetes Dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html).\n",
    "\n",
    "Features:\n",
    " - `Age` - Age in years.\n",
    " - `Sex` - Positive / Negative.\n",
    " - `BMI` - Body Mass Index.\n",
    " - `BP` - Blood Pressure.\n",
    " - `TC` - Total Serum Cholesterol.\n",
    " - `LDL` - Low Density Lipoproteins.\n",
    " - `HDL` - High Density Lipoproteins.\n",
    " - `TCH` - Total Cholesterol (HDL).\n",
    " - `LTG` - Log of Serum Triglycerides Level.\n",
    " - `GLU` - Blood Sugar Level.\n",
    "\n",
    "Each of these 10 feature variables have been mean centered and scaled.\n",
    "Each features scaled by the standard deviation times the square root of the number of samples.\n",
    "Hence the sum of squares of each column totals $1$.\n",
    "\n",
    "Target: A quantitative measure of disease progression one year after baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "\n",
    "dfA, dsB = load_diabetes(return_X_y = True, as_frame = True)\n",
    "\n",
    "dfA = dfA.rename(columns = {\n",
    "    'age': 'Age',\n",
    "    'sex': 'Sex',\n",
    "    'bmi': 'BMI',\n",
    "    'bp': 'Blood Pressure',\n",
    "    's1': 'TC',\n",
    "    's2': 'LDL',\n",
    "    's3': 'HDL',\n",
    "    's4': 'TCH',\n",
    "    's5': 'LTG',\n",
    "    's6': 'GLU'\n",
    "})\n",
    "dfA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is the $\\lambda$ path of the optimization problem:\n",
    "\n",
    "$$ \\arg \\min_{\\boldsymbol{x}} \\frac{1}{2} {\\left\\| \\boldsymbol{A} \\boldsymbol{x} - \\boldsymbol{b} \\right\\|}_{2}^{2} + \\lambda {\\left\\| \\boldsymbol{x} \\right\\|}_{0} $$\n",
    "\n",
    "Where $\\boldsymbol{A}$ is the matrix of features and $\\boldsymbol{b}$ is the target of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric Data\n",
    "\n",
    "mA = dfA.to_numpy()\n",
    "vB = dsB.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Function for the Data\n",
    "\n",
    "mAA = mA.T @ mA\n",
    "vAb = mA.T @ vB\n",
    "\n",
    "hGradFun = lambda vX: mAA @ vX - vAb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the λ Path\n",
    "\n",
    "numSamples  = mA.shape[0]\n",
    "numFeatures = mA.shape[1]\n",
    "mXλ = np.zeros((numFeatures, len(vλ)))\n",
    "\n",
    "for ii, valλ in enumerate(vλ):\n",
    "    oProxGrad = ProxGradientDescent(np.zeros(numFeatures), hGradFun, μ, λ = numSamples * valλ, hProxFun = hProxFun, useAccel = True)\n",
    "    oProxGrad.ApplyIterations(numIterations // 10, logArg = False)\n",
    "    mXλ[:, ii] = np.fabs(oProxGrad.vX) #<! Significance is in absolute value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Lasso Path\n",
    "\n",
    "lFeatureName = dfA.columns.to_list()\n",
    "\n",
    "hF, hA = plt.subplots(figsize = (10, 8))\n",
    "for ii in range(numFeatures):\n",
    "    hA.plot(vλ, mXλ[ii, :], lw = 2, label = lFeatureName[ii])\n",
    "\n",
    "hA.set_title('Feature Significance')\n",
    "hA.set_xlabel('λ')\n",
    "hA.set_ylabel('Significance (Feature Absolute Weight)')\n",
    "\n",
    "hA.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
