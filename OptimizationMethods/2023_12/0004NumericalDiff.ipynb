{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Fixel Algorithms](https://fixelalgorithms.co/images/CCExt.png)](https://fixelalgorithms.gitlab.io)\n",
    "\n",
    "# Optimization Methods\n",
    "\n",
    "## Essential Linear Algebra - Numerical Differentiation\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 0.1.000 | 03/12/2023 | Royi Avital | First version                                                      |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/OptimizationMethods/2023_12/0001NumericalDiff.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought.\n",
    "\n",
    "Code Notations:\n",
    "\n",
    "```matlab\n",
    "someVar    = 2; %<! Notation for a variable\n",
    "vVector    = rand(4, 1); %<! Notation for 1D array\n",
    "mMatrix    = rand(4, 3); %<! Notation for 2D array\n",
    "tTensor    = rand(4, 3, 2, 3); %<! Notation for nD array (Tensor)\n",
    "cCell      = cell(3, 1); %<! Notation for a cell array\n",
    "sStructure = struct(); %<! Notation for a structure\n",
    "taTable    = table(); %<! Notation for a table\n",
    "hObj       = axes(); %<! Notation for an object / handler / function handler\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T09:30:06.492269Z",
     "start_time": "2022-02-02T09:30:06.220934Z"
    }
   },
   "outputs": [],
   "source": [
    "%% Configuration Parameters\n",
    "\n",
    "subStreamNumberDefault = 79;\n",
    "\n",
    "run('InitScript.m');\n",
    "\n",
    "figureIdx           = 0;\n",
    "figureCounterSpec   = '%04d';\n",
    "\n",
    "generateFigures = ON;\n",
    "\n",
    "imatlab_export_fig('print-svg');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% Constants\n",
    "\n",
    "DIFF_MODE_FORWARD   = 1;\n",
    "DIFF_MODE_BACKWARD  = 2;\n",
    "DIFF_MODE_CENTRAL   = 3;\n",
    "DIFF_MODE_COMPLEX   = 4;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% Parameters\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Differentiation\n",
    "\n",
    "This notebooks explores the use of [_Numerical Differentiation_](https://en.wikipedia.org/wiki/Numerical_differentiation) to caclulate the gradient of a function.\n",
    "\n",
    "The gradient of a multivariate scalar function, $f : \\mathbb{R}^{n} \\to \\mathbb{R}$, is given by:\n",
    "\n",
    "$$ {{\\nabla}_{x} f \\left( \\boldsymbol{x} \\right)}_{i} = \\lim_{t \\to 0} \\frac{ f \\left( \\boldsymbol{x} + t \\boldsymbol{e}_{i} \\right) - f \\left( \\boldsymbol{x} \\right) }{t} $$\n",
    "\n",
    "Where $\\boldsymbol{e}_{i} = \\left[ 0, 0, \\ldots, 0, \\underbrace{1}_{\\text{i -th index}}, 0, \\ldots, 0 \\right]$. \n",
    "\n",
    "This can be approximated by [_Finite Difference_](https://en.wikipedia.org/wiki/Finite_difference) with specific [_Finite Difference Coefficient_](https://en.wikipedia.org/wiki/Finite_difference_coefficient).  \n",
    "There 3 common approaches:\n",
    "\n",
    " - Forward: ${{\\nabla}_{x} f \\left( \\boldsymbol{x} \\right)}_{i} \\approx \\frac{ f \\left( \\boldsymbol{x} + h \\boldsymbol{e}_{i} \\right) - f \\left( \\boldsymbol{x} \\right) }{h}$.\n",
    " - Backward: ${{\\nabla}_{x} f \\left( \\boldsymbol{x} \\right)}_{i} \\approx \\frac{ f \\left( \\boldsymbol{x} \\right) - f \\left( \\boldsymbol{x} - h \\boldsymbol{e}_{i} \\right) }{h}$.\n",
    " - Central: ${{\\nabla}_{x} f \\left( \\boldsymbol{x} \\right)}_{i} \\approx \\frac{ f \\left( \\boldsymbol{x} + h \\boldsymbol{e}_{i} \\right) - f \\left( \\boldsymbol{x} - h \\boldsymbol{e}_{i} \\right) }{2 h}$.\n",
    "\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The notebook use the `CalcFunGrad.m` file for the actual calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Gradient of a Composition of a Linear Function and Element Wise Function\n",
    "\n",
    "Compute the directional derivative $\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]$ and the gradient $\\nabla f \\left( \\boldsymbol{x} \\right)$ of:\n",
    "\n",
    "$$ f \\left( \\boldsymbol{x} \\right) = {a}^{T} g \\left[ \\boldsymbol{x} \\right] $$\n",
    "\n",
    "Where $g \\left[ \\cdot \\right]$ is an element wise function $g \\left[ \\boldsymbol{x} \\right] = \\begin{bmatrix} g \\left( {x}_{1} \\right) \\\\ g \\left( {x}_{2} \\right) \\\\ \\vdots \\\\ g \\left( {x}_{d} \\right) \\end{bmatrix} \\in \\mathbb{R}^{d}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> We'll be using $\\left[ \\cdot \\right]$ as a notation for element wise functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The directional derivative of $g \\left( \\cdot \\right)$ is given by:\n",
    "\n",
    "$$ \\nabla g \\left( \\boldsymbol{x} \\right) = \\lim_{t \\to 0} \\frac{g \\left( \\boldsymbol{x} + t \\boldsymbol{h} \\right) - g \\left( \\boldsymbol{x} \\right)}{t} = \\lim_{t \\to 0} \\frac{1}{t} \\left( \\begin{bmatrix} g \\left( {x}_{1} + t {h}_{1} \\right) \\\\ g \\left( {x}_{2} + t {h}_{2} \\right) \\\\ \\vdots \\\\ g \\left( {x}_{d} + t {h}_{d} \\right) \\end{bmatrix} - \\begin{bmatrix} g \\left( {x}_{1} \\right) \\\\ g \\left( {x}_{2} \\right) \\\\ \\vdots \\\\ g \\left( {x}_{d} \\right) \\end{bmatrix} \\right) = \\begin{bmatrix} g' \\left( {x}_{1} \\right) {h}_{1} \\\\ g' \\left( {x}_{2} \\right) {h}_{2} \\\\ \\vdots \\\\ g' \\left( {x}_{d} \\right) {h}_{d} \\end{bmatrix} = g' \\left( \\boldsymbol{x} \\right) \\circ \\boldsymbol{h} $$\n",
    "\n",
    "* <font color='brown'>(**#**)</font> Pay attention that $g \\left( \\cdot \\right)$ is not a scalar function but a vector function.\n",
    "\n",
    "By definition $ f \\left( \\boldsymbol{x} \\right) = \\left \\langle \\boldsymbol{x}, g \\left( \\boldsymbol{x} \\right) \\right \\rangle$ hence:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right] & = \\left \\langle \\boldsymbol{a}, \\nabla g \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right] \\right \\rangle && \\text{Linear operator} \\\\\n",
    "& = \\left \\langle \\boldsymbol{a}, g' \\left( \\boldsymbol{x} \\right) \\circ \\boldsymbol{h} \\right \\rangle && \\text{} \\\\\n",
    "& = \\left \\langle \\boldsymbol{a}, \\operatorname{Diag} \\left( g' \\left( \\boldsymbol{x} \\right) \\right) \\boldsymbol{h} \\right \\rangle && \\text{Property of Hadamard product: $\\boldsymbol{a} \\circ \\boldsymbol{b} = \\operatorname{Diag} \\left( \\boldsymbol{a} \\right) \\boldsymbol{b}$} \\\\\n",
    "& = \\left \\langle \\operatorname{Diag} \\left( g' \\left( \\boldsymbol{x} \\right) \\right) \\boldsymbol{a}, \\boldsymbol{h} \\right \\rangle && \\text{Adjoint of diagonal matrix} \\\\\n",
    "& \\Rightarrow \\nabla f \\left( \\boldsymbol{x} \\right) = \\operatorname{Diag} \\left( g' \\left( \\boldsymbol{x} \\right) \\right) \\boldsymbol{a}\n",
    "&& \\blacksquare\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> The function $\\operatorname{diag} \\left( \\cdot \\right) : \\mathbb{R}^{d \\times d} \\to \\mathbb{R}^{d} $ returns the diagonal of a matrix, that is, $\\boldsymbol{b} = \\operatorname{diag} \\left( \\boldsymbol{X} \\right) \\implies \\boldsymbol{b} \\left[ i \\right] = \\left( \\boldsymbol{X} \\left[ i, i\\right] \\right)$.\n",
    "* <font color='brown'>(**#**)</font> The function $\\operatorname{Diag} \\left( \\cdot \\right) : \\mathbb{R}^{d} \\to \\mathbb{R}^{d \\times d} $ returns a diagonal matrix from a vector, that is, $B = \\operatorname{diag} \\left( \\boldsymbol{x} \\right) \\implies \\boldsymbol{B} \\left[ i, j \\right] = \\begin{cases}\n",
    "{x}_{i} & \\text{ if } i = j \\\\ \n",
    "0 & \\text{ if } i \\neq j \n",
    "\\end{cases}$.\n",
    "* <font color='brown'>(**#**)</font> Pay attention that $\\left \\langle \\boldsymbol{a}, \\operatorname{diag} \\left( X \\right) \\right \\rangle = \\left \\langle \\operatorname{Diag} \\left( \\boldsymbol{a} \\right), X \\right \\rangle$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Size Sensitivity Analysis\n",
    "\n",
    "In this section we'll analyze the sensitivity of the numerical differentiation to the step size, $h$.\n",
    "\n",
    "We'll use the function:\n",
    "\n",
    "$$ f \\left( \\boldsymbol{X} \\right) = \\left \\langle \\boldsymbol{A}, \\sin \\left[ \\boldsymbol{X} \\right] \\right \\rangle $$\n",
    "\n",
    "Where:\n",
    "\n",
    " - $\\boldsymbol{X} \\in \\mathbb{R}^{d \\times d}$.\n",
    " - The function $\\sin \\left[ \\cdot \\right]$ is the element wise $\\sin$ function: $\\boldsymbol{M} = \\sin \\left[ \\boldsymbol{X} \\right] \\implies \\boldsymbol{M} \\left[ i, j \\right] = \\sin \\left( \\boldsymbol{X} \\left[ i, j\\right] \\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla f \\left( X \\right) \\left[ \\boldsymbol{H} \\right] & = \\left \\langle A, \\left( \\cos \\left[ X \\right] \\right) \\circ H \\right \\rangle && \\text{Since $\\frac{d \\sin \\left( x \\right)}{dx} = \\cos \\left( x \\right)$} \\\\\n",
    "& = \\left \\langle \\cos \\left[ \\boldsymbol{X} \\right] \\circ \\boldsymbol{A}, H \\right \\rangle && \\text{Adjoint} \\\\\n",
    "& \\Rightarrow \\nabla f \\left( X \\right) = \\cos \\left[ \\boldsymbol{X} \\right] \\circ A\n",
    "&& \\blacksquare\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% Parameters\n",
    "\n",
    "numSteps = 1000;\n",
    "\n",
    "numRows = 100;\n",
    "numCols = 1; %<! Like a vector\n",
    "\n",
    "vStepSize = logspace(-3, -9, numSteps);\n",
    "\n",
    "vMethods    = [DIFF_MODE_FORWARD; DIFF_MODE_BACKWARD; DIFF_MODE_CENTRAL];\n",
    "vMethodName = [\"Forward\", \"Backward\", \"Central\"];\n",
    "\n",
    "% Data \n",
    "mA = randn(numRows, numCols);\n",
    "mX = randn(numRows, numCols);\n",
    "\n",
    "% Function\n",
    "hF = @(mX) sum(mA .* sin(mX));\n",
    "\n",
    "% Analytic Gradient\n",
    "hGradF = @(mX) cos(mX) .* mA;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% Sensitivity Analysis\n",
    "\n",
    "numMethods = length(vMethods);\n",
    "\n",
    "vG = hGradF(mX);\n",
    "mE = zeros(numSteps, numMethods);\n",
    "\n",
    "for jj = 1:numMethods\n",
    "  for ii = 1:numSteps\n",
    "    mE(ii, jj) = 20 * log10(norm(vG - CalcFunGrad(mX, hF, vMethods(jj), vStepSize(ii)), 'inf'));\n",
    "  end\n",
    "end\n",
    "\n",
    "figure();\n",
    "hA = axes();\n",
    "set(hA, 'NextPlot', 'add');\n",
    "for ii = 1:numMethods\n",
    "  plot(vStepSize, mE(:, ii), 'DisplayName', vMethodName(ii), 'LineWidth', lineWidthNormal);\n",
    "end\n",
    "set(get(hA, 'Title'), 'String', {['Numerical Differentiation Error - Max Absolute Error']}, 'FontSize', fontSizeTitle);\n",
    "set(get(hA, 'XLabel'), 'String', {['Step Size']}, 'FontSize', fontSizeAxis);\n",
    "set(get(hA, 'YLabel'), 'String', {['Error [dB]']}, 'FontSize', fontSizeAxis);\n",
    "ClickableLegend();\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Complex Step Trick\n",
    "\n",
    "In general, the finite differences step size si a function of the argument and the function itself.  \n",
    "There are many cases where the method becomes highly sensitive and with the finite floating point accuracy it might cause some errors.\n",
    "\n",
    "It turns out that for _real analytic functions_ (Think of a convergent Taylor Series) we can do a trick:\n",
    "\n",
    "$$ f \\left( x + ih \\right) = f \\left( x \\right) + f' \\left( x \\right) i h + \\frac{f'' \\left( x \\right)}{2} {\\left(ih \\right)}^{2} + \\mathcal{O}(h^3) \\implies \\mathrm{Im} \\,\\left( \\frac{ f \\left( x + ih \\right)}{h} \\right) = f' \\left( x \\right) + \\mathcal{O}(h^2). $$\n",
    "\n",
    "Which is much more stable regardless of the value of the step size.\n",
    "\n",
    "Yet, there are some cases to handle:\n",
    " - Use `abs()` which uses the definition `abs(x + i y) = sign(x) * (x + i y)`.\n",
    " - Use `min()` / `max()` which only use the real part for comparison.\n",
    " - Use `.'` instead of `'` to apply _transpose_ instead of _hermitian transpose_.\n",
    "\n",
    "Resources:\n",
    " - [Sebastien Boisgerault - Complex Step Differentiation](https://direns.mines-paristech.fr/Sites/Complex-analysis/Complex-Step%20Differentiation/).\n",
    " - [Nick Higham - What Is the Complex Step Approximation](https://nhigham.com/2020/10/06/what-is-the-complex-step-approximation/).\n",
    " - [Derek Elkins - Complex Step Differentiation](https://www.hedonisticlearning.com/posts/complex-step-differentiation.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "In order to verify the robustness of the problem we'll use:\n",
    "\n",
    "$$ f \\left( x \\right) = {e}^{x} $$\n",
    "\n",
    "At $x = 0$, which will allow us to use a perfect reference and the relative error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% Parameters\n",
    "\n",
    "numSteps = 1500;\n",
    "\n",
    "vStepSize = logspace(-3, -15, numSteps);\n",
    "\n",
    "vMethods    = [DIFF_MODE_FORWARD; DIFF_MODE_BACKWARD; DIFF_MODE_CENTRAL; DIFF_MODE_COMPLEX];\n",
    "vMethodName = [\"Forward\", \"Backward\", \"Central\", \"Complex\"];\n",
    "\n",
    "% Data \n",
    "valX = 0;\n",
    "\n",
    "% Function\n",
    "hF = @(x) exp(x);\n",
    "\n",
    "% Analytic Gradient\n",
    "gradF = 1; %<! At x = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% Sensitivity Analysis\n",
    "\n",
    "numMethods = length(vMethods);\n",
    "\n",
    "mE = zeros(numSteps, numMethods);\n",
    "\n",
    "for jj = 1:numMethods\n",
    "  for ii = 1:numSteps\n",
    "    mE(ii, jj) = 20 * log10(abs(gradF - CalcFunGrad(valX, hF, vMethods(jj), vStepSize(ii))));\n",
    "  end\n",
    "end\n",
    "\n",
    "figure();\n",
    "hA = axes();\n",
    "set(hA, 'NextPlot', 'add');\n",
    "for ii = 1:numMethods\n",
    "  plot(vStepSize, mE(:, ii), 'DisplayName', vMethodName(ii), 'LineWidth', lineWidthNormal);\n",
    "end\n",
    "set(get(hA, 'Title'), 'String', {['Numerical Differentiation Error - Relative Error']}, 'FontSize', fontSizeTitle);\n",
    "set(get(hA, 'XLabel'), 'String', {['Step Size']}, 'FontSize', fontSizeAxis);\n",
    "set(get(hA, 'YLabel'), 'String', {['Error [dB]']}, 'FontSize', fontSizeAxis);\n",
    "set(hA, 'XScale', 'log', 'XDir', 'reverse');\n",
    "ClickableLegend();\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MATLAB",
   "language": "matlab",
   "name": "imatlab"
  },
  "language_info": {
   "codemirror_mode": "octave",
   "file_extension": ".m",
   "mimetype": "text/x-matlab",
   "name": "matlab",
   "pygments_lexer": "matlab",
   "version": "23.2.0.2428915 (R2023b) Update 4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
